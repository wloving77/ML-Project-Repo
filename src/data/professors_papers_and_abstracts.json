[
    {
        "Professor": "Homa Alemzadeh",
        "Papers": [
            {
                "title": "Adverse events in robotic surgery: a retrospective study of 14 years of FDA data",
                "abstract": "Background Use of robotic systems for minimally invasive surgery has rapidly increased during the last decade. Understanding the causes of adverse events and their impact on patients in robot-assisted surgery will help improve systems and operational practices to avoid incidents in the future. Methods By developing an automated natural language processing tool, we performed a comprehensive analysis of the adverse events reported to the publicly available MAUDE database (maintained by the U.S. Food and Drug Administration) from 2000 to 2013. We determined the number of events reported per procedure and per surgical specialty, the most common types of device malfunctions and their impact on patients, and the potential causes for catastrophic events such as patient injuries and deaths. Results During the study period, 144 deaths (1.4% of the 10,624 reports), 1,391 patient injuries (13.1%), and 8,061 device malfunctions (75.9%) were reported. The numbers of injury and death events per procedure have stayed relatively constant (mean = 83.4, 95% confidence interval (CI), 74.2\u201392.7 per 100,000 procedures) over the years. Surgical specialties for which robots are extensively used, such as gynecology and urology, had lower numbers of injuries, deaths, and conversions per procedure than more complex surgeries, such as cardiothoracic and head and neck (106.3 vs. 232.9 per 100,000 procedures, Risk Ratio = 2.2, 95% CI, 1.9\u20132.6). Device and instrument malfunctions, such as falling of burnt/broken pieces of instruments into the patient (14.7%), electrical arcing of instruments (10.5%), unintended operation of instruments (8.6%), system errors (5%), and video/imaging problems (2.6%), constituted a major part of the reports. Device malfunctions impacted patients in terms of injuries or procedure interruptions. In 1,104 (10.4%) of all the events, the procedure was interrupted to restart the system (3.1%), to convert the procedure to non-robotic techniques (7.3%), or to reschedule it (2.5%). Conclusions Despite widespread adoption of robotic systems for minimally invasive surgery in the U.S., a non-negligible number of technical difficulties and complications are still being experienced during procedures. Adoption of advanced techniques in design and operation of robotic surgical systems and enhanced mechanisms for adverse event reporting may reduce these preventable incidents in the future.",
                "paper_link": "https://www.semanticscholar.org/paper/2d4a5cac38740263f7f0b81d32a2db1b6e11ef17"
            },
            {
                "title": "Targeted Attacks on Teleoperated Surgical Robots: Dynamic Model-based Detection and Mitigation",
                "abstract": "This paper demonstrates targeted cyber-physical attacks on teleoperated surgical robots. These attacks exploit vulnerabilities in the robot's control system to infer a critical time during surgery to drive injection of malicious control commands to the robot. We show that these attacks can evade the safety checks of the robot, lead to catastrophic consequences in the physical system (e.g., sudden jumps of robotic arms or system's transition to an unwanted halt state), and cause patient injury, robot damage, or system unavailability in the middle of a surgery. We present a model-based analysis framework that can estimate the consequences of control commands through real-time computation of robot's dynamics. Our experiments on the RAVEN II robot demonstrate that this framework can detect and mitigate the malicious commands before they manifest in the physical system with an average accuracy of 90%.",
                "paper_link": "https://www.semanticscholar.org/paper/b25819b1a2da57e279187ee27f153ec09ea9d195"
            },
            {
                "title": "On the safety of machine learning: Cyber-physical systems, decision sciences, and data products",
                "abstract": "Machine learning algorithms increasingly influence our decisions and interact with us in all parts of our daily lives. Therefore, just as we consider the safety of power plants, highways, and a variety of other engineered socio-technical systems, we must also take into account the safety of systems involving machine learning. Heretofore, the definition of safety has not been formalized in a machine learning context. In this article, we do so by defining machine learning safety in terms of risk, epistemic uncertainty, and the harm incurred by unwanted outcomes. We then use this definition to examine safety in all sorts of applications in cyber-physical systems, decision sciences, and data products. We find that the foundational principle of modern statistical machine learning, empirical risk minimization, is not always a sufficient objective. We discuss how four different categories of strategies for achieving safety in engineering, including inherently safe design, safety reserves, safe fail, and procedural safeguards can be mapped to a machine learning context. We then discuss example techniques that can be adopted in each category, such as considering interpretability and causality of predictive models, objective functions beyond expected prediction accuracy, human involvement for labeling difficult or rare examples, and user experience design of software and open data.",
                "paper_link": "https://www.semanticscholar.org/paper/f7d3f3a23c1b284a17adc93a922c56be38d221df"
            },
            {
                "title": "Analysis of safety-critical computer failures in medical devices",
                "abstract": "Malfunctioning medical devices are one of the leading causes of serious injury and death in the US. Between 2006 and 2011, 5,294 recalls and approximately 1.2 million adverse events were reported to the US Food and Drug Administration (FDA). Almost 23 percent of these recalls were due to computer-related failures, of which approximately 94 percent presented medium to high risk of severe health consequences (such as serious injury or death) to patients. This article investigates the causes of failures in computer-based medical devices and their impact on patients by analyzing human-written descriptions of recalls and adverse event reports obtained from public FDA databases. The authors characterize computer-related failures by deriving fault classes, failure modes, recovery actions, and number of devices affected by the recalls. This analysis is used as a basis for identifying safety issues in life-critical medical devices and providing insights on the future challenges in the design of safety-critical medical devices.",
                "paper_link": "https://www.semanticscholar.org/paper/39f39ef7d6c23adf2958eed3d6e1499ecadb1a08"
            },
            {
                "title": "On threat modeling and mitigation of medical cyber-physical systems",
                "abstract": "Medical Cyber Physical Systems (MCPS) are lifecritical networked systems of medical devices. These systems are increasingly used in hospitals to provide high-quality healthcare for patients. However, MCPS also bring concerns about security and safety and new challenges to protect patients from acts of theft or malice. In this paper, we focus our investigation on a thorough understanding of threat modeling in MCPS. We examine the roles of stakeholders and system components and sketch an abstract architecture of a MCPS to demonstrate various threat modeling options. We also discuss possible security techniques and their applicability and utility for the design of secure MCPS. This work forms a basis for understanding threatening conditions in MCPS, and embarks on promising state-of-the-art research trends for addressing MCPS security concerns.",
                "paper_link": "https://www.semanticscholar.org/paper/da01bd89e72a5d96d447a18218923e2fcaf40c1a"
            },
            {
                "title": "Experimental Resilience Assessment of An Open-Source Driving Agent",
                "abstract": "Autonomous vehicles (AV) depend on the sensors like RADAR and camera for the perception of the environment, path planning, and control. With the increasing autonomy and interactions with the complex environment, there have been growing concerns regarding the safety and reliability of AVs. This paper presents a Systems-Theoretic Process Analysis (STPA) based fault injection framework to assess the resilience of an open-source driving agent, called openpilot, under different environmental conditions and faults affecting sensor data. To increase the coverage of unsafe scenarios during testing, we use a strategic software fault-injection approach where the triggers for injecting the faults are derived from the unsafe scenarios identified during the high-level hazard analysis of the system. The experimental results show that the proposed strategic fault injection approach increases the hazard coverage compared to random fault injection and, thus, can help with more effective simulation of safety-critical faults and testing of AVs. In addition, the paper provides insights on the performance of openpilot safety mechanisms and its ability in timely detection and recovery from faulty inputs.",
                "paper_link": "https://www.semanticscholar.org/paper/55401dbebc1859801be36020ea70755dab25460d"
            },
            {
                "title": "A review of cognitive assistants for healthcare: Trends, prospects, and future directions",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Systems-theoretic Safety Assessment of Robotic Telesurgical Systems",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/9f69327c9f06ed2cf4f2f00e3eb5c725b0cbdaf5"
            },
            {
                "title": "Reliability in application specific mesh-based NoC architectures",
                "abstract": "Networks on chips (NoCs) provide a mechanism for handling complex communications in the next generation of integrated circuits. At the same time, lower yield in nano-technology, makes self repair communication channels a necessity in design of digital systems. This paper proposes a reliable NoC architecture based on specific application mapped onto an NoC. This architecture is capable of recovering from permanent switch failures via replacing them by neighboring switches. This method has hardware and power consumption overhead, but significantly improves reliability and has a very little effect on the performance of the system. We suggest a reliability analysis method based on the combinatorial reliability models and use it to evaluate our proposed fault-tolerant NoC architecture.",
                "paper_link": "https://www.semanticscholar.org/paper/cba60752ee42faddf9755d6e1ef9e3ec0d7828f5"
            },
            {
                "title": "An NLP-based cognitive system for disease status identification in electronic health records",
                "abstract": "This paper presents a natural language processing (NLP) based cognitive decision support system that automatically identifies the status of a disease from the clinical notes of a patient record. The system relies on IBM Watson Patient Record NLP analytics and supervised or semi-supervised learning techniques. It uses unstructured text in clinical notes, data from the structured part of a patient record, and disease control targets from the clinical guidelines. We evaluated the system using de-identified patient records of 414 hypertensive patients from a multi-specialty hospital system in the U.S. The experimental results show that, using supervised learning methods, our system can achieve an average 0.86 F1-score in identifying disease status passages and average accuracy of 0.77 in classifying the status as controlled or not. To the best of our knowledge, this is the first system to automatically identify disease control status from clinical notes.",
                "paper_link": "https://www.semanticscholar.org/paper/81a9dc3c29912974d5547157480a6348369dfde8"
            },
            {
                "title": "Real-Time Context-aware Detection of Unsafe Events in Robot-Assisted Surgery",
                "abstract": "Cyber-physical systems for robotic surgery have enabled minimally invasive procedures with increased precision and shorter hospitalization. However, with increasing complexity and connectivity of software and major involvement of human operators in the supervision of surgical robots, there remain significant challenges in ensuring patient safety. This paper presents a safety monitoring system that, given the knowledge of the surgical task being performed by the surgeon, can detect safety-critical events in real-time. Our approach integrates a surgical gesture classifier that infers the operational context from the time-series kinematics data of the robot with a library of erroneous gesture classifiers that given a surgical gesture can detect unsafe events. Our experiments using data from two surgical platforms show that the proposed system can detect unsafe events caused by accidental or malicious faults within an average reaction time window of 1,693 milliseconds and F1 score of 0.88 and human errors within an average reaction time window of 57 milliseconds and F1 score of 0.76.",
                "paper_link": "https://www.semanticscholar.org/paper/a51f37e8f87aabe13a35f93510819d62d9f5c123"
            },
            {
                "title": "Safety-critical cyber-physical attacks: Analysis, detection, and mitigation",
                "abstract": "Today's cyber-physical systems (CPSs) can have very different characteristics in terms of control algorithms, configurations, underlying infrastructure, communication protocols, and real-time requirements. Despite these variations, they all face the threat of malicious attacks that exploit the vulnerabilities in the cyber domain as footholds to introduce safety violations in the physical processes. In this paper, we focus on a class of attacks that impact the physical processes without introducing anomalies in the cyber domain. We present the common challenges in detecting this type of attacks in the contexts of two very different CPSs (i.e., power grids and surgical robots). In addition, we present a general principle for detecting such cyber-physical attacks, which combine the knowledge of both cyber and physical domains to estimate the adverse consequences of malicious activities in a timely manner.",
                "paper_link": "https://www.semanticscholar.org/paper/23cb7a97fff4b7e44461bab28d212ee7eb148138"
            },
            {
                "title": "Strategic safety-critical attacks against an advanced driver assistance system",
                "abstract": "A growing number of vehicles are being transformed into semi-autonomous vehicles (Level 2 autonomy) by relying on advanced driver assistance systems (ADAS) to improve the driving experience. However, the increasing complexity and connectivity of ADAS expose the vehicles to safety-critical faults and attacks. This paper investigates the resilience of a widely-used ADAS against safety-critical attacks that target the control system at opportune times during different driving scenarios and cause accidents. Experimental results show that our proposed Context-Aware attacks can achieve an 83.4% success rate in causing hazards, 99.7% of which occur without any warnings. These results highlight the intolerance of ADAS to safety-critical attacks and the importance of timely interventions by human drivers or automated recovery mechanisms to prevent accidents.",
                "paper_link": "https://www.semanticscholar.org/paper/259a45f55468b5cdf2333066064a1b5d88af862f"
            },
            {
                "title": "An embedded reconfigurable architecture for patient-specific multi-paramater medical monitoring",
                "abstract": "A robust medical monitoring device should be able to provide intelligent diagnosis based on accurate analysis of physiological parameters in real-time. At the same time, such device must be able to adapt to the characteristics of a specific patient and desired diagnostic needs, and continue to operate even in presence of unexpected artifacts and accidental errors. A reconfigurable architecture is proposed for real-time assessment of individual's health status based on development of a patient-specific health index and online analysis and fusion of multi-parameter physiological signals. This is achieved by static configuration of processing elements and communication blocks in the architecture based on the patient's diagnostic needs. The proposed architecture is prototyped as a single integrated device on an FPGA platform and is evaluated using multi-parameter data from intensive care units (ICUs). Three representative test cases of concurrently analyzing Blood Pressure, Heart Rate, and Electrocardiogram (ECG) data from MIMIC database are presented. The results show the effectiveness of the proposed technique in eliminating false alarms caused by patient movements, monitor noise, or imperfections in the detection schemes.",
                "paper_link": "https://www.semanticscholar.org/paper/e0f74d00effcdfbb7a65e79b03e204be3128e293"
            },
            {
                "title": "An efficient embedded hardware for high accuracy detection of epileptic seizures",
                "abstract": "This paper presents design, implementation and evaluation of an efficient embedded hardware for accurate automated detection of epileptic seizures. Three hardware configurations are proposed and evaluated in terms of accuracy of detection, utilization of hardware resources, and power consumption. The results show that a solution based on combination of the statistical function of variance (for feature extraction) and an artificial neural network (ANN) classifier allows to achieve high detection accuracy (99.18%) with moderate hardware footprint (around 44% of the FPGA resources). Furthermore, use of algorithmic and architectural optimization techniques (reduction in precision of the fixed-point number representation and reuse of hardware components) allows reducing hardware footprint by a factor of 4.4 and power consumption by a factor of 2.7 as compared with an un-optimized hardware configuration. High accuracy, real-time detection, simplicity, power efficiency and small hardware footprint make our approach a good candidate for embedded epileptic seizure detection implementation.",
                "paper_link": "https://www.semanticscholar.org/paper/2c118260d1a318b0519aee847efee6c7ba1849a6"
            },
            {
                "title": "Data-driven Design of Context-aware Monitors for Hazard Prediction in Artificial Pancreas Systems",
                "abstract": "Medical Cyber-physical Systems (MCPS) are vulnerable to accidental or malicious faults that can target their controllers and cause safety hazards and harm to patients. This paper proposes a combined model and data-driven approach for designing context-aware monitors that can detect early signs of hazards and mitigate them in MCPS. We present a framework for formal specification of unsafe system context using Signal Temporal Logic (STL) combined with an optimization method for patient-specific refinement of STL formulas based on real or simulated faulty data from the closed-loop system for the generation of monitor logic. We evaluate our approach in simulation using two state-of-the-art closed-loop Artificial Pancreas Systems (APS). The results show the context-aware monitor achieves up to 1.4 times increase in average hazard prediction accuracy (F1score) over several baseline monitors, reduces false-positive and false-negative rates, and enables hazard mitigation with a 54% success rate while decreasing the average risk for patients.",
                "paper_link": "https://www.semanticscholar.org/paper/eb6bfe875b373607cb9ed52daa86f085b95d7ac6"
            },
            {
                "title": "Analysis of Cyber-Security Vulnerabilities of Interconnected Medical Devices",
                "abstract": "With advances in sensing, networking, and computing, smart medical devices have been widely deployed in various clinical settings. However, cyber attacks on hospital networks and critical medical devices are serious threats to patient safety, security, and privacy. This paper studies the cyber-security attacks that target hospital networks and other interconnected clinical environments. Our goal is to characterize threat models in such environments by studying the public data from vulnerability databases on medical devices and reports on real attacks targeted at hospital networks. We use a keyword-based approach to identify security reports on medical devices. We summarize our observations from the analysis of the vulnerability reports and provide insights into the types and impacts of vulnerabilities.",
                "paper_link": "https://www.semanticscholar.org/paper/5a2d8208d65d07ff6899f230a1eedf784ad20327"
            },
            {
                "title": "CognitiveEMS: A Cognitive Assistant System for Emergency Medical Services",
                "abstract": "This paper presents our preliminary results on development of a Cognitive assistant system for Emergency Medical Services (CognitiveEMS) that aims to improve situational awareness and safety of first responders. CognitiveEMS integrates a suite of smart wearable sensors, devices, and analytics for real-time collection and analysis of in-situ data from incident scene and delivering dynamic data-driven insights to responders on the most effective response actions to take. We present the overall architecture of CognitiveEMS pipeline for processing information collected from the responder, which includes stages for converting speech to text, extracting medical and EMS protocol specific concepts, and modeling and execution of an EMS protocol. The performance of the pipeline is evaluated in both noise-free and noisy incident environments. The experiments are conducted using two types of publicly-available real EMS data: short radio calls and post-incident patient care reports. Three different noise profiles are considered for simulating the noisy environments: cafeteria, people talking, and emergency sirens. Noise was artificially added at 3 intensity levels of low, medium, and high to pre-recorded audio data. The results show that the i) state-of-the-art speech recognition tools such as Google Speech API are quite robust to low and medium noise intensities; ii) in the presence of high noise levels, the overall recall rate in medical concept annotation is reduced; and iii) the effect of noise often propagates to the final decision making stage and results in generating misleading feedback to responders.",
                "paper_link": "https://www.semanticscholar.org/paper/f9ba4ffd63a931d31ff842db2bb15828c0623972"
            },
            {
                "title": "Towards a cognitive assistant system for emergency response",
                "abstract": "This abstract presents our preliminary results on development of a cognitive assistant system for emergency response that aims to improve situational awareness and safety of first responders. This system integrates a suite of smart wearable sensors, devices, and analytics for real-time collection and analysis of in-situ data from incident scene and providing dynamic data-driven insights to responders on the most effective response actions to take.",
                "paper_link": "https://www.semanticscholar.org/paper/1de827b55e052e8b350005cd4216816b9db556f4"
            },
            {
                "title": "Analysis of Executional and Procedural Errors in Dry-lab Robotic Surgery Experiments",
                "abstract": "Analysing kinematic and video data can help identify potentially erroneous motions that lead to sub\u2010optimal surgeon performance and safety\u2010critical events in robot\u2010assisted surgery.",
                "paper_link": "https://www.semanticscholar.org/paper/36ed4c375786b5b31c3307b336a4ec878b0e8248"
            }
        ]
    },
    {
        "Professor": "Panagiotis Apostolellis",
        "Papers": [
            {
                "title": "Evaluating the effects of orchestrated, game-based learning in virtual environments for informal education",
                "abstract": "In informal learning spaces employing digital content, such as museums, visitors often do not get adequate exposure to content, or they passively receive instruction offered by a museum docent to the whole group. This research aims to identify which elements of co-located group collaboration, virtual environments, and serious games can be leveraged for an enhanced museum learning and entertaining experience. We developed C-OLiVE, an interactive virtual environment supporting tripartite group collaboration, which we used to explore our hypothesis that synchronous, co-located, group collaboration will afford greater learning compared to conventional approaches. In an empirical study, we found some evidence supporting this hypothesis, taking into consideration other factors such as game experience and social presence. Students participating in the three-player condition demonstrated a better understanding of the collaborative tasks compared to their single-player counterparts. We discuss these results and outline future studies using the same virtual environment.",
                "paper_link": "https://www.semanticscholar.org/paper/66059d289fc3d2105171c640d29f57f28058b2c0"
            },
            {
                "title": "RaBit EscAPE: a board game for computational thinking",
                "abstract": "Computational thinking (CT) is increasingly seen as a core literacy skill for the modern world on par with the longestablished skills of reading, writing, and arithmetic. To promote the learning of CT at a young age we capitalized on children's interest in play. We designed RabBit EscApe, a board game that challenges children, ages 610, to orient tangible, magnetized manipulatives to complete or create paths. We also ran an informal study to investigate the effectiveness of the game in fostering children's problemsolving capacity during collaborative game play. We used the results to inform our instructional interaction design that we think will better support the learning activities and help children hone the involved CT skills. Overall, we believe in the power of such games to challenge children to grow their understanding of CT in a focused and engaging activity.",
                "paper_link": "https://www.semanticscholar.org/paper/2dd8e05a3e4237cbb0fdfb01aee921dc2b48dd0a"
            },
            {
                "title": "Supporting social engagement for young audiences with serious games and virtual environments in museums",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/525f8b6bd0aa467f08d17fa631848ac4f0f9ec74"
            },
            {
                "title": "Poster: Exploring the integrality and separability of the Leap Motion Controller for direct manipulation 3D interaction",
                "abstract": "In this paper, we evaluate a new generation 5DOF tracker, the Leap Motion Controller, and the mouse for performing integral and separable 3D manipulation tasks in a stage lighting application. Based on the hypothesis that the Leap would outperform the mouse for the integral tasks of position and rotation while the mouse will prove better for the separable tasks of position and light intensity, as shown in a similar study by Jacob et al. [3], we designed an experiment to test this claim. Our findings did not support our hypothesis with the mouse performing significantly better both in terms of completion time and angular and position errors.",
                "paper_link": "https://www.semanticscholar.org/paper/6f09aa38bf26363fe520a33c581cc8e427776634"
            },
            {
                "title": "Small group learning with games in museums: effects of interactivity as mediated by cultural differences",
                "abstract": "Museums are rich and complex learning experiences, using a variety of interactive approaches to engage their audiences. However, the largely unstructured nature of free-choice learning calls for alternative approaches that can effectively engage groups of school age students with diverse cultural backgrounds. In this paper, we present our findings from a recent study in a museum in Greece, where triads of students had to learn about olive oil production using a game enabling different levels of interactivity and collaboration. We found that facilitation by an expert guide led to greater learning gains as compared to students playing alone, with one or three simultaneous game controllers. We also compared these results with a previous controlled experiment conducted in the US with middle school students, using the same game but without the ecologically valid facilitation. Drawing ideas from sociocultural and cognitive theories we interpret the contradictory findings, identifying the impact of culture on their (social) interactions, their subjective game experience, and eventually learning, in these spaces.",
                "paper_link": "https://www.semanticscholar.org/paper/1d441cd33410914ae5aad36e1d12ac0221e8dd70"
            },
            {
                "title": "Reducing risk in digital self-control tools: Design patterns and prototype",
                "abstract": "Many users take advantage of digital self-control tools to self-regulate their device usage through interventions such as timers and lockout mechanisms. One of the major challenges faced by these tools is the user reacting against their self-imposed constraints and abandoning the tool. Although lower-risk interventions would reduce the likelihood of abandonment, previous research on digital self-control tools has left this area of study relatively unexplored. In response, this paper contributes two foundational principles relating risk and effectiveness; four widely applicable novel design patterns for reducing risk of abandonment of digital self-control tools (continuously variable interventions, anti-aging design, obligatory bundling of interventions, and intermediary control systems); and a prototype digital self-control tool that implements these four low-risk design patterns.",
                "paper_link": "https://www.semanticscholar.org/paper/bb87e90e834fc5aeee5b11af793dee4d9520c562"
            },
            {
                "title": "Audience involvement and agency in digital games: Effects on learning, game experience, and social presence",
                "abstract": "One of the most popular audiences of museums is classroom-size groups of students, in the context of school field trips. However, students do not get adequate involvement during interactive group experiences, which might affect their impression and learning gained from the visit. In this paper, we present our findings from a recent study in middle schools, where 507 students were engaged with their class in a learning game about olive oil production. We had two players directly control the game and varied the level of involvement of the audience (the rest of the class), using iPads. We found that higher involvement in the game afforded greater retention of information after two days, while there was no difference after one day. Also, students with direct agency in the game revealed greater learning gains than the audience members. Results about the impact of socioeconomic status and social interactions on learning are reported, along with the most important design implications.",
                "paper_link": "https://www.semanticscholar.org/paper/2d8b0ba0d816dae911f0f5f968e4ec82f6f2c0fb"
            },
            {
                "title": "Audience interactivity as leverage for effective learning in gaming environments for dome theaters",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/03471dff59814607a74de8da4baac778e5ff1de7"
            },
            {
                "title": "Exploring the value of audience collaboration and game design in immersive virtual learning environments",
                "abstract": "Informal learning in public spaces like museums and location-based entertainment venues is increasingly popular during the last years. Especially in technology-enhanced museums such properties as Virtual Reality, Game-Based Collaboration, and Immersive Displays are considered to bear significant educational value. After extensive literature review we have come to the conclusion that little to no research has been carried out on the learning outcomes of these powerful properties. Thus, the scope of our research is to investigate the learning efficacy of an integrated schema of audience collaboration and game design in immersive virtual reality environments. In order to achieve this, we are going to build and evaluate a theoretical framework that supports collaboration within an audience of 9--14 years old children.",
                "paper_link": "https://www.semanticscholar.org/paper/691019a1bee0f29778308eb0551b3a14dc1af140"
            },
            {
                "title": "Educational virtual environments for digital dome display systems with audience participation",
                "abstract": "The COVID-19 pandemic has highlighted for the public and policy makers the size and implications of digital inequalities in the UK and globally. The \u2018digital divide\u2019 is not a new idea. It has been explored since the 1970s as telecommunications, computing, ICT, or information \u201cdivides\u201d. More recently research has focused on digital exclusion and inclusion and the post pandemic buzz-words are digital and data poverty. This lecture will explore the nature of digital inequalities across access, skills, affordability and motivation. It will then examine and consider the implications of these inequalities. These implications cut across and are intertwined with citizens economic, social and cultural lives and opportunities. The talk builds on two decades of empirical research \u2013 both quantitative and qualitative \u2013 conducted in collaboration with regional, national and international stakeholders, charities and governments. The lecture will conclude with thoughts on the key next steps for research, practitioners and policy makers. In this talk, Tammy will highlight the potential of hyperlocal community settings \u2013 neighborhoods, community centers, after school programs \u2013 for promoting STEM learning in everyday life. Highlighting findings from two studies, Science Everywhere and Data Everyday, Clegg will illustrate ways STEM learning can be connected to issues and topics relevant to community members\u2019 goals (e.g., cooking, sports). First, in Science Everywhere, with colleagues, Tammy has spent over six years designing, developing and situating a social media app, large community displays, and life-relevant science learning experiences for youth in two urban, resource-constrained neighborhood settings. From this project, she will highlight case studies of child and adult community members that illuminate the role of the Science Everywhere socio-technical system and hyperlocal context for influencing science disposition shifts in communities. Second, in the Data Everyday project, Clegg\u2019s research team is seeking to understand the opportunities for data literacy development within NCAA Division I sports. Drawing on an interview study with Division I athletes and athletics staff members across sports, she will highlight key tensions that reveal opportunities and challenges for situating data literacy development in the context of community context of elite athletics. Through these studies, Tammy will describe ways such community contexts can, over time, reshape community dispositions in ways that fuel dynamic new community-drive STEM learning experiences and broaden our conceptualizations of STEM learning. have wide to investigate a using 100 the lowest 50% of the it difficult to they Initial results indicate that students are engaged by the concept of virtual visits and that they can widen participation in extra-curricular activities. Furthermore, virtual visits may be an alternative promotional strategy for museums to increase visitor numbers. The overall aim of Teach4EDU is to enable the creation of an environment that supports implementation of new Education 4.0 learning and teaching approaches in Computer Science (CS). A systematic literature review was carried out, focusing on three research questions. RQ1: Which pedagogic approaches are used to support the teaching of CS?; RQ2: Which of these approaches align with Education 4.0?; RQ3: What skills and competences do HE educators require in order to align CS with Education 4.0? Our literature search identified 66 articles. Perhaps surprisingly none of the articles explicitly mentions \u201cEducation 4.0\u201d. The most common Education 4.0 characteristic was \u201c5) students will be exposed to more hands -on learning through field experience\u201d (73%), followed by \u201c9) students will become more independent in their own learning\u201d (67%), \u201c4) students will be exposed to more project - based learning\u201d (61%). A cluster analysis indicated a three-cluster solution: 1) EDU 4.0 light (n = 18), 2) project-based/hands-on learning (n = 22), and 3) full EDU 4.0 (n = 26). In EDU 4.0 light studies teachers mostly focussed on more independent learning (61%), learning anytime anywhere (44%), and personalised learning (39%). The second cluster had a strong focus on project-based (86%) and hands-on learning (86%), with relatively limited focus on choice how to learn (5%), personalised learning (5%), and learning anytime anywhere (18%). The third and final cluster, full EDU 4.0, was strongly focussed on hands-on learning (100%), becoming more independent (96%), personalised learning (85%), learning anytime anywhere (77%) and choice how to learn (77%). Overall, while there are some engaging and diverse practices in CS and Education 4.0 in Europe, it seems that relative to other countries (e.g., USA) more work needs to be done. This is one of the aims of the TEACH4EDU project will address. During June to August 2020 over 500 teachers and education professionals from across Africa took part in a supported pathway through the OpenLearn Take Your Teaching On-line Course offered in the Pathways for Learning project. Survey data suggested that e-assessment is the area that participants find the most problematic to design for in a digital environment and desired more understanding of this topic. A poll was conducted during the conclusion webinar and participants rated assessment as a key learning outcome for future courses. This project is therefore the next step in addressing this pressing need of ACDE members and African educators following an initial introduction to e-assessment concepts as part of the Pathways Tertiary Educator programme (supported in phase 1 Covid response). The current project brings together OU and members of the network created with ACDE in the Pathways project to co-design and co-develop practical teaching activities and curricula on e-assessment, deliver and test core components of a course, where possible using existing OER and webinars. The applied co-design approach of an e-assessment course in this project is a response to the practical challenges faced by leaders and innovators in African Higher Education Institutions as they rapidly respond to moving their Universities\u2019 teaching and learning online following the COVID-19 pandemic. This approach will directly identify and address immediate concerns and learning needs, and can then be used and adapted with HEIs world-wide as essential components in a programme of courses for a wider audience. It would be a springboard to the co-creation of an expanding range of courses at different levels (from free to paid) covering topics determined by participants. The move to remote online examinations during the COVID-19 pandemic has led to the take-up of online examination proctoring systems. In this presentation, we will present a review of ethical issues and controversies around online proctoring. We will frame the problem of online proctoring in our current research programme of ethical use, design and evaluation of educational technologies. Through a series of case studies and assessment strategies, we will discuss how assessment can be designed in an ethical manner, for promoting academic integrity, for reducing academic misconduct, and to discourage contract cheating and use of essay mills. We will highlight the ethical considerations and pedagogical advantages of designing \u2018authentic\u2019 assessment to replac e more traditional or essay-based approaches. We will describe how educational technologies could become an essential part of an ethical assessment toolkit. Twitter is an online Social Networking Service that allows users to \u2018tweet\u2019 out messages. As all tweets are public, hashtags and the search facility allow users to find people with similar interests. Distance learners often have less of a student identity and network and Twitter may allow students to \u2018meet\u2019 and build their own support network. My research aims to investigate how distanc e learners use Twitter including the networks they create, how the communication they take part in fits within a Community of Inquiry (CoI) model and the impact this communication has on feelings of identity and motivation. The poster outlines my 3 RQs (given below) as well as giving details on my methodology, data collection and analysis methods: The benefits of this research include filling a gap in existing research into how distance learners use Twitter, the language used by students in tweets and the voluntary use of Twitter by students. The research is also beneficial due to the growing importance of Social Presence online and, in particular, as Covid restrictions lead to more distance learning and use of Social Media. HEIs need to know how students build support networks on Twitter and how best to interact and facilitate. In this talk we will share the progress and setbacks we experienced in seeking to model confusion based on audio recorded responses (and transcripts of audio) from simulations about equity focused problems of practice in K-12 computer science teaching. The aim of the presentation is to illustrate a chain of evidence (which ranges from null findings to significant results) that helped shape the direction of detecting and responding to confusion in equity-focused simulations. The coordination of multiple investigations has led to our current understanding with the first set of results focused on detection of confusion and the second set of results focused on responding to confusion. We first review and report results from four approaches explored to detect confusion: 1) transcripts of audio recordings, 2) prosodic features of audio files, 3) self-evaluation by participants, 4) and researcher coding. Second we report results focused on responding to confusion considering both how confusion related to moments where teachers would provide students with support as well as the potential for technology to provide dynamic support. This talk serves two purposes: First it illustrates int",
                "paper_link": "https://www.semanticscholar.org/paper/affa8e884501e0f8ff59ec53631998839e9da117"
            },
            {
                "title": "\" Pump that press!\" design evaluation of audience interaction using collaborative digital and physical games",
                "abstract": "Museums have been long recognized as legitimate places for out-of-school learning, offering unique experiences for large student groups. However, the challenges of assessing learning and engagement outcomes using technological interventions for large audiences, often undermine the value of these practices. To address the need for understanding how learning unfolds within the context of a museum visit as an interplay between the people, the interactives, and the mediators, we conducted a design evaluation in a science museum. Our focus was understanding the impact of audience interaction on learning and engagement during collaborative games for large student groups. For the purposes of this case study we also designed a physical board game having the dual purpose of a learning activity and an assessment tool. We present the results of our observations and discourse analysis, and discuss them in the light of our previous study in public schools.",
                "paper_link": "https://www.semanticscholar.org/paper/004d8122487c4fea5ebc8d1dd0e028ccc4d92572"
            },
            {
                "title": "A gaming interface using body gestures for collaborative navigation",
                "abstract": "We designed a collaborative navigation interface in which one user (the controller) guides a second, remote user (the navigator) through a 3D game environment. Both sides of the interface use commercially available gaming hardware. The controller uses full-body gestures to send navigation cues to the navigator, who travels through the environment using typical 3D gaming controls. In an exploratory study, all participants completed the game scenario, and they viewed the system as usable and fun overall, although the limited communication provided by the system was somewhat of a hindrance to collaboration. Navigators, who were led to believe they were being guided by an intelligent system, did not guess that guidance was provided by another human, and felt that an \"intelligent system\" would typically be more consistent than a human being.",
                "paper_link": "https://www.semanticscholar.org/paper/489ca89212a4ee5ca79963bad1da67c8657c60b0"
            },
            {
                "title": "Exploring Learning through Audience Interaction in Virtual Reality Dome Theaters",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/4f8d0cbdcbb5018c328ebb93153914419f9a1738"
            },
            {
                "title": "Co-located collaborative play in virtual environments for group learning in museums",
                "abstract": "Having witnessed the unexplored potential of co-located group collaboration in contemporary museums, the proposed research aims to identify which elements of collaborative virtual environments and serious games can be leveraged for an enhanced learning experience. Our hypothesis is that synchronous, co-located, group collaboration will afford greater learning compared to the conventional approaches. We developed C-OLiVE, an interactive virtual learning environment supporting tripartite group collaboration, which we are using as a test bed to respond to our research questions. In this paper, we discuss the proposed research which involves building and testing a conceptual framework and also suggesting a list of design guidelines for anyone interested in developing virtual environments for informal learning spaces.",
                "paper_link": "https://www.semanticscholar.org/paper/7e1ff967349e3dd901d2f34b10cb0294d3a6de29"
            },
            {
                "title": "Creating effective project-based courses: personal relevance and its relations to successful group work",
                "abstract": "ABSTRACT Group projects are expected in contemporary engineering curricula, and yet they often pose a challenge to students and instructors alike. Could making projects personally relevant help? The present study created and tested a conceptual framework regarding the impact of personal relevance on groupwork in a project-based learning (PBL) course. We examined how measures of personal relevance (PR), both at the course level (value, interest in specialisation) and specifically regarding projects (interest and investment in the project, and contribution to the project idea) relate to students\u2019 expectancy, group connectedness, team dynamics (effectiveness, conflict, satisfaction, interdependence, and cohesiveness), and perceived effort; whether PR differs based on students\u2019 gender, academic year, or time of the semester; and whether PR predicts students\u2019 project performance. Seventy-one undergraduates in a project-based computer science course at a large public US university completed surveys assessing these constructs at five timepoints during the semester. Our findings suggest PR is related to positive outcomes in PBL courses, with interest and investment predicting an increase in the project grade. Similarly, gender predicted project grade with female students having significantly higher scores overall, above and beyond other measures. We discuss implications for creating project-based courses in higher education engineering courses.",
                "paper_link": "https://www.semanticscholar.org/paper/a8735051559fe497de73fd983303470b7cfa3976"
            },
            {
                "title": "Crafting an effective portfolio in user experience design",
                "abstract": "In careers involving user interface/user experience (UI/UX) design, an effective portfolio is key for showcasing one\u2019s skills and knowledge to potential employers and collaborators. In this workshop, participants will gain insights into the fundamentals of UI/UX design, elements of effective portfolio design, and tools available to create a portfolio. This will be a highly interactive session as participants will interact with faculty and graduate students as well as with each other. Participants will be given the opportunity to have their pre-existing portfolio reviewed by the faculty and graduate student presenters in addition to members of the Human Factors and Ergonomics Society student chapter at the University of Virginia.",
                "paper_link": "https://www.semanticscholar.org/paper/2fbaab0c2422335463f1180e5566a94285facda5"
            },
            {
                "title": "A Student Engagement Evaluation Methodology Inspired from Usability Engineering for Extracting Course Design Requirements",
                "abstract": "Measuring student engagement inside the classroom and developing techniques for improving it has been traditionally very challenging for educators. This research paper describes a student engagement evaluation model that combined data from three sources: in-class observations using the Behavioral Engagement Related to Instruction (BERI) protocol, one-to-one student interviews, and anonymous online surveys. We tested this model on a higher-level elective Computer Science class with 134 students, focusing on user experience (UX) design. We used the exact same usability engineering process that students employed in the course to design software products on assessing the course\u2019s effectiveness, with students acting as users and the class being the system under development. We present our exact methodology of data-driven analysis borrowed from UX design, which combines inductive reasoning (similar to narrative analysis or inductive coding) with deductive reasoning (similar to content analysis of qualitative data) used in social science research. Our paper includes a sample of the extracted requirements, similar to software non-functional requirements, which we used to redesign the course for the next semester. The results of this case study showed that the application of this mixed methods type of analysis informed by user-centered design of software systems was effective as a surrogate model of student-centered instructional design. Concluding, we extrapolate the lessons learned from this process and the significant implications we believe our industry-inspired methodology can have for engineering educators, in terms of evaluating student engagement in college classrooms.",
                "paper_link": "https://www.semanticscholar.org/paper/614995503e6fd8de30696d1548d0f395845cb33f"
            },
            {
                "title": "Evaluating group interaction and engagement using virtual environments and serious games for student audiences in informal learning settings",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/feb963740f73dfbab3b64a2291d2928affb64fbc"
            },
            {
                "title": "C-OLiVE: Group co-located interaction in VEs for contextual learning",
                "abstract": "In informal learning spaces employing digital content, such as museums, visitors either do not get adequate exposure to content or get information through passive instruction offered by a museum docent to the whole group. This research aims to identify which elements of co-located group collaboration, virtual environments, and serious games can be leveraged for an enhanced learning experience. Our hypothesis is that synchronous, co-located, group collaboration will afford greater learning compared to conventional approaches. We developed C-OLiVE, an interactive virtual learning environment supporting tripartite group collaboration, which we will use as a testbed to respond to our research questions. In this paper, we discuss our proposed research, which involves exploring some benefits of the involved technologies and proposing a list of design guidelines for anyone interested to exploit them in developing virtual environments for informal learning spaces.",
                "paper_link": "https://www.semanticscholar.org/paper/6bca36bfd76321286c02c4e9fa92efece2d3e44b"
            },
            {
                "title": "Designing for Meaningful Interactions and Digital Wellbeing",
                "abstract": "In the contemporary attention economy, tech companies design the interfaces of their digital platforms by adopting attention-capture dark patterns to drive their behavior and maximize time spent and daily visits. Two popular examples are viral recommendations and content autoplay on social networks. As these patterns exploit people\u2019s psychological vulnerabilities and may contribute to technology overuse and problematic behaviors, there is the need of promoting the design of technology that better align with people\u2019s digital wellbeing. This workshop seeks to advance this timely and urgent need, by inviting researchers and practitioners in interdisciplinary domains to engage in conversation around the design of interfaces that allow people to take advantage of digital platforms in a meaningful and conscious way.",
                "paper_link": "https://www.semanticscholar.org/paper/fd67385cd9b116127e2402233a1d9e0d1c3183aa"
            }
        ]
    },
    {
        "Professor": "Christopher L. Barrett",
        "Papers": [
            {
                "title": "Episimdemics: an efficient algorithm for simulating the spread of infectious disease over large realistic social networks",
                "abstract": "Preventing and controlling outbreaks of infectious diseases such as pandemic influenza is a top public health priority. We describe EpiSimdemics - a scalable parallel algorithm to simulate the spread of contagion in large, realistic social contact networks using individual-based models. EpiSimdemics is an interaction-based simulation of a certain class of stochastic reaction-diffusion processes. Straightforward simulations of such process do not scale well, limiting the use of individual-based models to very small populations. EpiSimdemics is specifically designed to scale to social networks with 100 million individuals. The scaling is obtained by exploiting the semantics of disease evolution and disease propagation in large networks. We evaluate an MPI-based parallel implementation of EpiSimdemics on a mid-sized HPC system, demonstrating that EpiSimdemics scales well. EpiSimdemics has been used in numerous sponsor defined case studies targeted at policy planning and course of action analysis, demonstrating the usefulness of EpiSimdemics in practical situations.",
                "paper_link": "https://www.semanticscholar.org/paper/6c73b2f8e15c69822f37fe20cc6413db2234b203"
            },
            {
                "title": "Extended mapping and exploration of the vanadium dioxide stress-temperature phase diagram",
                "abstract": "Single-crystal micro- and nanomaterials often exhibit higher yield strength than their bulk counterparts. This enhancement is widely recognized in structural materials but is rarely exploited to probe fundamental physics of electronic materials. Vanadium dioxide exhibits coupled electronic and structural phase transitions that involve different structures existing at different strain states. Full understanding of the driving mechanism of these coupled transitions necessitates concurrent structural and electrical measurements over a wide phase space. Taking advantages of the superior mechanical property of micro/nanocrystals of VO(2), we map and explore its stress-temperature phase diagram over a phase space that is more than an order of magnitude broader than previously attained. New structural and electronic aspects were observed crossing phase boundaries at high-strain states. Our work shows that the actively tuning strain in micro/nanoscale electronic materials provides an effective route to investigate their fundamental properties beyond what can be accessed in their bulk counterpart.",
                "paper_link": "https://www.semanticscholar.org/paper/f9187595938c7fdbf98c9066d6abcffc736bf37c"
            },
            {
                "title": "Generation and analysis of large synthetic social contact networks",
                "abstract": "We describe \u201cfirst principles\u201d based methods for developing synthetic urban and national scale social contact networks. Unlike simple random graph techniques, these methods use real world data sources and combine them with behavioral and social theories to synthesize networks. We develop a synthetic population for the United States modeling every individual in the population including household structure, demographics and a 24-hour activity sequence. The process involves collecting and manipulating public and proprietary data sets integrated into a common architecture for data exchange and then using these data sets to generate new relations. A social contact network is derived from the synthetic population based on physical co-location of interacting persons. We use graph measures to compare and contrast the structural characteristics of the social networks that span different urban regions. We then simulate diffusion processes on these networks and analyze similarities and differences in the structure of the networks.",
                "paper_link": "https://www.semanticscholar.org/paper/06ed109b9c92b9a87e81ec98fc75fbe0ef7ca86f"
            },
            {
                "title": "Commentary on Ferguson, et al.,\u201cImpact of non-pharmaceutical interventions (NPIs) to reduce COVID-19 mortality and healthcare demand\u201d",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/b1ce0a1cd9cb416b95a2ca2bd955b6a3be8592b0"
            },
            {
                "title": "The distance-2 matching problem and its relationship to the MAC-layer capacity of ad hoc wireless networks",
                "abstract": "We consider the problem of determining the maximum capacity of the media access (MAC) layer in wireless ad hoc networks. Due to spatial contention for the shared wireless medium, not all nodes can concurrently transmit packets to each other in these networks. The maximum number of possible concurrent transmissions is, therefore, an estimate of the maximum network capacity, and depends on the MAC protocol being used. We show that for a large class of MAC protocols based on virtual carrier sensing using RTS/CTS messages, which includes the popular IEEE 802.11 standard, this problem may be modeled as a maximum Distance-2 matching ( D2EMIS) in the underlying wireless network: Given a graph G(V,E), find a set of edges E'/spl sube/E such that no two edges in E' are connected by another edge in E. D2EMIS is NP-complete. Our primary goal is to show that it can be approximated efficiently in networks that arise in practice. We do this by focusing on an admittedly simplistic, yet natural, graph-theoretic model for ad hoc wireless networks based on disk graphs, where a node can reach all other nodes within some distance (nodes may have unequal reach distances). We show that our approximation yields good capacity bounds. Our work is the first attempt at characterizing an important \"maximum\" measure of wireless network capacity, and can be used to shed light on previous topology formation protocols like Span and GAF that attempt to produce \"good\" or \"capacity-preserving\" topologies, while allowing nodes to alternate between sleep and awake states. Our work shows an efficient way to compute an upper bound on maximum wireless network capacity, thereby allowing topology formation algorithms to determine how close they are to optimal. We also outline a distributed algorithm for the problem for unit disk graphs, and briefly discuss extensions of our results to: 1) different node interference models; 2) directional antennas; and 3) other transceiver connectivity structures besides disk graphs.",
                "paper_link": "https://www.semanticscholar.org/paper/f392d0c33b12f655dd109774ea1e054fecea6235"
            },
            {
                "title": "Elements of a theory of simulation II: sequential dynamical systems",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/83fcc2718c019b5faa316f2ba8925b8a1c0e9ba3"
            },
            {
                "title": "Parametric probabilistic sensor network routing",
                "abstract": "Motivated by realistic sensor network scenarios that have misinformed nodes and variable network topologies, we propose a fundamentally different approach to routing that combines the best features of limited-flooding and information-sensitive path-finding protocols into a reliable, low-power method that can make delivery guarantees independent of parameter values or information noise levels. We introduce Parametric Probabilistic Sensor Network Routing Protocols, a family of light-weight and robust multi-path routing protocols for sensor networks in which an intermediate sensor decides to forward a message with a probability that depends on various parameters, such as the distance of the sensor to the destination, the distance of the source sensor to the destination, or the number of hops a packet has already traveled. We propose two protocol variants of this family and compare the new methods to other probabilistic and deterministic protocols, namely constant-probability gossiping, uncontrolled flooding, random wandering, shortest path routing (and a variation), and a load-spreading shortest-path protocol inspired by [Servetto, Barrenechea, 2002]. We consider sensor networks where a sensor's knowledge of the local or global information is uncertain (parametrically noised) due to sensor mobility, and investigate the trade-off between robustness of the protocol as measured by quality of service (in particular, successful delivery rate and delivery lag) and use of resources (total network load). Our results show that the multi-path protocols are less sensitive to misinformation, and suggest that in the presence of noisy data, a limited flooding strategy will actually perform better and use fewer resources than an attempted single-path routing strategy, with the Parametric Probabilistic Sensor Network Routing Protocols outperforming other protocols. Our results also suggest that protocols using network information perform better than protocols that do not, even in the presence of strong noise.",
                "paper_link": "https://www.semanticscholar.org/paper/dee5b845fab7a0ea93d5bc1c13151050e1375eb3"
            },
            {
                "title": "Elements of a theory of computer simulation I: sequential CA over random graphs",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/e25f1f404184906edd128833f03c7a218a25164d"
            },
            {
                "title": "A first look at the empirical relation between spot and futures electricity prices in the United States",
                "abstract": "In this article we investigate the statistical properties of wholesale electricity spot and futures prices traded on the New York Mercantile Exchange for delivery at the California\u2013Oregon Border. Using daily data for the years 1998 and 1999, we find that many of the characteristics of the electricity market can be viewed to be broadly consistent with efficient markets. The futures risk premium for 6-month futures contracts is estimated to be 0.1328% per day or about 4% per month. Using a GARCH specification, we estimate minimum variance hedge ratios for electricity futures. Finally, we study the dynamic relation between spot and futures prices using an Exponential GARCH model and between the spot and futures returns series using a vector autoregression. \u00a9 2003 Wiley Periodicals, Inc. Jrl Fut Mark 23:931\u2013955, 2003",
                "paper_link": "https://www.semanticscholar.org/paper/0e46ba4cd62673201465b3a582747a198d664368"
            },
            {
                "title": "TRANSIMS traffic flow characteristics",
                "abstract": "Knowledge of fundamental traffic flow characteristics of traffi c simulation models is an essential requirement when using these models for the planning, design, and operation of transportation systems. In this paper we discuss the following: a description of how features relevant to traffic flow are currently under implementation in the TRANSIMS microsimulation, a proposition for standardized traffic flow tests for traffic simulation models, a nd the results of these tests for two different versions of the TRANSIMS microsimulation.",
                "paper_link": "https://www.semanticscholar.org/paper/36944aa4e34883d3e4cd4bc74cae0590b08a193e"
            },
            {
                "title": "Large kinetic asymmetry in the metal-insulator transition nucleated at localized and extended defects",
                "abstract": "Superheating and supercooling effects are characteristic kinetic processes in first-order phase transitions, and asymmetry between them is widely observed. In materials where electronic and structural degrees of freedom are coupled, a wide, asymmetric hysteresis may occur in the transition between electronic phases. Structural defects are known to seed heterogeneous nucleation of the phase transition, hence reduce the degree of superheating and supercooling. Here we show that in the metal-insulator transition of single-crystal VO2, a large kinetic asymmetry arises from the distinct spatial extension and distribution of two basic types of crystal defects: point defects and twin walls. Nanometer-thick twin walls are constantly consumed but regenerated during the transition to the metal phase, serving as dynamical heterogeneous nucleation seeds and eliminating superheating. On the other hand, the transition back to the insulator phase relies on nucleation at point defects because twinning is structurally forbidden in the metal phase, leading to a large supercooling. By controlling the formation, location, and extinction of these defects, the kinetics of the phase transition might be externally modulated, offering possible routes toward unique memory and logic device technologies.",
                "paper_link": "https://www.semanticscholar.org/paper/60c56a0990db0477c1149c672662c6b5427e4837"
            },
            {
                "title": "Using microsimulation feedback for trip adaptation for realistic traffic in Dallas",
                "abstract": "This paper presents a day-to-day re-routing relaxation approach for traffic simulations. Starting from an initial planset for the routes, the route- based microsimulation is executed. The result of the microsimulation is fed into a re-router, which re-routes a certain percentage of all trips. This approach makes the traffic patterns in the microsimulation much more reasonable. Further, it is shown that the method described in this paper can lead to strong oscillations in the solutions.",
                "paper_link": "https://www.semanticscholar.org/paper/45e6d8c05ff7bcd174531ccebf10ed2f84a1cca3"
            },
            {
                "title": "TRANSIMS: Transportation analysis simulation system",
                "abstract": "This paper summarizes the TRansportation ANalysis and SIMulation System (TRANSIMS) Project, the system`s major modules, and the project`s near-term plans. TRANSIMS will employ advanced computational and analytical techniques to create an integrated regional transportation systems analysis environment. The simulation environment will include a regional population of individual travelers and freight loads with travel activities and plans, whose individual interactions will be simulated on the transportation system, and whose environmental impact will be determined. We will develop an interim operational capability (IOC) for each major TRANSIMS module during the five-year program. When the IOC is ready, we will complete a specific case study to confirm the IOC features, applicability, and readiness.",
                "paper_link": "https://www.semanticscholar.org/paper/1542055ef1d36f656dd00baa26f5a5f77ce750b3"
            },
            {
                "title": "Elements of a theory of simulation III: Equivalence of SDS",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/9be03500424c958791f0bc4f192b57bb1164ebcd"
            },
            {
                "title": "Complexity of reachability problems for finite discrete dynamical systems",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/deeb1d5952d6b262644fcc299703404282834b41"
            },
            {
                "title": "Strong edge coloring for channel assignment in wireless radio networks",
                "abstract": "We give efficient sequential and distributed approximation algorithms for strong edge coloring graphs modeling wireless networks. Strong edge coloring is equivalent to computing a conflict-free assignment of channels or frequencies to pairwise links between transceivers in the network",
                "paper_link": "https://www.semanticscholar.org/paper/1b02bb9a7b97dd6388f949bc30de5eab2e1f6c81"
            },
            {
                "title": "Population mobility generator and simulator",
                "abstract": "Transportation science today is tasked with predicting the complex mobility needs of individuals, which necessitates the use of advanced mobility and travel demand models. However, the quality of the model outputs depends on the data quality. In transport, data privacy and data availability are two limitations. Therefore, transportation scientists rely increasingly on the usage of synthetic populations. Typically, a synthetic population is generated either on the level of individuals or on the level of households using simulation or machine learning approaches. This paper presents a follow-up work on the existing simulation techniques for the generation of synthetic households, addressing several literature gaps. In existing methodologies, the generation of individuals and their matching into households is done separately, through two sequential processes. Although the marginal distributions of key generated attributes might show a perfect \ufb01t, the \u201ctwo-step\u201d household generator produces unrealistic households. The generation of illogical observations is caused by neglecting the dependencies between individuals while grouping them into households. In order to create realistic households, this paper suggests a \u201csingle-step\u201d household simulator where relationships between individuals are considered simultaneously within the generation process by imposing various statistical constraints. However, as shown in the past, the simulation methods struggle to deliver accurate results in a reasonable time while dealing with high-dimensional datasets. We propose the so-called \u201cDivide and Conquer Gibbs Sampler\u201d that solves this problem by decomposing and parallelizing the generation process based on the level of correlation. This approach increases accuracy and e\ufb03ciency, as highly correlated areas are isolated, enabling a better representation of less probable values. The case study compares the developed approach with state-of-the-art methodologies based on 2015 Swiss census data.",
                "paper_link": "https://www.semanticscholar.org/paper/00696d8deeccb485c8956a9bda27cb21beb8dd2a"
            },
            {
                "title": "Colossal thermal-mechanical actuation via phase transition in single-crystal VO2 microcantilevers",
                "abstract": "The spontaneous strain associated with the structural change in the metal-insulator transition in VO2 is orders of magnitude higher than thermal expansion mismatch used in bimetallic strips. Here we show that this strain can be leveraged to thermally activate bending of crystalline VO2-based bilayer microcantilevers at extremely large curvatures, making them suitable for thermal sensors, energy transducers and actuators with unprecedented sensitivities. The single-crystallinity, deposition conditions, and postdeposition treatments were utilized to control the metal-insulator domain structure along the cantilever, by which we achieved bending curvatures a few hundred times higher than conventional bilayer cantilevers with the same geometry.",
                "paper_link": "https://www.semanticscholar.org/paper/07f4b692d7b548adddd726d4aa3093ce337742b4"
            },
            {
                "title": "Modeling and simulation of large biological, information and socio-technical systems: an interaction based approach",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Gardens of Eden and fixed points in sequential dynamical systems",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            }
        ]
    },
    {
        "Professor": "Madhur Behl",
        "Papers": [
            {
                "title": "Autonomous vehicles on the edge: A survey on autonomous vehicle racing",
                "abstract": "The rising popularity of self-driving cars has led to the emergence of a new research field in recent years: Autonomous racing. Researchers are developing software and hardware for high-performance race vehicles which aim to operate autonomously on the edge of the vehicle\u2019s limits: High speeds, high accelerations, low reaction times, highly uncertain, dynamic, and adversarial environments. This paper represents the first holistic survey that covers the research in the field of autonomous racing. We focus on the field of autonomous racecars only and display the algorithms, methods, and approaches used in the areas of perception, planning, control, and end-to-end learning. Further, with an increasing number of autonomous racing competitions, researchers now have access to high-performance platforms to test and evaluate their autonomy algorithms. This survey presents a comprehensive overview of the current autonomous racing platforms, emphasizing the software-hardware co-evolution to the current stage. Finally, based on additional discussion with leading researchers in the field, we conclude with a summary of open research challenges that will guide future researchers in this field.",
                "paper_link": "https://www.semanticscholar.org/paper/33a675d532a2384338ab978895cf962f0f3fb09f"
            },
            {
                "title": "MLE+ a tool for integrated design and deployment of energy efficient building controls",
                "abstract": "We present MLE+, a tool for energy-efficient building automation design, co-simulation and analysis. The tool leverages the high-fidelity building simulation capabilities of EnergyPlus and the scientific computation and design capabilities of Matlab for controller design. MLE+ facilitates integrated building simulation and controller formulation with integrated support for system identification, control design, optimization, simulation analysis and communication between software applications and building equipment. It provides streamlined workflows, a graphical front-end, and debugging support to help control engineers eliminate design and programming errors and take informed decisions early in the design stage, leading to fewer iterations in the building automation development cycle. We show through an example and two case studies how MLE+ can be used for designing energy-efficient control algorithms for both simulated buildings in EnergyPlus and real building equipment via BACnet.",
                "paper_link": "https://www.semanticscholar.org/paper/3ddaae50075492ef099d3288e4495eee69871a7b"
            },
            {
                "title": "Forecasting groundwater table in a flood prone coastal city with long short-term memory and recurrent neural networks",
                "abstract": "Many coastal cities are facing frequent flooding from storm events that are made worse by sea level rise and climate change. The groundwater table level in these low relief coastal cities is an important, but often overlooked, factor in the recurrent flooding these locations face. Infiltration of stormwater and water intrusion due to tidal forcing can cause already shallow groundwater tables to quickly rise toward the land surface. This decreases available storage which increases runoff, stormwater system loads, and flooding. Groundwater table forecasts, which could help inform the modeling and management of coastal flooding, are generally unavailable. This study explores two machine learning models, Long Short-term Memory (LSTM) networks and Recurrent Neural Networks (RNN), to model and forecast groundwater table response to storm events in the flood prone coastal city of Norfolk, Virginia. To determine the effect of training data type on model accuracy, two types of datasets (i) the continuous time series and (ii) a dataset of only storm events, created from observed groundwater table, rainfall, and sea level data from 2010\u20132018 are used to train and test the models. Additionally, a real-time groundwater table forecasting scenario was carried out to compare the models\u2019 abilities to predict groundwater table levels given forecast rainfall and sea level as input data. When modeling the groundwater table with observed data, LSTM networks were found to have more predictive skill than RNNs (root mean squared error (RMSE) of 0.09 m versus 0.14 m, respectively). The real-time forecast scenario showed that models trained only on storm event data outperformed models trained on the continuous time series data (RMSE of 0.07 m versus 0.66 m, respectively) and that LSTM outperformed RNN models. Because models trained with the continuous time series data had much higher RMSE values, they were not suitable for predicting the groundwater table in the real-time scenario when using forecast input data. These results demonstrate the first use of LSTM networks to create hourly forecasts of groundwater table in a coastal city and show they are well suited for creating operational forecasts in real-time. As groundwater table levels increase due to sea level rise, forecasts of groundwater table will become an increasingly valuable part of coastal flood modeling and management.",
                "paper_link": "https://www.semanticscholar.org/paper/d024995b60cac391bbaddbb7ddf3df3b8e2cfa0d"
            },
            {
                "title": "F1/10: An open-source autonomous cyber-physical platform",
                "abstract": "In 2005 DARPA labeled the realization of viable autonomous vehicles (AVs) a grand challenge; a short time later the idea became a moonshot that could change the automotive industry. Today, the question of safety stands between reality and solved. Given the right platform the CPS community is poised to offer unique insights. However, testing the limits of safety and performance on real vehicles is costly and hazardous. The use of such vehicles is also outside the reach of most researchers and students. In this paper, we present F1/10: an open-source, affordable, and high-performance 1/10 scale autonomous vehicle testbed. The F1/10 testbed carries a full suite of sensors, perception, planning, control, and networking software stacks that are similar to full scale solutions. We demonstrate key examples of the research enabled by the F1/10 testbed, and how the platform can be used to augment research and education in autonomous systems, making autonomy more accessible.",
                "paper_link": "https://www.semanticscholar.org/paper/cbb4f102fa821fecf0f28dacd555a09183ae2495"
            },
            {
                "title": "DR-Advisor: A data-driven demand response recommender system",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/482d7101892b29a59c48856bc29e7f12df54a286"
            },
            {
                "title": "Training machine learning surrogate models from a high\u2010fidelity physics\u2010based model: Application for real\u2010time street\u2010scale flood prediction in an urban coastal community",
                "abstract": "Mitigating the adverse impacts caused by increasing flood risks in urban coastal communities requires effective flood prediction for prompt action. Typically, physics\u2010based 1\u2010D pipe/2\u2010D overland flow models are used to simulate urban pluvial flooding. Because these models require significant computational resources and have long run times, they are often unsuitable for real\u2010time flood prediction at a street scale. This study explores the potential of a machine learning method, Random Forest (RF), to serve as a surrogate model for urban flood predictions. The surrogate model was trained to relate topographic and environmental features to hourly water depths simulated by a high\u2010resolution 1\u2010D/2\u2010D physics\u2010based model at 16,914 road segments in the coastal city of Norfolk, Virginia, USA. Two training scenarios for the RF model were explored: (i) training on only the most flood\u2010prone street segments in the study area and (ii) training on all 16,914 street segments in the study area. The RF model yielded high predictive skill, especially for the scenario when the model was trained on only the most flood\u2010prone streets. The results also showed that the surrogate model reduced the computational run time of the physics\u2010based model by a factor of 3,000, making real\u2010time decision support more feasible compared to using the full physics\u2010based model. We concluded that machine learning surrogate models strategically trained on high\u2010resolution and high\u2010fidelity physics\u2010based models have the potential to significantly advance the ability to support decision making in real\u2010time flood management within urban communities.",
                "paper_link": "https://www.semanticscholar.org/paper/7919790ed3b0213c4ba84e06e171cb1c9918edc2"
            },
            {
                "title": "Green scheduling of control systems for peak demand reduction",
                "abstract": "Building systems such as heating, air quality control and refrigeration operate independently of each other and frequently result in temporally correlated energy demand surges. As peak power prices are 200\u2013400 times that of the nominal rate, this uncoordinated activity is both expensive and operationally inefficient. We present an approach to fine-grained coordination of energy demand by scheduling the control systems within a constrained peak while ensuring custom climate environments are facilitated. The peak constraint is minimized for energy efficiency, while we provide feasibility conditions for the constraint to be realizable by a scheduling policy for the control systems. The physical systems are then coordinated by the scheduling controller so as both the peak constraint and the climate/safety constraint are satisfied. We also introduce a simple scheduling approach called lazy scheduling. The proposed control and scheduling strategy is implemented in simulation examples from small to large scales, which show that it can achieve significant peak demand reduction while being efficient and scalable.",
                "paper_link": "https://www.semanticscholar.org/paper/82dbe2e1f4f2a4dc3c0847c4cc88e570a0c0e827"
            },
            {
                "title": "Leveraging open source software and parallel computing for model predictive control of urban drainage systems using EPA-SWMM5",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/eafb5afd1b236ba438826322c6fecebbe9925234"
            },
            {
                "title": "Data-driven model predictive control with regression trees\u2014an application to building energy management",
                "abstract": "Model Predictive Control (MPC) plays an important role in optimizing operations of complex cyber-physical systems because of its ability to forecast system\u2019s behavior and act under system level constraints. However, MPC requires reasonably accurate underlying models of the system. In many applications, such as building control for energy management, Demand Response, or peak power reduction, obtaining a high-fidelity physics-based model is cost and time prohibitive, thus limiting the widespread adoption of MPC. To this end, we propose a data-driven control algorithm for MPC that relies only on the historical data. We use multi-output regression trees to represent the system\u2019s dynamics over multiple future time steps and formulate a finite receding horizon control problem that can be solved in real-time in closed-loop with the physical plant. We apply this algorithm to peak power reduction in buildings to optimally trade-off peak power reduction against thermal comfort without having to learn white/grey box models of the systems dynamics.",
                "paper_link": "https://www.semanticscholar.org/paper/83825acde1a97e74d81af28bb1dd8f2967d2e797"
            },
            {
                "title": "Exploring real-time control of stormwater systems for mitigating flood risk due to sea level rise",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/c54a2d1f81d587abd43ab190ff1e9681a649fe42"
            },
            {
                "title": "Flood mitigation in coastal urban catchments using real-time stormwater infrastructure control and reinforcement learning",
                "abstract": "\n Flooding in coastal cities is increasing due to climate change and sea-level rise, stressing the traditional stormwater systems these communities rely on. Automated real-time control (RTC) of these systems can improve performance, and creating control policies for smart stormwater systems is an active area of study. This research explores reinforcement learning (RL) to create control policies to mitigate flood risk. RL is trained using a model of hypothetical urban catchments with a tidal boundary and two retention ponds with controllable valves. RL's performance is compared to the passive system, a model predictive control (MPC) strategy, and a rule-based control strategy (RBC). RL learns to proactively manage pond levels using current and forecast conditions and reduced flooding by 32% over the passive system. Compared to the MPC approach using a physics-based model and genetic algorithm, RL achieved nearly the same flood reduction, just 3% less than MPC, with a significant 88\u00d7 speedup in runtime. Compared to RBC, RL was able to quickly learn similar control strategies and reduced flooding by an additional 19%. This research demonstrates that RL can effectively control a simple system and offers a computationally efficient method that could scale to RTC of more complex stormwater systems.",
                "paper_link": "https://www.semanticscholar.org/paper/608e074a06c362af9d94e9c39c0425299e1cd7b1"
            },
            {
                "title": "Data Predictive Control for building energy management",
                "abstract": "Decisions on how to best optimize energy systems operations are becoming ever so complex and conflicting, that model-based predictive control (MPC) algorithms must play an important role. However, a key factor prohibiting the widespread adoption of MPC in buildings, is the cost, time, and effort associated with learning first-principles based dynamical models of the underlying physical system. This paper introduces an alternative approach for implementing finite-time receding horizon control using control-oriented data-driven models. We call this approach Data Predictive Control (DPC). Specifically, by utilizing separation of variables, two novel algorithms for implementing DPC using a single regression tree and with regression trees ensembles (random forest) are presented. The data predictive controller enables the building operator to trade off energy consumption against thermal comfort without having to learn white/grey box models of the systems dynamics. We present a comprehensive numerical study which compares the performance of DPC with an MPC based energy management strategy, using a single zone building model. Our results demonstrate that performance of DPC is comparable to an MPC controller, with only 3.8% additional cost in terms of optimal objective function and within 95% in terms of R2 score, thereby making it an alluring alternative to MPC, whenever the associated cost of learning the model is high.",
                "paper_link": "https://www.semanticscholar.org/paper/264ca43ca6d6d0cd68c4f80a1b26fbdbd69e8147"
            },
            {
                "title": "Data-driven modeling, control and tools for cyber-physical energy systems",
                "abstract": "Demand response (DR) is becoming important as the volatility on the grid continues to increase. Current DR approaches are either completely manual or involve deriving first principles based models which are extremely cost and time prohibitive to build. We consider the problem of data-driven DR for large buildings which involves predicting the demand response baseline, evaluating fixed DR strategies and synthesizing DR control actions. We provide a model based control with regression trees algorithm (mbCRT), which allows us to perform closed-loop control for DR strategy synthesis for large buildings. Our data-driven control synthesis algorithm outperforms rule- based DR by 17% for a large DoE commercial reference building and leads to a curtailment of 380 kW and over $45,000 in savings. Our methods have been integrated into an open source tool called DR-Advisor, which acts as a recommender system for the building's facilities manager and provides suitable control actions to meet the desired load curtailment while maintaining operations and maximizing the economic reward. DR-Advisor achieves 92.8% to 98.9% prediction accuracy for 8 buildings on Penn's campus. We compare DR-Advisor with other data driven methods and rank 2nd on ASHRAE's benchmarking data-set for energy prediction.",
                "paper_link": "https://www.semanticscholar.org/paper/f263c4eacbd5528cff87a5562c7d66c5ac3c081c"
            },
            {
                "title": "f1tenth. dev-an open-source ros based f1/10 autonomous racing simulator",
                "abstract": "This paper presents the ROS F1/10 simulator - a ROS and Gazebo based autonomous racing simulator designed to mirror the behaviour and performance of the physical F1/10 platform. The simulator can be used to verify the performance of autonomous racing algorithms before testing on the real F1/10 racecar. The simulator supports most major ROS SLAM and navigation packages with tutorials on how to get started available on www.f1tenth.dev under open-source license and we continue to add support for more algorithms and features. The paper presents details on the simulator architecture, design, and features and presents several research use-cases including an example of developing in the sim and transferring the performance to the race racecar.",
                "paper_link": "https://www.semanticscholar.org/paper/b600041ffd829e378fcf7434ffc5e4f55f41fe6d"
            },
            {
                "title": "Estimating impacts of recurring flooding on roadway networks: a Norfolk, Virginia case study",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/f9fe5545cd37a41e304b4c018bf032feb3a7e7d5"
            },
            {
                "title": "Autonomous electric vehicle charging system",
                "abstract": "Electric vehicle (EV) adoption has surpassed the growth of charging infrastructure. As the demand for charging stations surpass the supply, expanding charging infrastructure for consumers is crucial to improving the experience of owning and maintaining an EV. One solution is to simply provide more charging stations; however, this requires significant upfront hardware and space cost. In addition, parking spots allocated for EVs should only be used by EVs, forcing manufacturers to make a decision on the number of EV and non-EV parking spots. Current charging stations also have their own problems. When an EV is finished charging, any additional time it spends in the charging location is time that another EV could be using to charge itself. Innovative new products are necessary to create an adequate charging network. In this work, a mobile autonomous robot which charges parked EVs at any location with its own battery is presented. We created a proof-of-concept autonomous charging robot to demonstrate feasibility and motivate future work. The goal is to provide three main decoupled functionalities: parking lot navigation, EV plug guidance, and robot battery swapping. The current iteration meets these functionalities using a TurtleBot to navigate a mock parking lot, new designs and prototypes for swapping batteries, and a robotic arm paired with a computer vision algorithm to guide a 3D printed plug. Ongoing challenges for future iterations involve integrating the main functionalities and dealing with a wider range of less common use cases.",
                "paper_link": "https://www.semanticscholar.org/paper/81215d1078947e3c7635fe21fe21426d42011505"
            },
            {
                "title": "Green scheduling: Scheduling of control systems for peak power reduction",
                "abstract": "Heating, cooling and air quality control systems within buildings and datacenters operate independently of each other and frequently result in temporally correlated energy demand surges. As peak power prices are 200\u2013400 times that of the nominal rate, this uncoordinated activity is both expensive and operationally inefficient. While several approaches for load shifting and model predictive control have been proposed, we present an alternative approach to fine-grained coordination of energy demand by scheduling energy consuming control systems within a constrained peak power while ensuring custom climate environments are facilitated. Unlike traditional real-time scheduling theory, where the execution time and hence the schedule are a function of the system variables only, control system execution (i.e. when energy is supplied to the system) are a function of the environmental variables and the plant dynamics. To this effect, we propose a geometric interpretation of the system dynamics, where a scheduling policy is represented as a hybrid automaton and the scheduling problem is presented as designing a hybrid automaton. Tasks are constructed by extracting the temporal parameters of the system dynamics. We provide feasibility conditions and a lazy scheduling approach to reduce the peak power for a set of control systems. The proposed model is intuitive, scalable and effective for the large class of systems whose state-time profile can be linearly approximated.",
                "paper_link": "https://www.semanticscholar.org/paper/bd08097dc4261920ca752aa78efef22c40b7144b"
            },
            {
                "title": "Deep learning using physically-informed input data for wetland identification",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/c8c7987a997399a6dcc1a636f284b03a92309e86"
            },
            {
                "title": "DeepRacing: A framework for autonomous racing",
                "abstract": "We consider the challenging problem of high speed autonomous racing in realistic dynamic environments. DeepRacing is a novel end-to-end framework, and a virtual testbed for training and evaluating algorithms for autonomous racing. The virtual testbed is implemented using the realistic Formula One (F1) Codemasters game, which is used by many F1 drivers for training. We present AdmiralNet - a Convolution Neural Network (CNN) integrated with Long Short-Term Memory (LSTM) cells that can be tuned for the autonomous racing task in the highly realistic F1 game. We evaluate AdmiralNet\u2019s performance on unseen race tracks, and also evaluate the degree of transference between the simulation and the real world by implementing end-to-end racing on a physical 1/10 scale autonomous racecar.",
                "paper_link": "https://www.semanticscholar.org/paper/fdacad963dcabb2070a23277ba565fa5cc4bccaa"
            },
            {
                "title": "Scalable scheduling of building control systems for peak demand reduction",
                "abstract": "In large energy systems, peak demand might cause severe issues such as service disruption and high cost of energy production and distribution. Under the widely adopted peak-demand pricing policy, electricity customers are charged a very high price for their maximum demand to discourage their energy usage in peak load conditions. In buildings, peak demand is often the result of temporally correlated energy demand surges caused by uncoordinated operation of subsystems such as heating, ventilating, air conditioning and refrigeration (HVAC&R) systems and lighting systems. We have previously presented green scheduling as an approach to schedule the building control systems within a constrained peak demand envelope while ensuring that custom climate conditions are facilitated. This paper provides a sufficient schedulability condition for the peak constraint to be realizable for a large and practical class of system dynamics that can capture certain nonlinear dynamics, inter-dependencies, and constrained disturbances. We also present a method for synthesizing periodic schedules for the system. The proposed method is demonstrated in a simulation example to be scalable and effective for a large-scale system.",
                "paper_link": "https://www.semanticscholar.org/paper/36344d42caed085cc1845d7f6540c60678b6d805"
            }
        ]
    },
    {
        "Professor": "Brad Campbell",
        "Papers": [
            {
                "title": "The internet of things has a gateway problem",
                "abstract": "The vision of an Internet of Things (IoT) has captured the imagination of the world and raised billions of dollars, all before we stopped to deeply consider how all these Things should connect to the Internet. The current state-of-the-art requires application-layer gateways both in software and hardware that provide application-specific connectivity to IoT devices. In much the same way that it would be difficult to imagine requiring a new web browser for each website, it is hard to imagine our current approach to IoT connectivity scaling to support the IoT vision. The IoT gateway problem exists in part because today's gateways conflate network connectivity, in-network processing, and user interface functions. We believe that disentangling these functions would improve the connectivity potential for IoT devices. To realize the broader vision, we propose an architecture that leverages the increasingly ubiquitous presence of Bluetooth Low Energy radios to connect IoT peripherals to the Internet. In much the same way that WiFi access points revolutionized laptop utility, we envision that a worldwide deployment of IoT gateways could revolutionize application-agnostic connectivity, thus breaking free from the stove-piped architectures now taking hold. In this paper, we present our proposed architecture, show example applications enabled by it, and explore research challenges in its implementation and deployment.",
                "paper_link": "https://www.semanticscholar.org/paper/d4d71d1fa1f476051c52b398e565f71dd2ed213b"
            },
            {
                "title": "Multiprogramming a 64kb computer safely and efficiently",
                "abstract": "Low-power microcontrollers lack some of the hardware features and memory resources that enable multiprogrammable systems. Accordingly, microcontroller-based operating systems have not provided important features like fault isolation, dynamic memory allocation, and flexible concurrency. However, an emerging class of embedded applications are software platforms, rather than single purpose devices, and need these multiprogramming features. Tock, a new operating system for low-power platforms, takes advantage of limited hardware-protection mechanisms as well as the type-safety features of the Rust programming language to provide a multiprogramming environment for microcontrollers. Tock isolates software faults, provides memory protection, and efficiently manages memory for dynamic application workloads written in any language. It achieves this while retaining the dependability requirements of long-running applications.",
                "paper_link": "https://www.semanticscholar.org/paper/2988e8fff63ee9f953df00cf53e92a4df8ed69aa"
            },
            {
                "title": "Surepoint: Exploiting ultra wideband flooding and diversity to provide robust, scalable, high-fidelity indoor localization",
                "abstract": "We present SurePoint, a system for drop-in, high-fidelity indoor localization. SurePoint builds on recently available commercial ultra-wideband radio hardware. While ultra-wideband radio hardware can provide the timing primitives necessary for a simple adaptation of two-way ranging, we show that with the addition of frequency and spatial diversity, we can achieve a 53% decrease in median ranging error. Because this extra diversity requires many additional packets for each range estimate, we next develop an efficient broadcast ranging protocol for localization that ameliorates this overhead. We evaluate the performance of this ranging protocol in stationary and fast-moving environments and find that it achieves up to 0.08 m median error and 0.53 m 99th percentile error. As ranging requires the tag to have exclusive access to the channel, we next develop a protocol to coordinate the localization of multiple tags in space. This protocol builds on recent work exploiting the constructive interference phenomenon. The ultra-wideband PHY uses a different modulation scheme compared to the narrowband PHY used by previous work, thus we first explore the viability and performance of constructive interference with ultra-wideband radios. Finally, as the ranging protocol requires careful management of the ultra-wideband radio and tight timing, we develop TriPoint, a dedicated \"drop-in\" ranging module that provides a simple I2C interface. We show that this additional microcontroller demands only marginal energy overhead while facilitating interoperability by freeing the primary microcontroller to handle other tasks.",
                "paper_link": "https://www.semanticscholar.org/paper/6e5bc23ad8ef1b41cbd02ec138c001d143b9fc91"
            },
            {
                "title": "Graph neural networks in IoT: A survey",
                "abstract": "The Internet of Things (IoT) boom has revolutionized almost every corner of people\u2019s daily lives: healthcare, environment, transportation, manufacturing, supply chain, and so on. With the recent development of sensor and communication technology, IoT artifacts, including smart wearables, cameras, smartwatches, and autonomous systems can accurately measure and perceive their surrounding environment. Continuous sensing generates massive amounts of data and presents challenges for machine learning. Deep learning models (e.g., convolution neural networks and recurrent neural networks) have been extensively employed in solving IoT tasks by learning patterns from multi-modal sensory data. Graph neural networks (GNNs), an emerging and fast-growing family of neural network models, can capture complex interactions within sensor topology and have been demonstrated to achieve state-of-the-art results in numerous IoT learning tasks. In this survey, we present a comprehensive review of recent advances in the application of GNNs to the IoT field, including a deep dive analysis of GNN design in various IoT sensing environments, an overarching list of public data and source codes from the collected publications, and future research directions. To keep track of newly published works, we collect representative papers and their open-source implementations and create a Github repository at GNN4IoT.",
                "paper_link": "https://www.semanticscholar.org/paper/d09608593caa20b79a8aaddfe19df7e31513d711"
            },
            {
                "title": "Is Rust used safely by software developers?",
                "abstract": "Rust, an emerging programming language with explosive growth, provides a robust type system that enables programmers to write memory-safe and data-race free code. To allow access to a machine's hardware and to support low-level performance optimizations, a second language, Unsafe Rust, is embedded in Rust. It contains support for operations that are difficult to statically check, such as C-style pointers for access to arbitrary memory locations and mutable global variables. When a program uses these features, the compiler is unable to statically guarantee the safety properties Rust promotes. In this work, we perform a large-scale empirical study to explore how software developers are using Unsafe Rust in real-world Rust libraries and applications. Our results indicate that software engineers use the keyword unsafe in less than 30% of Rust libraries, but more than half cannot be entirely statically checked by the Rust compiler because of Unsafe Rust hidden somewhere in a library's call chain. We conclude that although the use of the keyword unsafe is limited, the propagation of unsafeness offers a challenge to the claim of Rust as a memory-safe language. Furthermore, we recommend changes to the Rust compiler and to the central Rust repository's interface to help Rust software developers be aware of when their Rust code is unsafe.",
                "paper_link": "https://www.semanticscholar.org/paper/8f564873814a12526a844d69c216ba2b599bdf9a"
            },
            {
                "title": "Monjolo: An energy-harvesting energy meter architecture",
                "abstract": "Conventional AC power meters perform at least two distinct functions: power conversion, to supply the meter itself, and energy metering, to measure the load consumption. This paper presents Monjolo, a new energy-metering architecture that combines these two functions to yield a new design point in the metering space. The key insight underlying this work is that the output of a current transformer -- nominally used to measure a load current -- can be harvested and used to intermittently power a wireless sensor node. The hypothesis is that the node's activation frequency increases monotonically with the primary load's draw, making it possible to estimate load power from the interval between activations, assuming the node consumes a fixed energy quanta during each activation. This paper explores this thesis by designing, implementing, and evaluating the Monjolo metering architecture. The results demonstrate that it is possible to build a meter that draws zero-power under zero-load conditions, offers high accuracy for near-unity power factor loads, works with non-unity power factor loads in combination with a whole-house meter, wirelessly reports readings to a data aggregator, is resilient to communication failures, and is parsimonious with the radio channel, even under heavy loads. Monjolo eliminates the high-voltage AC-DC power supply and AC metering circuitry present in earlier designs, enabling a smaller, simpler, safer, and lower-cost design point that supports novel deployment scenarios like non-intrusive circuit-level metering.",
                "paper_link": "https://www.semanticscholar.org/paper/5ae9cdcd8052f3a527d662e947a12b802c5b76f7"
            },
            {
                "title": "BLE can see: a reinforcement learning approach for RF-based indoor occupancy detection",
                "abstract": "The emergence of radio frequency (RF) dependent device-free indoor occupancy detection has seen slow acceptance due to its high fragility. Experimentation shows that an RF-dependent occupancy detector initially performs well in the room to be sensed. However, once the physical arrangement of objects changes in the room, the performance of the classifier degrades significantly. To address this issue, we propose BLECS, a Bluetooth-dependent indoor occupancy detection system which can adapt itself in the dynamic environment. BLECS uses a reinforcement learning approach to predict the occupancy of an indoor environment and updates its decision policy by interacting with existing IoT devices and sensors in the room. We tested this system in five different rooms for 520 hours in total, involving four occupants. Results show that, BLECS achieves 21.4% performance improvement in a dynamic environment compared to the state-of-the-art supervised learning algorithm with an average F1 score of 86.52%. This system can also predict occupancy with a maximum 89.23% F1 score in a completely unknown environment with no initial trained model.",
                "paper_link": "https://www.semanticscholar.org/paper/85c9e4fbf6fa150f0782dd6c3cbdad471f736893"
            },
            {
                "title": "Ownership is theft: Experiences building an embedded OS in Rust",
                "abstract": "Rust, a new systems programming language, provides compile-time memory safety checks to help eliminate runtime bugs that manifest from improper memory management. This feature is advantageous for operating system development, and especially for embedded OS development, where recovery and debugging are particularly challenging. However, embedded platforms are highly event-based, and Rust's memory safety mechanisms largely presume threads. In our experience developing an operating system for embedded systems in Rust, we have found that Rust's ownership model prevents otherwise safe resource sharing common in the embedded domain, conflicts with the reality of hardware resources, and hinders using closures for programming asynchronously. We describe these experiences and how they relate to memory safety as well as illustrate our workarounds that preserve the safety guarantees to the largest extent possible. In addition, we draw from our experience to propose a new language extension to Rust that would enable it to provide better memory safety tools for event-driven platforms.",
                "paper_link": "https://www.semanticscholar.org/paper/73aec339f9bfdfae50b84d9117d64ab903a7d7ed"
            },
            {
                "title": "The case for writing a kernel in rust",
                "abstract": "An operating system kernel written in the Rust language would have extremely fine-grained isolation boundaries, have no memory leaks, and be safe from a wide range of security threats and memory bugs. Previous efforts towards this end concluded that writing a kernel requires changing Rust. This paper reaches a different conclusion, that no changes to Rust are needed and a kernel can be implemented with a very small amount of unsafe code. It describes how three sample kernel mechanisms---DMA, USB, and buffer caches---can be built using these abstractions.",
                "paper_link": "https://www.semanticscholar.org/paper/5f9f54b21febff13e9109a4026451b0d6fe92d61"
            },
            {
                "title": "The signpost platform for city-scale sensing",
                "abstract": "City-scale sensing holds the promise of enabling a deeper understanding of our urban environments. However, a city-scale deployment requires physical installation, power management, and communications all challenging tasks standing between a good idea and a realized one. This indicates the need for a platform that enables easy deployment and experimentation for applications operating at city scale. To address these challenges, we present Signpost, a modular, energy-harvesting platform for city-scale sensing. Signpost simplifies deployment by eliminating the need for connection to wired infrastructure and instead harvesting energy from an integrated solar panel. The platform furnishes the key resources necessary to support multiple, pluggable sensor modules while providing fair, safe, and reliable sharing in the face of dynamic energy constraints. We deploy Signpost with several sensor modules, showing the viability of an energy-harvesting, multi-tenant, sensing system, and evaluate its ability to support sensing applications. We believe Signpost reduces the difficulty inherent in city-scale deployments, enables new experimentation, and provides improved insights into urban health.",
                "paper_link": "https://www.semanticscholar.org/paper/0dbf32db0deb5d64b9fdbbfa7c093ba0c1493a8c"
            },
            {
                "title": "An energy-harvesting sensor architecture and toolkit for building monitoring and event detection",
                "abstract": "Understanding building usage patterns and resource consumption, particularly for existing buildings, requires a sensing infrastructure for the building. Often, deploying these sensors and obtaining real-time information is hindered by installation and maintenance difficulties resulting from scaling down and powering these devices. Devices that rely on batteries are limited by the scale of the batteries and the maintenance cost of replacing them while AC mains powered sensors incur high upfront installation costs. To mitigate these burdens, we present a new architecture for designing building-monitoring focused energy-harvesting sensors. The key to this architecture is masking the inevitable inter-mittency provided by energy-harvesting with a trigger abstraction that activates the device only when there is useful work to be done. In this paper, we describe our architecture and demonstrate how it supports existing energy-harvesting sensor designs. Further, we realize three additional design points within the architecture and demonstrate how the sensors are effective at building monitoring and event detection. The sensors, however, are classically disruptive: they improve ease of installation and maintenance, but to do so, they sacrifice some fidelity and reliability. Whether this trade-off is acceptable remains to be explored, but the technology needed to do so is now here.",
                "paper_link": "https://www.semanticscholar.org/paper/e62e7eae6b4928d33fcfdc4b8ea401475a9e898d"
            },
            {
                "title": "Decentralized federated learning framework for the neighborhood: a case study on residential building load forecasting",
                "abstract": "The fast-growing trend of Internet of Things (IoT) has provided its users with opportunities to improve user experience such as voice assistants, smart cameras, and home energy management systems. Such smart home applications often require large numbers of diverse training data to accomplish a robust model. As single user may not have enough data to train such a model, users intent to collaboratively train their collected data in order to achieve better performance in such applications, which raise the concern of data privacy protection. Existing approaches for collaborative training need to aggregate data or intermediate model training updates in the cloud to perform load forecasting, which could directly or indirectly cause personal data leakage, alongside with significant communication bandwidth and extra cloud service monetary cost. In this paper, to ensure the performance of smart home applications as well as the protection of user data privacy, we introduce the decentralized federated learning framework for the neighborhood and show the study on residential building load forecasting application as an example. We present PriResi, a privacy-preserved, communication-efficient and cloud-service-free load forecasting system to solve the above problems in a residential building. We first introduce a decentralized federated learning framework, which allows the residents to process all collected data locally on the edge by broadcasting the model updates between the smart home agent in each residence. Second, we propose a gradient selection mechanism to reduce the number of aggregated gradients and the frequency of gradient broadcasting to achieve communication-efficient and high prediction results. The real-word dataset experiments show that our method can achieve 97% of load forecasting accuracy while preserving residences' privacy. We believe that our proposed decentralized federated learning framework can be widely used in other smart home applications as well.",
                "paper_link": "https://www.semanticscholar.org/paper/dbb37813dba4533add167cd5d15f1f7af0beeeb0"
            },
            {
                "title": "Energy-harvesting thermoelectric sensing for unobtrusive water and appliance metering",
                "abstract": "Fine-grained energy metering in homes and buildings provides a promising technique for addressing the unmaintainable energy consumption levels of worldwide buildings. Metering electricity, lighting, natural gas, HVAC, occupancy, and water on a per appliance or room basis can provide invaluable insight when trying to reduce a building's energy footprint. A myriad of sensor designs and systems collect data on particular building aspects, but are often hampered by installation difficulty or ongoing maintenance needs (like battery replacement). We address these common pitfalls for water and heat metering by developing a small, energy-harvesting sensor that meters using the same thermoelectric generator with which it powers itself. In short, the rate at which the harvester captures energy is proportional to the heat production of the monitored appliance or pipe and this relationship allows us to estimate energy use simply based on the sensor's ability to harvest. We prototype our sensor in a bracelet shaped form-factor that can attach to a shower head pipe, faucet, or appliance to provide local hot water or heat metering.",
                "paper_link": "https://www.semanticscholar.org/paper/ab6d56952b90d425e58c2d6eecde0725f22d1bea"
            },
            {
                "title": "Gemini: A non-invasive, energy-harvesting true power meter",
                "abstract": "Power meters are critical for sub metering loads in residential and commercial settings, but high installation cost and complexity hamper their broader adoption. Recent approaches address installation burdens by proposing non-invasive meters that easily clip onto a wire, or stick onto a circuit breaker, to perform contact less metering. Unfortunately, these designs require regular maintenance (e.g. Battery replacement) or reduce measurement accuracy (e.g. Work poorly with non-unity power factors). This paper presents Gemini, a new design point in the power metering space. Gemini addresses the drawbacks of prior approaches by decoupling and distributing the AC voltage and current measurement acquisitions, and recombining them wirelessly using a low-bandwidth approach, to offer non-invasive real, reactive, and apparent power metering. Battery maintenance is eliminated by using an energy-harvesting design that enables the meter to power itself using a current transformer. Accuracy is substantially improved over other non-invasive meters by virtualizing the voltage channel -- effectively allowing the meter to calculate power as if it could directly measure voltage (since true power requires sample-by-sample multiplication of current and voltage measurements acquired with tight timing constraints). Collectively, these improvements result in a new design point that meters resistive loads with 0.6 W average error and a range of reactive and switching loads with 2.2 W average error -- matching commercial, mains-powered solutions.",
                "paper_link": "https://www.semanticscholar.org/paper/aac67f1d9aa42732cb2e31183b34b43432b3df4e"
            },
            {
                "title": "Towards a perpetual wireless sensor node",
                "abstract": "We report our experimental findings on a perpetual wireless sensor node. Our perpetual node is based on the integration of a wireless sensor node that runs IEEE 802.15.4e technology on TI's MSP430 with a solar energy harvesting module that is based on TI's bq25504 modules. We show that by default running the time synchronized channel hopping (TSCH) MAC layer draws more power than the harvester is able to provide. To close that gap, enhancements that improve the power efficiency of TSCH protocol are described and results are presented. The enhancements proposed consist of shared slot suppression, slotframe skipping, and connection setup time reduction. It is shown that even with these enhancements consumed power is still greater than harvested power. We provide future directions on how to further reduce consumed power at the node.",
                "paper_link": "https://www.semanticscholar.org/paper/5b10f2214bff7d5c17a9dd91e9d99cabeb7f2e56"
            },
            {
                "title": "The tock embedded operating system",
                "abstract": "Low-power microcontrollers lack some of the hardware features and most of the memory resources that usually enable multiprogrammable systems. Accordingly, operating system software for these platforms has not provided important features like memory isolation, dynamic memory allocation, and flexible concurrency. However, an emerging class of embedded applications are software platforms, rather than single purpose devices. Tock, a new operating system for low-power platforms, takes advantage of the limited hardware-protection mechanisms available on recent microcontrollers and the type-safety features of the Rust programming language to provide a multiprogramming environment that offers isolation of software faults, memory protection, and efficient memory management for dynamic application workloads written in any language while retaining the dependability requirements of long-running devices.",
                "paper_link": "https://www.semanticscholar.org/paper/f1783c2cfd1432987556c9da7de11decf2d253fc"
            },
            {
                "title": "Decawave: Exploring state of the art commercial localization",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Cinamin: A Perpetual and Nearly Invisible BLE Beacon.",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "The signpost network: Demo abstract",
                "abstract": "The era of city-scale sensing is dawning. Supported by new sensing capabilities, the capability to detect and measure phenomena throughout a large area will allow deeper insight and understanding into how cities work. The challenge of city-scale sensing is not limited to developing new sensing applications, however. A sensor must be installed in a location. It must be provided power, storage, and communications. All these tasks stand aside from the desired sensing effort, but are necessary nevertheless. In this demo, we introduce an initial prototype for a modular, city-scale sensing platform---the signpost network. The platform, designed to be physically attached to sign posts throughout a city, reduces the burden for sensor and application developers by providing the necessary resources to modules attached to it. Power is provided by harvesting from solar panels with battery storage, with each module allocated a certain subset of the system energy. The signpost platform also provides data storage, long-range communication, data processing, module isolation, and an installation point for connected modules. The signpost acts as a modular base station for researchers, citizen scientists, and other interested parties to deploy custom sensors for applications such as pedestrian counting, air quality monitoring, and RF spectrum sensing at a city-wide scale.",
                "paper_link": "https://www.semanticscholar.org/paper/78903df2bb504eb2124e6d1d0d6e7e6edee5ec6e"
            },
            {
                "title": "No batteries needed: Providing physical context with energy-harvesting beacons",
                "abstract": "Battery-powered digital beacons have played a significant role in shrinking the gap between physical and digital world. At the same time, ubiquitous sensing encourages tiny, unobtrusive, energy-harvesting devices to eliminate the limited lifetime of battery-powered devices. In this paper, we design a new fire-and-forget room number broadcaster beacon to investigate the feasibility and performance of such a design point. We study how several factors including different deployment spaces, the storage capacity of the harvester, indoor light intensity levels, and spatial position of the receiver impact the performance in three real-world deployments. We find that the 95th percentile of inter-packet reception time is 35 s or less in a lab space with exposure to sunlight and indoor lights, 29 s or less in an industrial plant with indoor lights, and 405 s or more in office rooms. With strategic beacon placement and a light intensity level of only 390 lx, performance can be improved by 61%. We believe that these results will help guide future energy-harvesting beacon deployments. We also outline possible improvements for future energy-harvesting beacon designs.",
                "paper_link": "https://www.semanticscholar.org/paper/097a00da12542ad36846c80005210b7459b890b2"
            }
        ]
    },
    {
        "Professor": "Rohan Chandra",
        "Papers": [
            {
                "title": "Traphic: Trajectory prediction in dense and heterogeneous traffic using weighted interactions",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Emotions don't lie: An audio-visual deepfake detection method using affective cues",
                "abstract": "We present a learning-based method for detecting real and fake deepfake multimedia content. To maximize information for learning, we extract and analyze the similarity between the two audio and visual modalities from within the same video. Additionally, we extract and compare affective cues corresponding to perceived emotion from the two modalities within a video to infer whether the input video is \"real\" or \"fake\". We propose a deep learning network, inspired by the Siamese network architecture and the triplet loss. To validate our model, we report the AUC metric on two large-scale deepfake detection datasets, DeepFake-TIMIT Dataset and DFDC. We compare our approach with several SOTA deepfake detection methods and report per-video AUC of 84.4% on the DFDC and 96.6% on the DF-TIMIT datasets, respectively. To the best of our knowledge, ours is the first approach that simultaneously exploits audio and video modalities and also perceived emotions from the two modalities for deepfake detection.",
                "paper_link": "https://www.semanticscholar.org/paper/66970c48758b65a36d77c0fa9add1198d36ecd59"
            },
            {
                "title": "M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
                "abstract": "We present M3ER, a learning-based method for emotion recognition from multiple input modalities. Our approach combines cues from multiple co-occurring modalities (such as face, text, and speech) and also is more robust than other methods to sensor noise in any of the individual modalities. M3ER models a novel, data-driven multiplicative fusion method to combine the modalities, which learn to emphasize the more reliable cues and suppress others on a per-sample basis. By introducing a check step which uses Canonical Correlational Analysis to differentiate between ineffective and effective modalities, M3ER is robust to sensor noise. M3ER also generates proxy features in place of the ineffectual modalities. We demonstrate the efficiency of our network through experimentation on two benchmark datasets, IEMOCAP and CMU-MOSEI. We report a mean accuracy of 82.7% on IEMOCAP and 89.0% on CMU-MOSEI, which, collectively, is an improvement of about 5% over prior work.",
                "paper_link": "https://www.semanticscholar.org/paper/ab9dd55911c9ac0daf3172fe5759bf8afffe6ee4"
            },
            {
                "title": "Forecasting trajectory and behavior of road-agents using spectral clustering in graph-lstms",
                "abstract": "We present a novel approach for traffic forecasting in urban traffic scenarios using a combination of spectral graph analysis and deep learning. We predict both the low-level information (future trajectories) as well as the high-level information (road-agent behavior) from the extracted trajectory of each road-agent. Our formulation represents the proximity between the road agents using a weighted dynamic geometric graph (DGG). We use a two-stream graph-LSTM network to perform traffic forecasting using these weighted DGGs. The first stream predicts the spatial coordinates of road-agents, while the second stream predicts whether a road-agent is going to exhibit overspeeding, underspeeding, or neutral behavior by modeling spatial interactions between road-agents. Additionally, we propose a new regularization algorithm based on spectral clustering to reduce the error margin in long-term prediction (3-5 seconds) and improve the accuracy of the predicted trajectories. Moreover, we prove a theoretical upper bound on the regularized prediction error. We evaluate our approach on the Argoverse, Lyft, Apolloscape, and NGSIM datasets and highlight the benefits over prior trajectory prediction methods. In practice, our approach reduces the average prediction error by approximately 75% over prior algorithms and achieves a weighted average accuracy of 91.2% for behavior prediction. Additionally, our spectral regularization improves long-term prediction by up to $\\text{70}\\%$.",
                "paper_link": "https://www.semanticscholar.org/paper/4dc94fbff63e7b3878439a1fc95b9793f5595018"
            },
            {
                "title": "Emoticon: Context-aware multimodal emotion recognition using frege's principle",
                "abstract": "We present EmotiCon, a learning-based algorithm for context-aware perceived human emotion recognition from videos and images. Motivated by Frege's Context Principle from psychology, our approach combines three interpretations of context for emotion recognition. Our first interpretation is based on using multiple modalities (e.g.faces and gaits) for emotion recognition. For the second interpretation, we gather semantic context from the input image and use a self-attention-based CNN to encode this information. Finally, we use depth maps to model the third interpretation related to socio-dynamic interactions and proximity among agents. We demonstrate the efficiency of our network through experiments on EMOTIC, a benchmark dataset. We report an Average Precision (AP) score of 35.48 across 26 classes, which is an improvement of 7-8 over prior methods. We also introduce a new dataset, GroupWalk, which is a collection of videos captured in multiple real-world settings of people walking. We report an AP of 65.83 across 4 categories on GroupWalk, which is also an improvement over prior methods.",
                "paper_link": "https://www.semanticscholar.org/paper/d19bc4735cdb26935dbf50d2ac6d6b82fcc20302"
            },
            {
                "title": "Step: Spatial temporal graph convolutional networks for emotion perception from gaits",
                "abstract": "We present a novel classifier network called STEP, to classify perceived human emotion from gaits, based on a Spatial Temporal Graph Convolutional Network (ST-GCN) architecture. Given an RGB video of an individual walking, our formulation implicitly exploits the gait features to classify the perceived emotion of the human into one of four emotions: happy, sad, angry, or neutral. We train STEP on annotated real-world gait videos, augmented with annotated synthetic gaits generated using a novel generative network called STEP-Gen, built on an ST-GCN based Conditional Variational Autoencoder (CVAE). We incorporate a novel push-pull regularization loss in the CVAE formulation of STEP-Gen to generate realistic gaits and improve the classification accuracy of STEP. We also release a novel dataset (E-Gait), which consists of 4,227 human gaits annotated with perceived emotions along with thousands of synthetic gaits. In practice, STEP can learn the affective features and exhibits classification accuracy of 88% on E-Gait, which is 14\u201330% more accurate over prior methods.",
                "paper_link": "https://www.semanticscholar.org/paper/23135a88c48de1b15bfdf2f4c09621385ae5d3de"
            },
            {
                "title": "M3detr: Multi-representation, multi-scale, mutual-relation 3d object detection with transformers",
                "abstract": "We present a novel architecture for 3D object detection, M3DETR, which combines different point cloud representations (raw, voxels, bird-eye view) with different feature scales based on multi-scale feature pyramids. M3DETR is the first approach that unifies multiple point cloud representations, feature scales, as well as models mutual relationships between point clouds simultaneously using transformers. We perform extensive ablation experiments that highlight the benefits of fusing representation and scale, and modeling the relationships. Our method achieves state-of-the-art performance on the KITTI 3D object detection dataset and Waymo Open Dataset. Results show that M3DETR improves the baseline significantly by 1.48% mAP for all classes on Waymo Open Dataset. In particular, our approach ranks 1st on the well-known KITTI 3D Detection Benchmark for both car and cyclist classes, and ranks 1st on Waymo Open Dataset with single frame point cloud input. Our code is available at: https://github.com/rayguan97/M3DETR.",
                "paper_link": "https://www.semanticscholar.org/paper/10354c03ad56421c454164db6cffa5ace7545d49"
            },
            {
                "title": "Ga-nav: Efficient terrain segmentation for robot navigation in unstructured outdoor environments",
                "abstract": "We propose GA-Nav, a novel group-wise attention mechanism to identify safe and navigable regions in unstructured environments from RGB images. Our group-wise attention method extracts multi-scale features from each type of terrain independently and classifies terrains based on their navigability levels using coarse-grained semantic segmentation. Our novel loss can be embedded within any backbone network to explicitly focus on the different groups\u2019 features, at a low spatial resolution. Our design leads to efficient inference while maintaining a high level of accuracy compared to existing SOTA methods. Our extensive evaluations on the RUGD and RELLIS-3D datasets shows that GA-Nav achieves the state-of-the-art performance on RUGD and RELLIS-3D datasets. We interface GA-Nav with a deep reinforcement learning-based navigation algorithm and highlight its benefits in terms of navigation in real-world unstructured terrains. We integrate our GA-Nav-based navigation algorithm with ClearPath Jackal and Husky robots, and observe an improvement in terms of navigation success rate and better trajectory selections.",
                "paper_link": "https://www.semanticscholar.org/paper/af08d8e0e70d2ddc0fc64931a6df1f15124ec579"
            },
            {
                "title": "Phasepack: A phase retrieval library",
                "abstract": "Phase retrieval deals with the estimation of complex-valued signals solely from the magnitudes of linear measurements. While there has been a recent explosion in the development of phase retrieval algorithms, the lack of a common interface has made it difficult to compare new methods against the state-of-the-art. The purpose of PhasePack is to create a common software interface for a wide range of phase retrieval algorithms and to provide a common testbed using both synthetic data and empirical imaging datasets. PhasePack is able to benchmark a large number of recent phase retrieval methods against one another to generate comparisons using a range of different performance metrics. The software package handles single method testing as well as multiple method comparisons. The algorithm implementations in PhasePack differ slightly from their original descriptions in the literature in order to achieve faster speed and improved robustness. In particular, PhasePack uses adaptive stepsizes, line-search methods, and fast eigensolvers to speed up and automate convergence.",
                "paper_link": "https://www.semanticscholar.org/paper/922ba0a82a20d605c5d6bd3a50ab22b723dcea8d"
            },
            {
                "title": "Densecavoid: Real-time navigation in dense crowds using anticipatory behaviors",
                "abstract": "We present DenseCAvoid, a novel algorithm for navigating a robot through dense crowds and avoiding collisions by anticipating pedestrian behaviors. Our formulation uses visual sensors and a pedestrian trajectory prediction algorithm to track pedestrians in a set of input frames and compute bounding boxes that extrapolate to the pedestrian positions in a future time. Our hybrid approach combines this trajectory prediction with a Deep Reinforcement Learning-based collision avoidance method to train a policy to generate smoother, safer, and more robust trajectories during run-time. We train our policy in realistic 3-D simulations of static and dynamic scenarios with multiple pedestrians. In practice, our hybrid approach generalizes well to unseen, real-world scenarios and can navigate a robot through dense crowds (\u223c1-2 humans per square meter) in indoor scenarios, including narrow corridors and lobbies. As compared to cases where prediction was not used, we observe that our method reduces the occurrence of the robot freezing in a crowd by up to 48%, and performs comparably with respect to trajectory lengths and mean arrival times to goal.",
                "paper_link": "https://www.semanticscholar.org/paper/664475057d57f5de772e175bbfa5dc5a2fa0e29e"
            },
            {
                "title": "Robusttp: End-to-end trajectory prediction for heterogeneous road-agents in dense traffic with noisy sensor inputs",
                "abstract": "We present RobustTP, an end-to-end algorithm for predicting future trajectories of road-agents in dense traffic with noisy sensor input trajectories obtained from RGB cameras (either static or moving) through a tracking algorithm. In this case, we consider noise as the deviation from the ground truth trajectory. The amount of noise depends on the accuracy of the tracking algorithm. Our approach is designed for dense heterogeneous traffic, where the road agents corresponding to a mixture of buses, cars, scooters, bicycles, or pedestrians. RobustTP is an approach that first computes trajectories using a combination of a non-linear motion model and a deep learning-based instance segmentation algorithm. Next, these noisy trajectories are trained using an LSTM-CNN neural network architecture that models the interactions between road-agents in dense and heterogeneous traffic. Our trajectory prediction algorithm outperforms state-of-the-art methods for end-to-end trajectory prediction using sensor inputs. We achieve an improvement of upto 18% in average displacement error and an improvement of up to 35.5% in final displacement error at the end of the prediction window (5 seconds) over the next best method. All experiments were set up on an Nvidia TiTan Xp GPU. Additionally, we release a software framework, TrackNPred. The framework consists of implementations of state-of-the-art tracking and trajectory prediction methods and tools to benchmark and evaluate them on real-world dense traffic datasets.",
                "paper_link": "https://www.semanticscholar.org/paper/8081208e021eb5b5326373513fae45e309ada43c"
            },
            {
                "title": "Take an emotion walk: Perceiving emotions from gaits using hierarchical attention pooling and affective mapping",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/3a718341f5db00071de408a4bd52e92541052e13"
            },
            {
                "title": "Principles and guidelines for evaluating social robot navigation algorithms",
                "abstract": "A major challenge to deploying robots widely is navigation in human-populated environments, commonly referred to as social robot navigation. While the field of social navigation has advanced tremendously in recent years, the fair evaluation of algorithms that tackle social navigation remains hard because it involves not just robotic agents moving in static environments but also dynamic human agents and their perceptions of the appropriateness of robot behavior. In contrast, clear, repeatable, and accessible benchmarks have accelerated progress in fields like computer vision, natural language processing and traditional robot navigation by enabling researchers to fairly compare algorithms, revealing limitations of existing solutions and illuminating promising new directions. We believe the same approach can benefit social navigation. In this paper, we pave the road towards common, widely accessible, and repeatable benchmarking criteria to evaluate social robot navigation. Our contributions include (a) a definition of a socially navigating robot as one that respects the principles of safety, comfort, legibility, politeness, social competency, agent understanding, proactivity, and responsiveness to context, (b) guidelines for the use of metrics, development of scenarios, benchmarks, datasets, and simulators to evaluate social navigation, and (c) a design of a social navigation metrics framework to make it easier to compare results from different simulators, robots and datasets.",
                "paper_link": "https://www.semanticscholar.org/paper/727a235276c81cc0fb9075f0edeb5454ca381eef"
            },
            {
                "title": "Gameplan: Game-theoretic multi-agent planning with human drivers at intersections, roundabouts, and merging",
                "abstract": "We present a new method for multi-agent planning involving human drivers and autonomous vehicles (AVs) in unsignaled intersections, roundabouts, and during merging. In multi-agent planning, the main challenge is to predict the actions of other agents, especially human drivers, as their intentions are hidden from other agents. Our algorithm uses game theory to develop a new auction, called GamePlan, that directly determines the optimal action for each agent based on their driving style (which is observable via commonly available sensors). GamePlan assigns a higher priority to more aggressive or impatient drivers and a lower priority to more conservative or patient drivers; we theoretically prove that such an approach is game-theoretically optimal prevents collisions and deadlocks. We compare our approach with prior state-of-the-art auction techniques including economic auctions, time-based auctions (first-in first-out), and random bidding and show that each of these methods result in collisions among agents when taking into account driver behavior. We additionally compare with methods based on deep reinforcement learning, deep learning, and game theory and present our benefits over these approaches. Finally, we show that our approach can be implemented in the real-world with human drivers.",
                "paper_link": "https://www.semanticscholar.org/paper/9f78cacc7d3875782805c3ce9192a0c4d3b0f91a"
            },
            {
                "title": "Cmetric: A driving behavior measure using centrality functions",
                "abstract": "We present a new measure, CMetric, to classify driver behaviors using centrality functions. Our formulation combines concepts from computational graph theory and social traffic psychology to quantify and classify the behavior of human drivers. CMetric is used to compute the probability of a vehicle executing a driving style, as well as the intensity used to execute the style. Our approach is designed for realtime autonomous driving applications, where the trajectory of each vehicle or road-agent is extracted from a video. We compute a dynamic geometric graph (DGG) based on the positions and proximity of the road-agents and centrality functions corresponding to closeness and degree. These functions are used to compute the CMetric based on style likelihood and style intensity estimates. Our approach is general and makes no assumption about traffic density, heterogeneity, or how driving behaviors change over time. We present an algorithm to compute CMetric and demonstrate its performance on real-world traffic datasets. To test the accuracy of CMetric, we introduce a new evaluation protocol (called \"Time Deviation Error\") that measures the difference between human prediction and the prediction made by CMetric.",
                "paper_link": "https://www.semanticscholar.org/paper/2a7e3d8d58ddb891f524dc943454754c56b1af46"
            },
            {
                "title": "B-gap: Behavior-rich simulation and navigation for autonomous driving",
                "abstract": "We address the problem of ego-vehicle navigation in dense simulated traffic environments populated by road agents with varying driver behaviors. Navigation in such environments is challenging due to unpredictability in agents\u2019 actions caused by their heterogeneous behaviors. We present a new simulation technique consisting of enriching existing traffic simulators with behavior-rich trajectories corresponding to varying levels of aggressiveness. We generate these trajectories with the help of a driver behavior modeling algorithm. We then use the enriched simulator to train a deep reinforcement learning (DRL) policy that consists of a set of high-level vehicle control commands and use this policy at test time to perform local navigation in dense traffic. Our policy implicitly models the interactions between traffic agents and computes safe trajectories for the ego-vehicle accounting for aggressive driver maneuvers such as overtaking, over-speeding, weaving, and sudden lane changes. Our enhanced behavior-rich simulator can be used for generating datasets that consist of trajectories corresponding to diverse driver behaviors and traffic densities, and our behavior-based navigation scheme can be combined with state-of-the-art navigation algorithms.",
                "paper_link": "https://www.semanticscholar.org/paper/b360c1a116474ec64af28d2804f39be6df418718"
            },
            {
                "title": "Using graph-theoretic machine learning to predict human driver behavior",
                "abstract": "Studies have shown that autonomous vehicles (AVs) behave conservatively in a traffic environment composed of human drivers and do not adapt to local conditions and socio-cultural norms. It is known that socially aware AVs can be designed if there exists a mechanism to understand the behaviors of human drivers. We present an approach that leverages machine learning to predict, the behaviors of human drivers. This is similar to how humans implicitly interpret the behaviors of drivers on the road, by only observing the trajectories of their vehicles. We use graph-theoretic tools to extract driver behavior features from the trajectories and machine learning to obtain a computational mapping between the extracted trajectory of a vehicle in traffic and the driver behaviors. Compared to prior approaches in this domain, we prove that our method is robust, general, and extendable to broad-ranging applications such as autonomous navigation. We evaluate our approach on real-world traffic datasets captured in the U.S., India, China, and Singapore, as well as in simulation.",
                "paper_link": "https://www.semanticscholar.org/paper/f5c8d8f657d26fa896376d93a4c1d2a3a1a7a8f4"
            },
            {
                "title": "SS-SFDA: Self-supervised source-free domain adaptation for road segmentation in hazardous environments",
                "abstract": "We present a novel approach for unsupervised road segmentation in adverse weather conditions such as rain or fog. This includes a new algorithm for source-free domain adaptation (SFDA) using self-supervised learning. More-over, our approach uses several techniques to address various challenges in SFDA and improve performance, including online generation of pseudo-labels and self-attention as well as use of curriculum learning, entropy minimization and model distillation. We have evaluated the performance on 6 datasets corresponding to real and synthetic adverse weather conditions. Our method outperforms all prior works on unsupervised road segmentation and SFDA by at least 10.26%, and improves the training time by 18\u2212180\u00d7. Moreover, our self-supervised algorithm exhibits similar accuracy performance in terms of mIOU score as compared to prior supervised methods.",
                "paper_link": "https://www.semanticscholar.org/paper/e67159ba48fbb27980660614475513d4b7df44db"
            },
            {
                "title": "Graphrqi: Classifying driver behaviors using graph spectrums",
                "abstract": "We present a novel algorithm (GraphRQI) to identify driver behaviors from road-agent trajectories. Our approach assumes that the road-agents exhibit a range of driving traits, such as aggressive or conservative driving. Moreover, these traits affect the trajectories of nearby road-agents as well as the interactions between road-agents. We represent these inter-agent interactions using unweighted and undirected traffic graphs. Our algorithm classifies the driver behavior using a supervised learning algorithm by reducing the computation to the spectral analysis of the traffic graph. Moreover, we present a novel eigenvalue algorithm to compute the spectrum efficiently. We provide theoretical guarantees for the running time complexity of our eigenvalue algorithm and show that it is faster than previous methods by 2 times. We evaluate the classification accuracy of our approach on traffic videos and autonomous driving datasets corresponding to urban traffic. In practice, GraphRQI achieves an accuracy improvement of up to 25% over prior driver behavior classification algorithms. We also use our classification algorithm to predict the future trajectories of road-agents.",
                "paper_link": "https://www.semanticscholar.org/paper/d677db9be3b20c6bc91398cd035c5548fefaf06e"
            },
            {
                "title": "Densepeds: Pedestrian tracking in dense crowds using front-rvo and sparse features",
                "abstract": "We present a pedestrian tracking algorithm, DensePeds, that tracks individuals in highly dense crowds (>2 pedestrians per square meter). Our approach is designed for videos captured from front-facing or elevated cameras. We present a new motion model called Front-RVO (FRVO) for predicting pedestrian movements in dense situations using collision avoidance constraints and combine it with state-of-the-art Mask R-CNN to compute sparse feature vectors that reduce the loss of pedestrian tracks (false negatives). We evaluate DensePeds on the standard MOT benchmarks as well as a new dense crowd dataset. In practice, our approach is 4.5 \u00d7 faster than prior tracking algorithms on the MOT benchmark and we are state-of-the-art in dense crowd videos by over 2.6% on the absolute scale on average.",
                "paper_link": "https://www.semanticscholar.org/paper/9c0760f0d3a76ae8e942f38f75877288b5041d7c"
            }
        ]
    },
    {
        "Professor": "Chen Chen",
        "Papers": [
            {
                "title": "Fairness in graph mining: a survey",
                "abstract": "Graph mining algorithms have been playing a significant role in myriad fields over the years. However, despite their promising performance on various graph analytical tasks, most of these algorithms lack fairness considerations. As a consequence, they could lead to discrimination towards certain populations when exploited in human-centered applications. Recently, algorithmic fairness has been extensively studied in graph-based applications. In contrast to algorithmic fairness on independent and identically distributed (i.i.d.) data, fairness in graph mining has exclusive backgrounds, taxonomies, and fulfilling techniques. In this survey, we provide a comprehensive and up-to-date introduction of existing literature under the context of fair graph mining. Specifically, we propose a novel taxonomy of fairness notions on graphs, which sheds light on their connections and differences. We further present an organized summary of existing techniques that promote fairness in graph mining. Finally, we discuss current research challenges and open questions, aiming at encouraging cross-breeding ideas and further advances.",
                "paper_link": "https://www.semanticscholar.org/paper/8e11c4f14567d1257d4c8d8ac4e14446e1b99c36"
            },
            {
                "title": "Node Immunization on Large Graphs: Theory and Algorithms",
                "abstract": "Given a large graph, like a computer communication network, which k nodes should we immunize (or monitor, or remove), to make it as robust as possible against a computer virus attack? This problem, referred to as the node immunization problem, is the core building block in many high-impact applications, ranging from public health, cybersecurity to viral marketing. A central component in node immunization is to find the best k bridges of a given graph. In this setting, we typically want to determine the relative importance of a node (or a set of nodes) within the graph, for example, how valuable (as a bridge) a person or a group of persons is in a social network. First of all, we propose a novel `bridging' score D\u03bb, inspired by immunology, and we show that its results agree with intuition for several realistic settings. Since the straightforward way to compute D\u03bb is computationally intractable, we then focus on the computational issues and propose a surprisingly efficient way (O(nk2 + m)) to estimate it. Experimental results on real graphs show that (1) the proposed `bridging' score gives mining results consistent with intuition; and (2) the proposed fast solution is up to seven orders of magnitude faster than straightforward alternatives.",
                "paper_link": "https://www.semanticscholar.org/paper/4e085352c2d0d7080a58d27bce080fea9e76a136"
            },
            {
                "title": "Multi-Layered Network Embedding",
                "abstract": "Network embedding has gained more attentions in recent years. It has been shown that the learned low-dimensional node vector representations could advance a myriad of graph mining tasks such as node classi\ufb01-cation, community detection, and link prediction. A vast majority of the existing e\ufb00orts are overwhelmingly devoted to single-layered networks or homogeneous networks with a single type of nodes and node interactions. However, in many real-world applications, a variety of networks could be abstracted and presented in a multi-layered fashion. Typical multi-layered networks include critical infrastructure systems, collaboration platforms, social recommender systems, to name a few. Despite the widespread use of multi-layered networks, it remains a daunting task to learn vector representations of di\ufb00erent types of nodes due to the bewildering combination of both within-layer connections and cross-layer network dependencies. In this paper, we study a novel problem of multi-layered network embedding. In particular, we propose a principled framework - MANE to model both within-layer connections and cross-layer network dependencies simultaneously in a uni\ufb01ed optimization framework for embedding representation learning. Experiments on real-world multi-layered networks corroborate the e\ufb00ectiveness of the proposed framework.",
                "paper_link": "https://www.semanticscholar.org/paper/7c28b81dff1899e5a148ff57888faacc9945ab22"
            },
            {
                "title": "Knowledge editing for large language models: A survey",
                "abstract": "\n Large Language Models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently,\n Knowledge-based Model Editing\n (KME), also known as\n Knowledge Editing\n or\n Model Editing\n , has attracted increasing attention, which aims to precisely modify the LLMs to incorporate specific knowledge, without negatively influencing other irrelevant knowledge. In this survey, we aim to provide a comprehensive and in-depth overview of recent advances in the field of KME. We first introduce a general formulation of KME to encompass different KME strategies. Afterward, we provide an innovative taxonomy of KME techniques based on how the new knowledge is introduced into pre-trained LLMs, and investigate existing KME strategies while analyzing key insights, advantages, and limitations of methods from each category. Moreover, representative metrics, datasets, and applications of KME are introduced accordingly. Finally, we provide an in-depth analysis regarding the practicality and remaining challenges of KME and suggest promising research directions for further advancement in this field.\n",
                "paper_link": "https://www.semanticscholar.org/paper/42016f91e5b1da63174d45acb96bc89b64aa124d"
            },
            {
                "title": "FASCINATE: Fast Cross-Layer Dependency Inference on Multi-layered Networks",
                "abstract": "Multi-layered networks have recently emerged as a new network model, which naturally finds itself in many high-impact application domains, ranging from critical inter-dependent infrastructure networks, biological systems, organization-level collaborations, to cross-platform e-commerce, etc. Cross-layer dependency, which describes the dependencies or the associations between nodes across different layers/networks, often plays a central role in many data mining tasks on such multi-layered networks. Yet, it remains a daunting task to accurately know the cross-layer dependency a prior. In this paper, we address the problem of inferring the missing cross-layer dependencies on multi-layered networks. The key idea behind our method is to view it as a collective collaborative filtering problem. By formulating the problem into a regularized optimization model, we propose an effective algorithm to find the local optima with linear complexity. Furthermore, we derive an online algorithm to accommodate newly arrived nodes, whose complexity is just linear wrt the size of the neighborhood of the new node. We perform extensive empirical evaluations to demonstrate the effectiveness and the efficiency of the proposed methods.",
                "paper_link": "https://www.semanticscholar.org/paper/727fa1f55462f732bcc8e3ae41a119d24e38bd69"
            },
            {
                "title": "Federated Graph Machine Learning: A Survey of Concepts, Techniques, and Applications",
                "abstract": "The integration of Artificial Intelligence (AI) into the field of drug discovery has been a growing area of interdisciplinary scientific research. However, conventional AI models are heavily limited in handling complex biomedical structures (such as 2D or 3D protein and molecule structures) and providing interpretations for outputs, which hinders their practical application. As of late, Graph Machine Learning (GML) has gained considerable attention for its exceptional ability to model graph-structured biomedical data and investigate their properties and functional relationships. Despite extensive efforts, GML methods still suffer from several deficiencies, such as the limited ability to handle supervision sparsity and provide interpretability in learning and inference processes, and their ineffectiveness in utilising relevant domain knowledge. In response, recent studies have proposed integrating external biomedical knowledge into the GML pipeline to realise more precise and interpretable drug discovery with limited training instances. However, a systematic definition for this burgeoning research direction is yet to be established. This survey presents a comprehensive overview of long-standing drug discovery principles, provides the foundational concepts and cutting-edge techniques for graph-structured data and knowledge databases, and formally summarises Knowledge-augmented Graph Machine Learning (KaGML) for drug discovery. we propose a thorough review of related KaGML works, collected following a carefully designed search methodology, and organise them into four categories following a novel-defined taxonomy. To facilitate research in this promptly emerging field, we also share collected practical resources that are valuable for intelligent drug discovery and provide an in-depth discussion of the potential avenues for future advancements.",
                "paper_link": "https://www.semanticscholar.org/paper/bb6dff2b47784f92f6bda57abc69ed346ec44be9"
            },
            {
                "title": "Eigen-Optimization on Large Graphs by Edge Manipulation",
                "abstract": "Large graphs are prevalent in many applications and enable a variety of information dissemination processes, e.g., meme, virus, and influence propagation. How can we optimize the underlying graph structure to affect the outcome of such dissemination processes in a desired way (e.g., stop a virus propagation, facilitate the propagation of a piece of good idea, etc)? Existing research suggests that the leading eigenvalue of the underlying graph is the key metric in determining the so-called epidemic threshold for a variety of dissemination models. In this paper, we study the problem of how to optimally place a set of edges (e.g., edge deletion and edge addition) to optimize the leading eigenvalue of the underlying graph, so that we can guide the dissemination process in a desired way. We propose effective, scalable algorithms for edge deletion and edge addition, respectively. In addition, we reveal the intrinsic relationship between edge deletion and node deletion problems. Experimental results validate the effectiveness and efficiency of the proposed algorithms.",
                "paper_link": "https://www.semanticscholar.org/paper/3b8227f54a2237a0b9386d9359b7f0cf66f6af07"
            },
            {
                "title": "REFORM: Error-Aware Few-Shot Knowledge Graph Completion",
                "abstract": "Knowledge graphs (KGs) are of great importance in various artificial intelligence systems, such as question answering, relation extraction, and recommendation. Nevertheless, most real-world KGs are highly incomplete, with many missing relations between entities. To discover new triples (i.e., head entity, relation, tail entity), many KG completion algorithms have been proposed in recent years. However, a vast majority of existing studies often require a large number of training triples for each relation, which contradicts the fact that the frequency distribution of relations in KGs often follows a long tail distribution, meaning a majority of relations have only very few triples. Meanwhile, since most existing large-scale KGs are constructed automatically by extracting information from crowd-sourcing data using heuristic algorithms, plenty of errors could be inevitably incorporated due to the lack of human verification, which greatly reduces the performance for KG completion. To tackle the aforementioned issues, in this paper, we study a novel problem of error-aware few-shot KG completion and present a principled KG completion framework REFORM. Specifically, we formulate the problem under the few-shot learning framework, and our goal is to accumulate meta-knowledge across different meta-tasks and generalize the accumulated knowledge to the meta-test task for error-aware few-shot KG completion. To address the associated challenges resulting from insufficient training samples and inevitable errors, we propose three essential modules neighbor encoder, cross-relation aggregation, and error mitigation in each meta-task. Extensive experiments on three widely used KG datasets demonstrate the superiority of the proposed framework REFORM over competitive baseline methods.",
                "paper_link": "https://www.semanticscholar.org/paper/708723c74c6d957e241e833270b415fedbb3bb60"
            },
            {
                "title": "Task-Adaptive Few-shot Node Classification",
                "abstract": "Node classification is of great importance among various graph mining tasks. In practice, real-world graphs generally follow the long-tail distribution, where a large number of classes only consist of limited labeled nodes. Although Graph Neural Networks (GNNs) have achieved significant improvements in node classification, their performance decreases substantially in such a few-shot scenario. The main reason can be attributed to the vast generalization gap between meta-training and meta-test due to the task variance caused by different node/class distributions in meta-tasks (i.e., node-level and class-level variance). Therefore, to effectively alleviate the impact of task variance, we propose a task-adaptive node classification framework under the few-shot learning setting. Specifically, we first accumulate meta-knowledge across classes with abundant labeled nodes. Then we transfer such knowledge to the classes with limited labeled nodes via our proposed task-adaptive modules. In particular, to accommodate the different node/class distributions among meta-tasks, we propose three essential modules to perform node-level, class-level, and task-level adaptations in each meta-task, respectively. In this way, our framework can conduct adaptations to different meta-tasks and thus advance the model generalization performance on meta-test tasks. Extensive experiments on four prevalent node classification datasets demonstrate the superiority of our framework over the state-of-the-art baselines. Our code is provided at https://github.com/SongW-SW/TENT https://github.com/SongW-SW/TENT.",
                "paper_link": "https://www.semanticscholar.org/paper/6bb052d6359f96a09fb135dd181624a66caaca46"
            },
            {
                "title": "Deconfounding with Networked Observational Data in A Dynamic Environment",
                "abstract": "One fundamental problem in causal inference is to learn the individual treatment effects (ITE) -- assessing the causal effects of a certain treatment (e.g., prescription of medicine) on an important outcome (e.g., cure of a disease) for each data instance, but the effectiveness of most existing methods is often limited due to the existence of hidden confounders. Recent studies have shown that the auxiliary relational information among data can be utilized to mitigate the confounding bias. However, these works assume that the observational data and the relations among them are static, while in reality, both of them will continuously evolve over time and we refer such data as time-evolving networked observational data. In this paper, we make an initial investigation of ITE estimation on such data. The problem remains difficult due to the following challenges: (1) modeling the evolution patterns of time-evolving networked observational data; (2) controlling the hidden confounders with current data and historical information; (3) alleviating the discrepancy between the control group and the treated group. To tackle these challenges, we propose a novel ITE estimation framework Dynamic Networked Observational Data Deconfounder (\\mymodel) which aims to learn representations of hidden confounders over time by leveraging both current networked observational data and historical information. Additionally, a novel adversarial learning based representation balancing method is incorporated toward unbiased ITE estimation. Extensive experiments validate the superiority of our framework when measured against state-of-the-art baselines. The implementation can be accessed in \\hrefhttps://github.com/jma712/DNDC https://github.com/jma712/DNDC.",
                "paper_link": "https://www.semanticscholar.org/paper/810096084c00c4c5efe24294170150ae858fb278"
            },
            {
                "title": "Fast Eigen-Functions Tracking on Dynamic Graphs",
                "abstract": "Many important graph parameters can be expressed as eigenfunctions of its adjacency matrix. Examples include epidemic threshold, graph robustness, etc. It is often of key importance to accurately monitor these parameters. For example, knowing that Ebola virus has already been brought to the US continent, to avoid the virus from spreading away, it is important to know which emerging connections among related people would cause great reduction on the epidemic threshold of the network. However, most, if not all, of the existing algorithms computing these measures assume that the input graph is static, despite the fact that almost all real graphs are evolving over time. In this paper, we propose two online algorithms to track the eigen-functions of a dynamic graph with linear complexity wrt the number of nodes and number of changed edges in the graph. The key idea is to leverage matrix perturbation theory to efficiently update the top eigen-pairs of the underlying graph without recomputing them from scratch at each time stamp. Experiment results demonstrate that our methods can reach up to 20\u00d7 speedup with precision more than 80% for fairly long period of time.",
                "paper_link": "https://www.semanticscholar.org/paper/ddc3dc43095173a6b5f0a6f5fbdde79e1abac34d"
            },
            {
                "title": "Understanding the Coevolution of Mask Wearing and Epidemics: A Network Perspective",
                "abstract": "Significance Nonpharmaceutical interventions such as mask wearing play a critical role in reducing disease prevalence. Under the dueling dynamics of mask wearing and disease, we observe a robust nonmonotonic relationship between the attack rate (i.e., the fraction of the ever-infected population) and the transmission probability of the disease. Specifically, the attack rate exhibits an abrupt reduction as the transmission probability increases to a critical threshold. Furthermore, we characterize regimes of the transmission probability where multiple waves of infection and mask adoption are expected. Our results highlight the necessity of continued public mask-wearing mandates to suppress the epidemic and effectively prevent its revival.",
                "paper_link": "https://www.semanticscholar.org/paper/e369579e01761e61446bf65a7429a2326e222662"
            },
            {
                "title": "On the Connectivity of Multi-layered Networks: Models, Measures and Optimal Control",
                "abstract": "Networks appear naturally in many high-impact real-world applications. In an increasingly connected and coupled world, the networks arising from many application domains are often collected from different channels, forming the so-called multi-layered networks, such as cyber-physical systems, organization-level collaboration platforms, critical infrastructure networks and many more. Compared with single-layered networks, multi-layered networks are more vulnerable as even a small disturbance on one supporting layer/network might cause a ripple effect to all the dependent layers, leading to a catastrophic/cascading failure of the entire system. The state-of-the-art has been largely focusing on modeling and manipulating the cascading effect of two-layered interdependent network systems for some specific type of network connectivity measure. This paper generalizes the challenge to multiple dimensions. First, we propose a new data model for multi-layered networks MULAN, which admits an arbitrary number of layers with a much more flexible dependency structure among different layers, beyond the current pair-wise dependency. Second, we unify a wide range of classic network connectivity measures SUBLINE. Third, we show that for any connectivity measure in the SUBLINE family, it enjoys the diminishing returns property which in turn lends itself to a family of provable near-optimal control algorithms with linear complexity. Finally, we conduct extensive empirical evaluations on real network data, to validate the effectiveness of the proposed algorithms.",
                "paper_link": "https://www.semanticscholar.org/paper/01a0cb74b39fdc625c7925dc658452f9e382b2ec"
            },
            {
                "title": "Towards Optimal Connectivity on Multi-layered Networks",
                "abstract": "Networks are prevalent in many high impact domains. Moreover, cross-domain interactions are frequently observed in many applications, which naturally form the dependencies between different networks. Such kind of highly coupled network systems are referred to as multi-layered networks, and have been used to characterize various complex systems, including critical infrastructure networks, cyber-physical systems, collaboration platforms, biological systems, and many more. Different from single-layered networks where the functionality of their nodes is mainly affected by within-layer connections, multi-layered networks are more vulnerable to disturbance as the impact can be amplified through cross-layer dependencies, leading to the cascade failure to the entire system. To manipulate the connectivity in multi-layered networks, some recent methods have been proposed based on two-layered networks with specific types of connectivity measures. In this paper, we address the above challenges in multiple dimensions. First, we propose a family of connectivity measures (SubLine) that unifies a wide range of classic network connectivity measures. Third, we reveal that the connectivity measures in the  SubLine family enjoy diminishing returns property, which guarantees a near-optimal solution with linear complexity for the connectivity optimization problem. Finally, we evaluate our proposed algorithm on real data sets to demonstrate its effectiveness and efficiency.",
                "paper_link": "https://www.semanticscholar.org/paper/9ce8b8fc376515f37e5baa98b4723bb1d9d0482f"
            },
            {
                "title": "FAITH: Few-Shot Graph Classification with Hierarchical Task Graphs",
                "abstract": "Few-shot graph classification aims at predicting classes for graphs, given limited labeled graphs for each class. To tackle the bottleneck of label scarcity, recent works propose to incorporate few-shot learning frameworks for fast adaptations to graph classes with limited labeled graphs. Specifically, these works propose to accumulate meta-knowledge across diverse meta-training tasks, and then generalize such meta-knowledge to the target task with a disjoint label set. However, existing methods generally ignore task correlations among meta-training tasks while treating them independently. Nevertheless, such task correlations can advance the model generalization to the target task for better classification performance. On the other hand, it remains non-trivial to utilize task correlations due to the complex components in a large number of meta-training tasks. To deal with this, we propose a novel few-shot learning framework FAITH that captures task correlations via constructing a hierarchical task graph at different granularities. Then we further design a loss-based sampling strategy to select tasks with more correlated classes. Moreover, a task-specific classifier is proposed to utilize the learned task correlations for few-shot classification. Extensive experiments on four prevalent few-shot graph classification datasets demonstrate the superiority of FAITH over other state-of-the-art baselines.",
                "paper_link": "https://www.semanticscholar.org/paper/8e2bfba2487d4b57f6511776a7772e65ee6115ab"
            },
            {
                "title": "Network Connectivity Optimization: Fundamental Limits and Effective Algorithms",
                "abstract": "Network connectivity optimization, which aims to manipulate network connectivity by changing its underlying topology, is a fundamental task behind a wealth of high-impact data mining applications, ranging from immunization, critical infrastructure construction, social collaboration mining, bioinformatics analysis, to intelligent transportation system design. To tackle its exponential computation complexity, greedy algorithms have been extensively used for network connectivity optimization by exploiting its diminishing returns property. Despite the empirical success, two key challenges largely remain open. First, on the theoretic side, the hardness, as well as the approximability of the general network connectivity optimization problem are still nascent except for a few special instances. Second, on the algorithmic side, current algorithms are often hard to balance between the optimization quality and the computational efficiency. In this paper, we systematically address these two challenges for the network connectivity optimization problem. First, we reveal some fundamental limits by proving that, for a wide range of network connectivity optimization problems, (1) they are NP-hard and (2) (1-1/e) is the optimal approximation ratio for any polynomial algorithms. Second, we propose an effective, scalable and general algorithm (CONTAIN) to carefully balance the optimization quality and the computational efficiency.",
                "paper_link": "https://www.semanticscholar.org/paper/d3f63363219b55f2823ae4c1abbb1dd246042843"
            },
            {
                "title": "Federated Few-shot Learning",
                "abstract": "Federated Learning (FL) enables multiple clients to collaboratively learn a machine learning model without exchanging their own local data. In this way, the server can exploit the computational power of all clients and train the model on a larger set of data samples among all clients. Although such a mechanism is proven to be effective in various fields, existing works generally assume that each client preserves sufficient data for training. In practice, however, certain clients can only contain a limited number of samples (i.e., few-shot samples). For example, the available photo data taken by a specific user with a new mobile device is relatively rare. In this scenario, existing FL efforts typically encounter a significant performance drop on these clients. Therefore, it is urgent to develop a few-shot model that can generalize to clients with limited data under the FL scenario. In this paper, we refer to this novel problem as federated few-shot learning. Nevertheless, the problem remains challenging due to two major reasons: the global data variance among clients (i.e., the difference in data distributions among clients) and the local data insufficiency in each client (i.e., the lack of adequate local data for training). To overcome these two challenges, we propose a novel federated few-shot learning framework with two separately updated models and dedicated training strategies to reduce the adverse impact of global data variance and local data insufficiency. Extensive experiments on four prevalent datasets that cover news articles and images validate the effectiveness of our framework compared with the state-of-the-art baselines.",
                "paper_link": "https://www.semanticscholar.org/paper/62420cf61f84cc934a85960e26c615d504c4ee63"
            },
            {
                "title": "On the Eigen-Functions of Dynamic Graphs: Fast Tracking and Attribution Algorithms",
                "abstract": "Eigen\u2010functions are of key importance in graph mining since they can be used to approximate many graph parameters, such as node centrality, epidemic threshold, graph robustness, with high accuracy. As real\u2010world graphs are changing over time, those parameters may get sharp changes correspondingly. Taking virus propagation network for example, new connections between infected and susceptible people appear all the time, and some of the crucial infections may lead to large decreasing on the epidemic threshold of the network. As a consequence, the virus would spread around the network quickly. However, if we can keep track of the epidemic threshold as the graph structure changes, those crucial infections would be identified timely so that counter measures can be taken proactively to contain the spread process. In our paper, we propose two online eigen\u2010functions tracking algorithms which can effectively monitor those key parameters with linear complexity. Furthermore, we propose a general attribution analysis framework which can be used to identify important structural changes in the evolving process. In addition, we introduce an error estimation method for the proposed eigen\u2010functions tracking algorithms to estimate the tracking error at each time stamp. Finally, extensive evaluations are conducted to validate the effectiveness and efficiency of the proposed algorithms. \u00a9 2016 Wiley Periodicals, Inc. Statistical Analysis and Data Mining: The ASA Data Science Journal, 2016",
                "paper_link": "https://www.semanticscholar.org/paper/3ae8f410d9838d995b5f5ac5d62e65038c712aee"
            },
            {
                "title": "Few-shot Node Classification with Extremely Weak Supervision",
                "abstract": "Few-shot node classification aims at classifying nodes with limited labeled nodes as references. Recent few-shot node classification methods typically learn from classes with abundant labeled nodes (i.e., meta-training classes) and then generalize to classes with limited labeled nodes (i.e., meta-test classes). Nevertheless, on real-world graphs, it is usually difficult to obtain abundant labeled nodes for many classes. In practice, each meta-training class can only consist of several labeled nodes, known as the extremely weak supervision problem. In few-shot node classification, with extremely limited labeled nodes for meta-training, the generalization gap between meta-training and meta-test will become larger and thus lead to suboptimal performance. To tackle this issue, we study a novel problem of few-shot node classification with extremely weak supervision and propose a principled framework X-FNC under the prevalent meta-learning framework. Specifically, our goal is to accumulate meta-knowledge across different meta-training tasks with extremely weak supervision and generalize such knowledge to meta-test tasks. To address the challenges resulting from extremely scarce labeled nodes, we propose two essential modules to obtain pseudo-labeled nodes as extra references and effectively learn from extremely limited supervision information. We further conduct extensive experiments on four node classification datasets with extremely weak supervision to validate the superiority of our framework compared to the state-of-the-art baselines.",
                "paper_link": "https://www.semanticscholar.org/paper/bd20e6b879130079e41b8dfa5ad0b062d53b7988"
            },
            {
                "title": "Graph Few-shot Learning with Task-specific Structures",
                "abstract": "Graph few-shot learning is of great importance among various graph learning tasks. Under the few-shot scenario, models are often required to conduct classification given limited labeled samples. Existing graph few-shot learning methods typically leverage Graph Neural Networks (GNNs) and perform classification across a series of meta-tasks. Nevertheless, these methods generally rely on the original graph (i.e., the graph that the meta-task is sampled from) to learn node representations. Consequently, the graph structure used in each meta-task is identical. Since the class sets are different across meta-tasks, node representations should be learned in a task-specific manner to promote classification performance. Therefore, to adaptively learn node representations across meta-tasks, we propose a novel framework that learns a task-specific structure for each meta-task. To handle the variety of nodes across meta-tasks, we extract relevant nodes and learn task-specific structures based on node influence and mutual information. In this way, we can learn node representations with the task-specific structure tailored for each meta-task. We further conduct extensive experiments on five node classification datasets under both single- and multiple-graph settings to validate the superiority of our framework over the state-of-the-art baselines. Our code is provided at https://github.com/SongW-SW/GLITTER.",
                "paper_link": "https://www.semanticscholar.org/paper/c867609d5f373cb73ec7b4cad125a6b8758fc865"
            }
        ]
    },
    {
        "Professor": "Yue Cheng",
        "Papers": [
            {
                "title": "Tifl: A tier-based federated learning system",
                "abstract": "Federated Learning (FL) enables learning a shared model acrossmany clients without violating the privacy requirements. One of the key attributes in FL is the heterogeneity that exists in both resource and data due to the differences in computation and communication capacity, as well as the quantity and content of data among different clients. We conduct a case study to show that heterogeneity in resource and data has a significant impact on training time and model accuracy in conventional FL systems. To this end, we propose TiFL, a Tier-based Federated Learning System, which divides clients into tiers based on their training performance and selects clients from the same tier in each training round to mitigate the straggler problem caused by heterogeneity in resource anddata quantity. To further tame the heterogeneity caused by non-IID (Independent and Identical Distribution) data and resources, TiFL employs an adaptive tier selection approach to update the tiering on-the-fly based on the observed training performance and accuracy. We prototype TiFL in a FL testbed following Google's FL architecture and evaluate it using the state-of-the-art FL benchmarks. Experimental evaluation shows that TiFL outperforms the conventional FL in various heterogeneous conditions. With the proposed adaptive tier selection policy, we demonstrate that TiFL achieves much faster training performance while achieving the same or better test accuracy across the board.",
                "paper_link": "https://www.semanticscholar.org/paper/e3791c1083eea118fe68575648127f76aab9a20c"
            },
            {
                "title": "FedAT: A High-Performance and Communication-Efficient Federated Learning System with Asynchronous Tiers",
                "abstract": "Federated learning (FL) involves training a model over massive distributed devices, while keeping the training data localized and private. This form of collaborative learning exposes new tradeoffs among model convergence speed, model accuracy, balance across clients, and communication cost, with new challenges including: (1) straggler problem-where clients lag due to data or (computing and network) resource heterogeneity, and (2) communication bottleneck-where a large number of clients communicate their local updates to a central server and bottleneck the server. Many existing FL methods focus on optimizing along only one single dimension of the tradeoff space. Existing solutions use asynchronous model updating or tiering-based, synchronous mechanisms to tackle the straggler problem. However, asynchronous methods can easily create a communication bottleneck, while tiering may introduce biases that favor faster tiers with shorter response latencies. To address these issues, we present FedAT, a novel Federated learning system with Asynchronous Tiers under Non-i.i.d. training data. FedAT synergistically combines synchronous, intra-tier training and asynchronous, cross-tier training. By bridging the synchronous and asynchronous training through tiering, FedAT minimizes the straggler effect with improved convergence speed and test accuracy. FedAT uses a straggler-aware, weighted aggregation heuristic to steer and balance the training across clients for further accuracy improvement. FedAT compresses uplink and downlink communications using an efficient, polyline-encoding-based compression algorithm, which minimizes the communication cost. Results show that FedAT improves the prediction performance by up to 21.09% and reduces the communication cost by up to 8.5\u00d7, compared to state-of-the-art FL methods.",
                "paper_link": "https://www.semanticscholar.org/paper/18f0d5122e59d05c258b26169ff9d3aded196588"
            },
            {
                "title": "Wukong: A scalable and locality-enhanced framework for serverless parallel computing",
                "abstract": "Executing complex, burst-parallel, directed acyclic graph (DAG) jobs poses a major challenge for serverless execution frameworks, which will need to rapidly scale and schedule tasks at high throughput, while minimizing data movement across tasks. We demonstrate that, for serverless parallel computations, decentralized scheduling enables scheduling to be distributed across Lambda executors that can schedule tasks in parallel, and brings multiple benefits, including enhanced data locality, reduced network I/Os, automatic resource elasticity, and improved cost effectiveness. We describe the implementation and deployment of our new serverless parallel framework, called Wukong, on AWS Lambda. We show that Wukong achieves near-ideal scalability, executes parallel computation jobs up to 68.17X faster, reduces network I/O by multiple orders of magnitude, and achieves 92.96% tenant-side cost savings compared to numpywren.",
                "paper_link": "https://www.semanticscholar.org/paper/5d36abeacc3e839d7271ec25d503cdd31806139e"
            },
            {
                "title": "InfiniCache: Exploiting Ephemeral Serverless Functions to Build a Cost-Effective Memory Cache",
                "abstract": "Internet-scale web applications are becoming increasingly storage-intensive and rely heavily on in-memory object caching to attain required I/O performance. We argue that the emerging serverless computing paradigm provides a well-suited, cost-effective platform for object caching. We present InfiniCache, a first-of-its-kind in-memory object caching system that is completely built and deployed atop ephemeral serverless functions. InfiniCache exploits and orchestrates serverless functions' memory resources to enable elastic pay-per-use caching. InfiniCache's design combines erasure coding, intelligent billed duration control, and an efficient data backup mechanism to maximize data availability and cost-effectiveness while balancing the risk of losing cached state and performance. We implement InfiniCache on AWS Lambda and show that it: (1) achieves 31 -- 96X tenant-side cost savings compared to AWS ElastiCache for a large-object-only production workload, (2) can effectively provide 95.4% data availability for each one hour window, and (3) enables comparative performance seen in a typical in-memory cache.",
                "paper_link": "https://www.semanticscholar.org/paper/3ccbf5113f15c1e54a2df46de62db6ea7044853e"
            },
            {
                "title": "FaaSNet: Scalable and Fast Provisioning of Custom Serverless Container Runtimes at Alibaba Cloud Function Compute",
                "abstract": "Serverless computing, or Function-as-a-Service (FaaS), enables a new way of building and scaling applications by allowing users to deploy fine-grained functions while providing fully-managed resource provisioning and auto-scaling. Custom FaaS container support is gaining traction as it enables better control over OSes, versioning, and tooling for modernizing FaaS applications. However, providing rapid container provisioning introduces non-trivial challenges for FaaS providers, since container provisioning is costly, and real-world FaaS workloads exhibit highly dynamic patterns. In this paper, we design FaaSNet, a highly-scalable middleware system for accelerating FaaS container provisioning. FaaSNet is driven by the workload and infrastructure requirements of the FaaS platform at one of the world's largest cloud providers, Alibaba Cloud Function Compute. FaaSNet enables scalable container provisioning via a lightweight, adaptive function tree (FT) structure. FaaSNet uses an I/O efficient, on-demand fetching mechanism to further reduce provisioning costs at scale. We implement and integrate FaaSNet in Alibaba Cloud Function Compute. Evaluation results show that FaaSNet: (1) finishes provisioning 2500 function containers on 1000 virtual machines in 8.3 seconds, (2) scales 13.4x and 16.3x faster than Alibaba Cloud's current FaaS platform and a state-of-the-art P2P container registry (Kraken), respectively, and (3) sustains a bursty workload using 75.2% less time than an optimized baseline.",
                "paper_link": "https://www.semanticscholar.org/paper/c423df1bd716ca5b6b6e302ae801e18fa7f5285b"
            },
            {
                "title": "Improving docker registry design based on production workload analysis",
                "abstract": "Containers offer an efficient way to run workloads as independent microservices that can be developed, tested and deployed in an agile manner. To facilitate this process, container frameworks offer a registry service that enables users to publish and version container images and share them with others. The registry service plays a critical role in the startup time of containers since many container starts entail the retrieval of container images from a registry. To support research efforts on optimizing the registry service, large-scale and realistic traces are required. In this paper, we perform a comprehensive characterization of a large-scale registry workload based on traces that we collected over the course of 75 days from five IBM data centers hosting production-level registries. We present a trace replayer to perform our analysis and infer a number of crucial insights about container workloads, such as request type distribution, access patterns, and response times. Based on these insights, we derive design implications for the registry and demonstrate their ability to improve performance. Both the traces and the replayer are open-sourced to facilitate further research.",
                "paper_link": "https://www.semanticscholar.org/paper/969f61aa00a4f39f9303238cdaba7f5ae6cfdd03"
            },
            {
                "title": "Towards Taming the Resource and Data Heterogeneity in Federated Learning",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/b015e0ca242f8c9ef06e8de2c3e51b42c95b37b7"
            },
            {
                "title": "Characterizing Co-located Datacenter Workloads: An Alibaba Case Study",
                "abstract": "Warehouse-scale cloud datacenters co-locate workloads with different and often complementary characteristics for improved resource utilization. To better understand the challenges in managing such intricate, heterogeneous workloads while providing quality-assured resource orchestration and user experience, we analyze Alibaba's co-located workload trace, the first publicly available dataset with precise information about the category of each job. Two types of workload---long-running, user-facing, containerized production jobs, and transient, highly dynamic, non-containerized, and non-production batch jobs---are running on a shared cluster of 1313 machines. Our multifaceted analysis reveals insights that we believe are useful for system designers and IT practitioners working on cluster management systems.",
                "paper_link": "https://www.semanticscholar.org/paper/84c4a9ee434ec22140061baeed97a61b18f2aa8f"
            },
            {
                "title": "An in-memory object caching framework with adaptive load balancing",
                "abstract": "The extreme latency and throughput requirements of modern web applications are driving the use of distributed in-memory object caches such as Memcached. While extant caching systems scale-out seamlessly, their use in the cloud --- with its unique cost and multi-tenancy dynamics --- presents unique opportunities and design challenges. In this paper, we propose MBal, a high-performance in-memory object caching framework with adaptive Multiphase load Balancing, which supports not only horizontal (scale-out) but vertical (scale-up) scalability as well. MBal is able to make efficient use of available resources in the cloud through its fine-grained, partitioned, lockless design. This design also lends itself naturally to provide adaptive load balancing both within a server and across the cache cluster through an event-driven, multi-phased load balancer. While individual load balancing approaches are being lever-aged in in-memory caches, MBal goes beyond the extant systems and offers a holistic solution wherein the load balancing model tracks hotspots and applies different strategies based on imbalance severity -- key replication, server-local or cross-server coordinated data migration. Performance evaluation on an 8-core commodity server shows that compared to a state-of-the-art approach, MBal scales with number of cores and executes 2.3x and 12x more queries/second for GET and SET operations, respectively.",
                "paper_link": "https://www.semanticscholar.org/paper/51098280164dcc12b1ef69632430a8a362b70452"
            },
            {
                "title": "CAST: Tiering Storage for Data Analytics in the Cloud",
                "abstract": "Enterprises are increasingly moving their big data analytics to the cloud with the goal of reducing costs without sacrificing application performance. Cloud service providers offer their tenants a myriad of storage options, which while flexible, makes the choice of storage deployment non trivial. Crafting deployment scenarios to leverage these choices in a cost-effective manner - under the unique pricing models and multi-tenancy dynamics of the cloud environment - presents unique challenges in designing cloud-based data analytics frameworks. In this paper, we propose CAST, a Cloud Analytics Storage Tiering solution that cloud tenants can use to reduce monetary cost and improve performance of analytics workloads. The approach takes the first step towards providing storage tiering support for data analytics in the cloud. CAST performs offline workload profiling to construct job performance prediction models on different cloud storage services, and combines these models with workload specifications and high-level tenant goals to generate a cost-effective data placement and storage provisioning plan. Furthermore, we build CAST++ to enhance CAST's optimization model by incorporating data reuse patterns and across-jobs interdependencies common in realistic analytics workloads. Tests with production workload traces from Facebook and a 400-core Google Cloud based Hadoop cluster demonstrate that CAST++ achieves 1.21X performance and reduces deployment costs by 51.4% compared to local storage configuration.",
                "paper_link": "https://www.semanticscholar.org/paper/f81a58ac4ae01f731b53259c41a40102076eaed4"
            },
            {
                "title": "Erasing Belady\u2019s Limitations: In Search of Flash Cache Offline Optimality",
                "abstract": "NAND-based solid-state (flash) drives are known for providing better performance than magnetic disk drives, but they have limits on endurance, the number of times data can be erased and overwritten. Furthermore, the unit of erasure can be many times larger than the basic unit of I/O; this leads to complexity with respect to consolidating live data and erasing obsolete data. When flash drives are used as a cache for a larger, disk-based storage system, the choice of a cache replacement algorithm can make a significant difference in both performance and endurance. While there are many cache replacement algorithms, their effectiveness is hard to judge due to the lack of a baseline against which to compare them: Belady's MIN, the usual offline best-case algorithm, considers read hit ratio but not endurance. \n \nWe explore offline algorithms for flash caching in terms of both hit ratio and flash lifespan. We design and implement a multi-stage heuristic by synthesizing several techniques that manage data at the granularity of a flash erasure unit (which we call a container) to approximate the offline optimal algorithm. We find that simple techniques contribute most of the available erasure savings. Our evaluation shows that the container-optimized offline heuristic is able to provide the same optimal read hit ratio as MIN with 67% fewer flash erasures. More fundamentally, our investigation provides a useful approximate baseline for evaluating any online algorithm, highlighting the importance of comparing new policies for caching compound blocks in flash.",
                "paper_link": "https://www.semanticscholar.org/paper/3a0f1485dc13ad3a1cf78900be809f3d040be6eb"
            },
            {
                "title": "Beyond efficiency: A systematic survey of resource-efficient large language models",
                "abstract": "The burgeoning field of Large Language Models (LLMs), exemplified by sophisticated models like OpenAI's ChatGPT, represents a significant advancement in artificial intelligence. These models, however, bring forth substantial challenges in the high consumption of computational, memory, energy, and financial resources, especially in environments with limited resource capabilities. This survey aims to systematically address these challenges by reviewing a broad spectrum of techniques designed to enhance the resource efficiency of LLMs. We categorize methods based on their optimization focus: computational, memory, energy, financial, and network resources and their applicability across various stages of an LLM's lifecycle, including architecture design, pretraining, finetuning, and system design. Additionally, the survey introduces a nuanced categorization of resource efficiency techniques by their specific resource types, which uncovers the intricate relationships and mappings between various resources and corresponding optimization techniques. A standardized set of evaluation metrics and datasets is also presented to facilitate consistent and fair comparisons across different models and techniques. By offering a comprehensive overview of the current sota and identifying open research avenues, this survey serves as a foundational reference for researchers and practitioners, aiding them in developing more sustainable and efficient LLMs in a rapidly evolving landscape.",
                "paper_link": "https://www.semanticscholar.org/paper/8983da8428099b9b7350d17de992826f99f6aa7a"
            },
            {
                "title": "Mos: Workload-aware elasticity for cloud object stores",
                "abstract": "The use of cloud object stores has been growing rapidly in recent years as they combine key advantages such as HTTP-based RESTful APIs, high availability, elasticity with a \"pay-as-you-go\" pricing model that allows applications to scale as needed. The current practice is to either use a single set of configuration parameters or rely on statically configured storage policies for a cloud object store deployment, even when the store is used to support different types of applications with evolving requirements. This crucial mismatch between the different applications requirements and capabilities of the object store is problematic and should be addressed to achieve high efficiency and performance. In this paper, we propose MOS, a Micro Object Storage architecture, which supports independently configured microstores each tuned dynamically to the needs of a particular type of workload. We also design an enhancement, MOS++, that extends MOS's capabilities through fine-grained resource management to effectively meet the tenants' SLAs while maximizing resource efficiency. We have implemented a prototype of MOS ++ in OpenStack Swift using Docker containers. Our evaluation shows that MOS ++ can effectively support heterogeneous workloads across multiple tenants. Compared to default and statically configured object store setups, for a two-tenant setup, MOS++ improves the sustained access bandwidth by up to 79% for a large-object workload, while reducing the 95th percentile latency by up to 70.2% for a small-object workload.",
                "paper_link": "https://www.semanticscholar.org/paper/28eb88b180674f43381ede3e9573689496cfd321"
            },
            {
                "title": "In Search of a Fast and Efficient Serverless DAG Engine",
                "abstract": "Python-written data analytics applications can be modeled as and compiled into a directed acyclic graph (DAG) based workflow, where the nodes are fine-grained tasks and the edges are task dependencies.Such analytics workflow jobs are increasingly characterized by short, fine-grained tasks with large fan-outs. These characteristics make them well-suited for a new cloud computing model called serverless computing or Function-as-a-Service (FaaS), which has become prevalent in recent years. The auto-scaling property of serverless computing platforms accommodates short tasks and bursty workloads, while the pay-per-use billing model of serverless computing providers keeps the cost of short tasks low. In this paper, we thoroughly investigate the problem space of DAG scheduling in serverless computing. We identify and evaluate a set of techniques to make DAG schedulers serverless-aware. These techniques have been implemented in WUKONG , a serverless, DAG scheduler attuned to AWS Lambda. WUKONG provides decentralized scheduling through a combination of static and dynamic scheduling. We present the results of an empirical study in which WUKONG is applied to a range of microbenchmark and real-world DAG applications. Results demonstrate the efficacy of WUKONG in minimizing the performance overhead introduced by AWS Lambda \u2014 WUKONG achieves competitive performance compared to a serverful DAG scheduler, while improving the performance of real-world DAG jobs by as much as 4.1x at larger scale.",
                "paper_link": "https://www.semanticscholar.org/paper/4f36c0d93b11552af0f0db7f03992d915a2a1cbe"
            },
            {
                "title": "Analyzing alibaba\u2019s co-located datacenter workloads",
                "abstract": "Warehouse-scale cloud datacenters co-locate workloads with different and often complementary characteristics for improved resource utilization. To better understand the challenges in managing such intricate, heterogeneous workloads while providing quality-assured resource orchestration and user experience, we analyze Alibaba\u2019s co-located workload trace, the first publicly available dataset with precise information about the category of each job. Two types of workload\u2014long-running, user-facing, containerized production jobs, and transient, highly dynamic, non-containerized, and non-production batch jobs\u2014 are running on a shared cluster of 1313 machines. Through workload characterization, we find evidences that imply that one workload scheduler makes seemingly independent scheduling decisions regardless of the co-existence of the other. This upsurges an imminent need for a more integrated, global coordinating system that transparently connect multiple resource schedulers together and cohesively coordinates the multiple heterogeneous workloads for greater efficiency. Our multifaceted analysis reveals insights that we believe are useful for system designers and IT practitioners working on cluster management systems.",
                "paper_link": "https://www.semanticscholar.org/paper/3c7c1b0f0fba3148fe95292a6257e06e3f215dba"
            },
            {
                "title": "Asynchronous federated learning for sensor data with concept drift",
                "abstract": "Federated learning (FL) involves multiple distributed devices jointly training a shared model without any of the participants having to reveal their local data to a centralized server. Most of previous FL approaches assume that data on devices are fixed and stationary during the training process. However, this assumption is unrealistic because these devices usually have varying sampling rates and different system configurations. In addition, the underlying distribution of the device data can change dynamically over time, which is known as concept drift. Concept drift makes the learning process complicated because of the inconsistency between existing and upcoming data. Traditional concept drift handling techniques such as chunk based and ensemble learning-based methods are not suitable in the federated learning frameworks due to the heterogeneity of local devices. We propose a novel approach, FedConD, to detect and deal with the concept drift on local devices and minimize the effect on the performance of models in asynchronous FL. The drift detection strategy is based on an adaptive mechanism which uses the historical performance of the local models. The drift adaptation is realized by adjusting the regularization parameter of objective function on each local device. Additionally, we design a communication strategy on the server side to select local updates in a prudent fashion and speed up model convergence. Experimental evaluations on three evolving data streams and two image datasets show that FedConD detects and handles concept drift, and also reduces the overall communication cost compared to other baseline methods.",
                "paper_link": "https://www.semanticscholar.org/paper/09d1aeaeece8c6fe551eab7a330fd706da3236dd"
            },
            {
                "title": "Taming the Cloud Object Storage with MOS",
                "abstract": "Cloud object stores today are deployed using a single set of configuration parameters for all different types of applications. This homogeneous setup results in all applications experiencing the same service level (e.g., data transfer throughput, etc.). However, the vast variety of applications expose extremely different latency and throughput requirements. To this end, we propose MOS, a <u>M</u>icro <u>O</u>bject <u>S</u>torage architecture with independently configured microstores each tuned dynamically for a particular type of workload. We then expose these microstores to the tenant who can then choose to place their data in the appropriate microstore according the latency and throughput requirements of their workloads. Our evaluation shows that compared with default setup, MOS can improve the performance up to 200% for small objects and 28% for large objects while providing opportunity of tradeoff between two.",
                "paper_link": "https://www.semanticscholar.org/paper/9ea730e124efc3543b737bb3f72dedd609f5e5d5"
            },
            {
                "title": "Bolt: Towards a Scalable Docker Registry",
                "abstract": "Docker container images are typically stored in a centralized registry to allow easy sharing of images. However, with the growing popularity of containerized software, the number of images that a registry needs to store and the rate of requests it needs to serve are increasing rapidly. Current registry design requires hosting registry services across multiple loosely connected servers with different roles such as load balancers, proxies, registry servers, and object storage servers. Due to the various individual components, registries are hard to scale and benefits from optimizations such as caching are limited. In this paper we propose, implement, and evaluate BOLT-a new hyperconverged design for container registries. In BOLT, all registry servers are part of a tightly connected cluster and play the same consolidated role: each registry server caches images in its memory, stores images in its local storage, and provides computational resources to process client requests. The design employs a custom consistent hashing function to take advantage of the layered structure and addressing of images and to load balance requests across different servers. Our evaluation using real production workloads shows that BOLT outperforms the conventional registry design significantly and improves latency by an order of magnitude and throughput by up to 5x. Compared to state-of-the-art, BOLT can utilize cache space more efficiently and serve up to 35% more requests from its cache. Furthermore, BOLT scales linearly and recovers from failure recovery without significant performance degradation.",
                "paper_link": "https://www.semanticscholar.org/paper/5e0812dbe1a504a28e79e264a51cfbe4ad693c8c"
            },
            {
                "title": "BESPOKV: Application tailored scale-out key-value stores",
                "abstract": "Enterprise KV stores are not well suited for HPC applications, and entail customization and cumbersome end-to-end KV design to extract the HPC application needs. To this end, in this paper we present BESPOKV, an adaptive, extensible, and scale-out KV store framework. BESPOKV decouples the KV store design into the control plane for distributed management and the data plane for local data store. BESPOKV takes as input a single-server KV store, called a datalet, and transparently enables a scalable and fault-tolerant distributed KV store service. The resulting distributed stores are also adaptive to consistency or topology requirement changes and can be easily extended for new types of services. Experiments show that BESPOKV-enabled distributed KV stores scale horizontally to a large number of nodes, and performs comparably and sometimes better than the state-of-the-art systems.",
                "paper_link": "https://www.semanticscholar.org/paper/46e57b152c379c3c84eb4dfad74098521ba879ab"
            },
            {
                "title": "Pricing Games for Hybrid Object Stores in the Cloud: Provider vs. Tenant",
                "abstract": "Cloud object stores are increasingly becoming the de facto storage choice for big data analytics platforms, mainly because they simplify the management of large blocks of data at scale. To ensure cost-effectiveness of the storage service, the object stores use hard disk drives (HDDs). However, the lower performance of HDDs affect tenants who have strict performance requirements for their big data applications. The use of faster storage devices such as solid state drives (SSDs) is thus desirable by the tenants, but incurs significant maintenance costs to the provider. We design a tiered object store for the cloud, which comprises both fast and slow storage devices. The resulting hybrid store exposes the tiering to tenants with a dynamic pricing model that is based on the tenants' usage and the provider's desire to maximize profits. The tenants leverage knowledge of their workloads and current pricing information to select a data placement strategy that would meet the application requirements at the lowest cost. Our approach allows both a service provider and its tenants to engage in a pricing game, which our results show yields a win-win situation",
                "paper_link": "https://www.semanticscholar.org/paper/ecc8f0c37c90861a02024b071af5c8577ffefd5c"
            }
        ]
    },
    {
        "Professor": "Sandhya Dwarkadas",
        "Papers": [
            {
                "title": "Treadmarks: Shared memory computing on networks of workstations",
                "abstract": "Shared memory facilitates the transition from sequential to parallel processing. Since most data structures can be retained, simply adding synchronization achieves correct, efficient programs for many applications. We discuss our experience with parallel computing on networks of workstations using the TreadMarks distributed shared memory system. DSM allows processes to assume a globally shared virtual memory even though they execute on nodes that do not physically share memory. We illustrate a DSM system consisting of N networked workstations, each with its own memory. The DSM software provides the abstraction of a globally shared memory, in which each processor can access any data item without the programmer having to worry about where the data is or how to obtain its value.",
                "paper_link": "https://www.semanticscholar.org/paper/8ffb63bae50a62172e9c746d712169b1c59dd1ab"
            },
            {
                "title": "Parallel metropolis coupled Markov chain Monte Carlo for Bayesian phylogenetic inference",
                "abstract": "MOTIVATION\nBayesian estimation of phylogeny is based on the posterior probability distribution of trees. Currently, the only numerical method that can effectively approximate posterior probabilities of trees is Markov chain Monte Carlo (MCMC). Standard implementations of MCMC can be prone to entrapment in local optima. Metropolis coupled MCMC [(MC)(3)], a variant of MCMC, allows multiple peaks in the landscape of trees to be more readily explored, but at the cost of increased execution time.\n\n\nRESULTS\nThis paper presents a parallel algorithm for (MC)(3). The proposed parallel algorithm retains the ability to explore multiple peaks in the posterior distribution of trees while maintaining a fast execution time. The algorithm has been implemented using two popular parallel programming models: message passing and shared memory. Performance results indicate nearly linear speed improvement in both programming models for small and large data sets.",
                "paper_link": "https://www.semanticscholar.org/paper/d5d887304cf61ffb207aaaf84320fe1bafd0ef53"
            },
            {
                "title": "Treadmarks: Distributed shared memory on standard workstations and operating systems",
                "abstract": "TreadMarks is a distributed shared memory (DSM) system for standard Unix systems such as SunOS and Ultrix. This paper presents a performance evaluation of TreadMarks running on Ultrix using DECstation-5000/240's that are connected by a 100-Mbps switch-based ATM LAN and a 10-Mbps Ethernet. Our objective is to determine the efficiency of a user-level DSM implementation on commercially available workstations and operating systems. \n \nWe achieved good speedups on the 8-processor ATM network for Jacobi (7.4), TSP (7.2), Quicksort (6.3), and ILINK (5.7). For a slightly modified version of Water from the SPLASH benchmark suite, we achieved only moderate speedups (4.0) due to the high communication and synchronization rate. Speedups decline on the 10-Mbps Ethernet (5.5 for Jacobi, 6.5 for TSP, 4.2 for Quicksort, 5.1 for ILINK, and 2.1 for Water), reflecting the bandwidth limitations of the Ethernet. These results support the contention that, with suitable networking technology, DSM is a viable technique for parallel computation on clusters of workstations. \n \nTo achieve these speedups, TreadMarks goes to great lengths to reduce the amount of communication performed to maintain memory consistency. It uses a lazy implementation of release consistency, and it allows multiple concurrent writers to modify a page, reducing the impact of false sharing. Great care was taken to minimize communication overhead. In particular, on the ATM network, we used a standard low-level protocol, AAL3/4, bypassing the TCP/IP protocol stack. Unix communication overhead, however, remains the main obstacle in the way of better performance for programs like Water. Compared to the Unix communication overhead, memory management cost (both kernel and user level) is small and wire time is negligible. \n \nThis research was supported in part by the National Science Foundation under Grants CCR-9116343, CCR-9211004, CDA-9222911, and CDA-9310073, by the Texas Advanced Technology Program under Grant 003604014, and by a NASA Graduate Fellowship.",
                "paper_link": "https://www.semanticscholar.org/paper/ddee584755dd72a60b515c449fb1841f198b43b1"
            },
            {
                "title": "Peer-to-peer information retrieval using self-organizing semantic overlay networks",
                "abstract": "Content-based full-text search is a challenging problem in Peer-to-Peer (P2P) systems. Traditional approaches have either been centralized or use flooding to ensure accuracy of the results returned.In this paper, we present pSearch, a decentralized non-flooding P2P information retrieval system. pSearch distributes document indices through the P2P network based on document semantics generated by Latent Semantic Indexing (LSI). The search cost (in terms of different nodes searched and data transmitted) for a given query is thereby reduced, since the indices of semantically related documents are likely to be co located in the network.We also describe techniques that help distribute the indices more evenly across the nodes, and further reduce the number of nodes accessed using appropriate index distribution as well as using index samples and recently processed queries to guide the search.Experiments show that pSearch can achieve performance comparable to centralized information retrieval systems by searching only a small number of nodes. For a system with 128,000 nodes and 528,543 documents (from news, magazines, etc.), pSearch searches only 19 nodes and transmits only 95.5KB data during the search, whereas the top 15 documents returned by pSearch and LSI have a 91.7% intersection.",
                "paper_link": "https://www.semanticscholar.org/paper/868636b303b783b441fa39b4e08ef9b424b5ab6a"
            },
            {
                "title": "Energy-efficient processor design using multiple clock domains with dynamic voltage and frequency scaling",
                "abstract": "As clock frequency increases and feature size decreases, clock distribution and wire delays present a growing challenge to the designers of singly-clocked, globally synchronous systems. We describe an alternative approach, which we call a multiple clock domain (MCD) processor, in which the chip is divided into several clock domains, within which independent voltage and frequency scaling can be performed. Boundaries between domains are chosen to exploit existing queues, thereby minimizing inter-domain synchronization costs. We propose four clock domains, corresponding to the front end , integer units, floating point units, and load-store units. We evaluate this design using a simulation infrastructure based on SimpleScalar and Wattch. In an attempt to quantify potential energy savings independent of any particular on-line control strategy, we use off-line analysis of traces from a single-speed run of each of our benchmark applications to identify profitable reconfiguration points for a subsequent dynamic scaling run. Using applications from the MediaBench, Olden, and SPEC2000 benchmark suites, we obtain an average energy-delay product improvement of 20% with MCD compared to a modest 3% savings from voltage scaling a single clock and voltage system.",
                "paper_link": "https://www.semanticscholar.org/paper/4bac34db99b0cb23a6da11c43486165e6a9f17d4"
            },
            {
                "title": "Memory hierarchy reconfiguration for energy and performance in general-purpose processor architectures",
                "abstract": "Conventional microarchitectures choose a single memory hierarchy design point targeted at the average application. In this paper, we propose a cache and TLB layout and design that leverages repeater insertion to provide dynamic low-cost configurability trading off size and speed on a per application phase basis. A novel configuration management algorithm dynamically detects phase changes and reacts to an application's hit and miss intolerance in order to improve memory hierarchy performance while taking energy consumption into consideration. When applied to a two-level cache and TLB hierarchy at 0.1 /spl mu/m technology, the result is an average 15% reduction in cycles per instruction (CPI), corresponding to an average 27% reduction in memory-CPI, across a broad class of applications compared to the best conventional two-level hierarchy of comparable size. Projecting to sub-.1 /spl mu/m technology design considerations that call for a three-level conventional cache hierarchy for performance reasons, we demonstrate that a configurable L2/L3 cache hierarchy coupled with a conventional LI results in an average 43% reduction in memory hierarchy energy in addition to improved performance.",
                "paper_link": "https://www.semanticscholar.org/paper/f77a12ca006add3b760c796440b309c6f3388ad7"
            },
            {
                "title": "Towards practical page coloring-based multicore cache management",
                "abstract": "Modern multi-core processors present new resource management challenges due to the subtle interactions of simultaneously executing processes sharing on-chip resources (particularly the L2 cache). Recent research demonstrates that the operating system may use the page coloring mechanism to control cache partitioning, and consequently to achieve fair and efficient cache utilization. However, page coloring places additional constraints on memory space allocation, which may conflict with application memory needs. Further, adaptive adjustments of cache partitioning policies in a multi-programmed execution environment may incur substantial overhead for page recoloring (or copying). This paper proposes a hot-page coloring approach enforcing coloring on only a small set of frequently accessed (or hot) pages for each process. The cost of identifying hot pages online is reduced by leveraging the knowledge of spatial locality during a page table scan of access bits. Our results demonstrate that hot page identification and selective coloring can significantly alleviate the coloring-induced adverse effects in practice. However, we also reach the somewhat negative conclusion that without additional hardware support, adaptive page coloring is only beneficial when recoloring is performed infrequently (meaning long scheduling time quanta in multi-programmed executions).",
                "paper_link": "https://www.semanticscholar.org/paper/5889ce4de110f5a7f69a83d7f63984147033832d"
            },
            {
                "title": "Incremental and interactive sequence mining",
                "abstract": "The discovery of frequent sequences in temporal databases is an important data mining problem. Most current work assumes that the database is static, and a database update requires rediscovering all the patterns by scanning the entire old and new database. In this paper, we propose novel techniques for maintaining sequences in the presence of a) database updates, and b) user interaction (e.g. modifying mining parameters). This is a very challenging task, since such updates can invalidate existing sequences or introduce new ones. In both the above scenarios, we avoid re-executing the algorithm on the entire dataset, thereby reducing execution time. Experimental results confirm that our approach results in execution time improvements of up to several orders of magnitude in practice.",
                "paper_link": "https://www.semanticscholar.org/paper/e882591941950eca38e01dd05d9fe537e371381b"
            },
            {
                "title": "Characterizing and predicting program behavior and its variability",
                "abstract": "To reach the next level of performance and energy efficiency, optimizations are increasingly applied in a dynamic and adaptive manner. Current adaptive systems are typically reactive and optimize hardware or software in response to detecting a shift in program behavior. We argue that program behavior variability requires adaptive systems to be predictive rather than reactive. In order to be effective, systems need to adapt according to future rather than most recent past behavior. We explore the potential of incorporating prediction into adaptive systems. We study the time-varying behavior of programs using metrics derived from hardware counters on two different microarchitectures. Our evaluation shows that programs do indeed exhibit significant behavior variation even at a granularity of millions of instructions. In addition, while the actual behavior across metrics may be different, periodicity in the behavior is shared across metrics. We exploit these characteristics in the design of on-line statistical and table-based predictors. We introduce a new class of predictors, cross-metric predictors, that use one metric to predict another, thus making possible an efficient coupling of multiple predictors. We evaluate these predictors on the SPECcpu2000 benchmark suite and show that table-based predictors outperform statistical predictors by as much as 69% on benchmarks with high variability.",
                "paper_link": "https://www.semanticscholar.org/paper/80a8c32bb71bda6bbc8dd2c326bbc7810511eda5"
            },
            {
                "title": "Reducing the complexity of the register file in dynamic superscalar processors",
                "abstract": "Dynamic superscalar processors execute multiple instructions out-of-order by looking for independent operations within a large window. The number of physical registers within the processor has a direct impact on the size of this window as most in-flight instructions require a new physical register at dispatch. A large multi-ported register file helps improve the instruction-level parallelism (ILP), but may have a detrimental effect on clock speed, especially in future wire-limited technologies. In this paper, we propose a register file organization that reduces register file size and port requirements for a given amount of ILP. We use a two-level register file organization to reduce register file size requirements, and a banked organization to reduce port requirements. We demonstrate empirically that the resulting register file organizations have reduced latency and (in the case of the banked organization) energy requirements for similar instructions per cycle (IPC) performance and improved instructions per second (IPS) performance in comparison to a conventional monolithic register file. The choice of organization is dependent on design goals.",
                "paper_link": "https://www.semanticscholar.org/paper/a185016977d25e3078039c85124486b139a7f1ca"
            },
            {
                "title": "Hybrid Global-Local Indexing for Efficient Peer-to-Peer Information Retrieval.",
                "abstract": "Content-based full-text search still remains a particularly challenging problem in peer-to-peer (P2P) systems. Traditionally, there have been two index partitioning structures--partitioning based on the document space or partitioning based on keywords. The former requires search of every node in the system to answer a query whereas the latter transmits a large amount of data when processing multi-term queries. In this paper, we propose eSearch--a P2P keyword search system based on a novel hybrid indexing structure. In eSearch, each node is responsible for certain terms. Given a document, eSearch uses a modern information retrieval algorithm to select a small number of top (important) terms in the document and publishes the complete term list for the document to nodes responsible for those top terms. This selective replication of term lists allows a multi-term query to proceed local to the nodes responsible for query terms. We also propose automatic query expansion to alleviate the degradation of quality of search results due to the selective replication, overlay source multicast to reduce the cost of disseminating term lists, and techniques to balance term list distribution across nodes. \n \neSearch is scalable and efficient, and obtains search results as good as state-of-the-art centralized systems. Despite the use of replication, eSearch actually consumes less bandwidth than systems based on keyword partitioning when publishing metadata for a document. During a retrieval operation, it searches only a small number of nodes and typically transmits a small amount of data (3.3KB) that is independent of the size of the corpus and grows slowly (logarithmically) with the number of nodes in the system. eSearch's efficiency comes at a modest storage cost, 6.8 times that of systems based on keyword partitioning. This cost can be further reduced by adopting index compression or pruning techniques.",
                "paper_link": "https://www.semanticscholar.org/paper/7de2684e483b6aa133686afa071855c537b9fa29"
            },
            {
                "title": "Cashmere-2L: Software coherent shared memory on a clustered remote-write network",
                "abstract": "Low-latency remote-write networks, such as DEC's Memory Channel, provide the possibility of transparent, inexpensive, large-scale shared-memory parallel computing on clusters of shared memory multiprocessors (SMPs). The challenge is to take advantage of hardware shared memory for sharing within an SMP, and to ensure that software overhead is incurred only when actively sharing data across SMPs in the cluster. In this paper, we describe a two-level software coherent shared memory system-Cashmere-2L-that meets this challenge. Cashmere-2L uses hardware to share memory within a node, while exploiting the Memory Channel's remote-write capabilities to implement moderately lazy release consistency with multiple concurrent writers, directories, home nodes, and page-size coherence blocks across nodes. Cashmere-2L employs a novel coherence protocol that allows a high level of asynchrony by eliminating global directory locks and the need for TLB shootdown. Remote interrupts are minimized by exploiting the remote-write capabilities of the Memory Channel network. Cashmere-2L currently runs on an 8-node, 32-processor DEC AlphaServer system. Speedups range from 8 to 31 on 32 processors for our benchmark suite, depending on the application's characteristics. We quantify the importance of our protocol optimizations by comparing performance to that of several alternative protocols that do not share memory in hardware within an SMP, and require more synchronization. In comparison to a one-level protocol that does not share memory in hardware within an SMP, Cashmere-2L improves performance by up to 46%.",
                "paper_link": "https://www.semanticscholar.org/paper/c38f22a14d362d91f288672fae8b5ce83dfcc3bc"
            },
            {
                "title": "Dynamic frequency and voltage control for a multiple clock domain microarchitecture",
                "abstract": "We describe the design, analysis, and performance of an on-line algorithm to dynamically control the frequency/voltage of a Multiple Clock Domain (MCD) microarchitecture. The MCD microarchitecture allows the frequency/voltage of microprocessor regions to be adjusted independently and dynamically, allowing energy savings when the frequency of some regions can be reduced without significantly impacting performance. Our algorithm achieves on average a 19.0% reduction in Energy Per Instruction (EPI), a 3.2% increase in Cycles Per Instruction (CPI), a 16.7% improvement in Energy-Delay Product, and a Power Savings to Performance Degradation ratio of 4.6. Traditional frequency/voltage scaling techniques which apply reductions globally to a fully synchronous processor achieve a Power Savings to Performance Degradation ratio of only 2-3. Our Energy-Delay Product improvement is 85.5% of what has been achieved using an off-line algorithm. These results were achieved using a broad range of applications from the MediaBench, Olden, and Spec2000 benchmark suites using an algorithm we show to require minimal hardware resources.",
                "paper_link": "https://www.semanticscholar.org/paper/d02f65ddb86f6ca76e28cceae66e2ad55a2eae93"
            },
            {
                "title": "Multiple clock domain microprocessor",
                "abstract": "A Multiple Clock Domain (MCD) processor addresses the challenges of clock distribution and power dissipation by dividing a chip into several (coarse-grained) clock domains, allowing frequency and voltage to be reduced in domains that are not currently on the application's critical path. Given a reconfiguration mechanism capable of choosing appropriate times and values for voltage/frequency scaling, an MCD processor has the potential to achieve significant energy savings with low performance degradation. Early work on MCD processors evaluated the potential for energy savings by manually inserting reconfiguration instructions into applications, or by employing an oracle driven by offline analysis of (identical) prior program runs. Subsequent work developed a hardware-based online mechanism that averages 75-85% of the energy-delay improvement achieved via offline analysis. We consider the automatic insertion of reconfiguration instructions into applications, using profile-driven binary rewriting. Profile-based reconfiguration introduces the need for \"training runs\" prior to production use of a given application, but avoids the hardware complexity of online reconfiguration. It also has the potential to yield significantly greater energy savings. Experimental results (training on small data sets and then running on larger, alternative data sets) indicate that the profile-driven approach is more stable than hardware-based reconfiguration, and yields virtually all of the energy-delay improvement achieved via offline analysis.",
                "paper_link": "https://www.semanticscholar.org/paper/bf47114c50b40c643c79012de2b349ee9ea5edc0"
            },
            {
                "title": "Dynamically tuning processor resources with adaptive processing",
                "abstract": "By using adaptive processing to dynamically tune major microprocessor resources, developers can achieve greater energy efficiency with reasonable hardware and software overhead while avoiding undue performance loss. Adaptive processors require few additional transistors. Further, because adaptation occurs only in response to infrequent trigger events, the decision logic can be placed into a low-leakage state until such events occur.",
                "paper_link": "https://www.semanticscholar.org/paper/ecb5ad1d5a78d82ac9881ec9c5b91d294db0d6bd"
            },
            {
                "title": "Flexible decoupled transactional memory support",
                "abstract": "A high-concurrency transactional memory (TM) implementation needs to track concurrent accesses, buffer speculative updates, and manage conflicts. We present a system, FlexTM (FLEXible Transactional Memory), that coordinates four decoupled hardware mechanisms: read and write signatures, which summarize per-thread access sets; per-thread conflict summary tables (CSTs), which identify the threads with which conflicts have occurred; Programmable Data Isolation, which maintains speculative updates in the local cache and employs a thread-private buffer (in virtual memory) in the rare event of overflow; and Alert-On-Update, which selectively notifies threads about coherence events. All mechanisms are software- accessible, to enable visualization and to support transactions of arbitrary length. FlexTM allows software to determine when to manage conflicts (either eagerly or lazily), and to employ a variety of conflict management and commit protocols. We describe an STM-inspired protocol that uses CSTs to manage conflicts in a distributed manner (no global arbitration) and allows parallel commits. In experiments with a prototype on Simics/GEMS, FlexTM exhibits ~5times speedup over high-quality software TM, with no loss in policy flexibility. Its distributed commit protocol is also more efficient than a central hardware manager. Our results highlight the importance of flexibility in determining when to manage conflicts: lazy maximizes concurrency and helps to ensure forward progress while eager provides better overall utilization in a multi-programmed system.",
                "paper_link": "https://www.semanticscholar.org/paper/9d8009cba9c199270a89ed15d9bacd187af319fd"
            },
            {
                "title": "An integrated compile-time/run-time software distributed shared memory system",
                "abstract": "On a distributed memory machine, hand-coded message passing leads to the most efficient execution, but it is difficult to use. Parallelizing compilers can approach the performance of hand-coded message passing by translating data-parallel programs into message passing programs, but efficient execution is limited to those programs for which precise analysis can be carried out. Shared memory is easier to program than message passing and its domain is not constrained by the limitations of parallelizing compilers, but it lags in performance. Our goal is to close that performance gap while retaining the benefits of shared memory. In other words, our goal is (1) to make shared memory as efficient as message passing, whether hand-coded or compiler-generated, (2) to retain its ease of programming, and (3) to retain the broader class of applications it supports.To this end we have designed and implemented an integrated compile-time and run-time software DSM system. The programming model remains identical to the original pure run-time DSM system. No user intervention is required to obtain the benefits of our system. The compiler computes data access patterns for the individual processors. It then performs a source-to-source transformation, inserting in the program calls to inform the run-time system of the computed data access patterns. The run-time system uses this information to aggregate communication, to aggregate data and synchronization into a single message, to eliminate consistency overhead, and to replace global synchronization with point-to-point synchronization wherever possible.We extended the Parascope programming environment to perform the required analysis, and we augmented the TreadMarks run-time DSM library to take advantage of the analysis. We used six Fortran programs to assess the performance benefits: Jacobi, 3D-FFT, Integer Sort, Shallow, Gauss, and Modified Gramm-Schmidt, each with two different data set sizes. The experiments were run on an 8-node IBM SP/2 using user-space communication. Compiler optimization in conjunction with the augmented run-time system achieves substantial execution time improvements in comparison to the base TreadMarks, ranging from 4% to 59% on 8 processors. Relative to message passing implementations of the same applications, the compile-time run-time system is 0-29% slower than message passing, while the base run-time system is 5-212% slower. For the five programs that XHPF could parallelize (all except IS), the execution times achieved by the compiler optimized shared memory programs are within 9% of XHPF.",
                "paper_link": "https://www.semanticscholar.org/paper/3e4754927f50248e1f7df105dc29558b7bd29571"
            },
            {
                "title": "Integrating adaptive on-chip storage structures for reduced dynamic power",
                "abstract": "Energy efficiency in microarchitectures has become a necessity. Significant dynamic energy savings can be realized for adaptive storage structures such as caches, issue queues, and register files by disabling unnecessary storage resources. Prior studies have analyzed individual structures and their control. A common theme to these studies is exploration of the configuration space and use of system IPC as feedback to guide reconfiguration. However when multiple structures adapt in concert, the number of possible configurations increases dramatically, and assigning causal effects to IPC change becomes problematic. To overcome this issue, we introduce designs that are reconfigured solely on local behavior. We introduce a novel cache design that permits direct calculation of efficient configurations. For buffer and queue structures, limited histogramming permits precise resizing control. When applying these techniques we show energy savings of up to 70% on the individual structures, and savings averaging 30% overall for the portion of energy attributed to these structures with an average of 2.1% performance degradation.",
                "paper_link": "https://www.semanticscholar.org/paper/84254836e8a7aed004bedda8371eb0c33e86daeb"
            },
            {
                "title": "An integrated hardware-software approach to flexible transactional memory",
                "abstract": "There has been considerable recent interest in both hardware andsoftware transactional memory (TM). We present an intermediateapproach, in which hardware serves to accelerate a TM implementation controlled fundamentally by software. Specifically, we describe an alert on update mechanism (AOU) that allows a thread to receive fast, asynchronous notification when previously-identified lines are written by other threads, and a programmable data isolation mechanism (PDI) that allows a thread to hide its speculative writes from other threads, ignoring conflicts, until software decides to make them visible. These mechanisms reduce bookkeeping, validation, and copying overheads without constraining software policy on a host of design decisions.\n We have used AOU and PDI to implement a hardwareacceleratedsoftware transactional memory system we call RTM. We have also used AOU alone to create a simpler \"RTM-Lite\". Across a range of microbenchmarks, RTM outperforms RSTM, a publicly available software transactional memory system, by as much as 8.7x (geometric mean of 3.5x) in single-thread mode. At 16 threads, it outperforms RSTM by as much as 5x, with an average speedup of 2x. Performance degrades gracefully when transactions overflow hardware structures. RTM-Lite is slightly faster than RTM for transactions that modify only small objects; full RTM is significantly faster when objects are large. In a strongargument for policy flexibility, we find that the choice between eager (first-access) and lazy (commit-time) conflict detection can lead to significant performance differences in both directions, depending on application characteristics.",
                "paper_link": "https://www.semanticscholar.org/paper/a0013802a852a886548df290dd5de1534f17c3f0"
            },
            {
                "title": "Message passing versus distributed shared memory on networks of workstations",
                "abstract": "The message passing programs are executed with the Parallel Virtual Machine (PVM) library and the shared memory programs are executed using TreadMarks. The programs are Water and Barnes-Hut from the SPLASH benchmark suite; 3-D FFT, Integer Sort (IS) and Embarrassingly Parallel (EP) from the NAS benchmarks; ILINK, a widely used genetic linkage analysis program; and Successive Over-Relaxation (SOR), Traveling Salesman (TSP), and Quicksort (QSORT). Two different input data sets were used for Water (Water-288 and Water-1728), IS (IS-Small and IS-Large), and SOR (SOR-Zero and SOR-NonZero). Our execution environment is a set of eight HP735 workstations connected by a 100Mbits per second FDDI network. For Water-1728, EP, ILINK, SOR-Zero, and SOR-NonZero, the performance of TreadMarks is within 10%of PVM. For IS-Small, Water-288, Barnes-Hut, 3-D FFT, TSP, and QSORT, differences are on the order of 10%to 30%. Finally, for IS-Large, PVM performs two times better than TreadMarks. More messages and more data are sent in TreadMarks, explaining the performance differences. This extra communication is caused by 1) the separation of synchronization and data transfer, 2) extra messages to request updates for data by the invalidate protocol used in TreadMarks, 3) false sharing, and 4) diff accumulation for migratory data in TreadMarks.",
                "paper_link": "https://www.semanticscholar.org/paper/d79399d927bd38343ced58a0f65053ce1c8d245a"
            }
        ]
    },
    {
        "Professor": "Matthew B. Dwyer",
        "Papers": []
    },
    {
        "Professor": "Sebastian Elbaum",
        "Papers": [
            {
                "title": "Supporting controlled experimentation with testing techniques: An infrastructure and its potential impact",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/94782b2954ae2451a5963cdc39a954b5bfd077aa"
            },
            {
                "title": "Test case prioritization: A family of empirical studies",
                "abstract": "To reduce the cost of regression testing, software testers may prioritize their test cases so that those which are more important, by some measure, are run earlier in the regression testing process. One potential goal of such prioritization is to increase a test suite's rate of fault detection. Previous work reported results of studies that showed that prioritization techniques can significantly improve rate of fault detection. Those studies, however, raised several additional questions: 1) Can prioritization techniques be effective when targeted at specific modified versions; 2) what trade-offs exist between fine granularity and coarse granularity prioritization techniques; 3) can the incorporation of measures of fault proneness into prioritization techniques improve their effectiveness? To address these questions, we have performed several new studies in which we empirically compared prioritization techniques using both controlled experiments and case studies.",
                "paper_link": "https://www.semanticscholar.org/paper/9d603bd5f6862687e0de7fe07dd8ae086758b25f"
            },
            {
                "title": "Prioritizing test cases for regression testing",
                "abstract": "Test case prioritization techniques schedule test cases in an order that increases their effectiveness in meeting some performance goal. One performance goal, rate of fault detection, is a measure of how quickly faults are detected within the testing process; an improved rate of fault detection can provide faster feedback on the system under test, and let software engineers begin locating and correcting faults earlier than might otherwise be possible. In previous work, we reported the results of studies that showed that prioritization techniques can significantly improve rate of fault detection. Those studies, however, raised several additional questions: (1) can prioritization techniques be effective when aimed at specific modified versions; (2) what tradeoffs exist between fine granularity and coarse granularity prioritization techniques; (3) can the incorporation of measures of fault proneness into prioritization techniques improve their effectiveness? This paper reports the results of new experiments addressing these questions.",
                "paper_link": "https://www.semanticscholar.org/paper/86294296ca6efafb2f8db96687e77a4b3cbed7c7"
            },
            {
                "title": "Incorporating varying test costs and fault severities into test case prioritization",
                "abstract": "Test case prioritization techniques schedule test cases for regression testing in an order that increases their ability to meet some performance goal. One performance goal, rate of fault detection, measures how quickly faults are detected within the testing process. In previous work (S. Elbaum et al., 2000; G. Rothermel et al., 1999), we provided a metric, APFD, for measuring rate of fault detection, and techniques for prioritizing test cases to improve APFD, and reported the results of experiments using those techniques. This metric and these techniques, however, applied only in cases in which test costs and fault severity are uniform. We present a new metric for assessing the rate of fault detection of prioritized test cases that incorporates varying test case and fault costs. We present the results of a case study illustrating the application of the metric. This study raises several practical questions that might arise in applying test case prioritization; we discuss how practitioners could go about answering these questions.",
                "paper_link": "https://www.semanticscholar.org/paper/c482f553e8ff9daf5eb21fc7f12153ef3cc52d36"
            },
            {
                "title": "Techniques for improving regression testing in continuous integration development environments",
                "abstract": "In continuous integration development environments, software engineers frequently integrate new or changed code with the mainline codebase. This can reduce the amount of code rework that is needed as systems evolve and speed up development time. While continuous integration processes traditionally require that extensive testing be performed following the actual submission of code to the codebase, it is also important to ensure that enough testing is performed prior to code submission to avoid breaking builds and delaying the fast feedback that makes continuous integration desirable. In this work, we present algorithms that make continuous integration processes more cost-effective. In an initial pre-submit phase of testing, developers specify modules to be tested, and we use regression test selection techniques to select a subset of the test suites for those modules that render that phase more cost-effective. In a subsequent post-submit phase of testing, where dependent modules as well as changed modules are tested, we use test case prioritization techniques to ensure that failures are reported more quickly. In both cases, the techniques we utilize are novel, involving algorithms that are relatively inexpensive and do not rely on code coverage information -- two requirements for conducting testing cost-effectively in this context. To evaluate our approach, we conducted an empirical study on a large data set from Google that we make publicly available. The results of our study show that our selection and prioritization techniques can each lead to cost-effectiveness improvements in the continuous integration process.",
                "paper_link": "https://www.semanticscholar.org/paper/02667ab3882c595bdcaef16f14eac8fbe3dd56ed"
            },
            {
                "title": "Differential symbolic execution",
                "abstract": "Detecting and characterizing the effects of software changes is a fundamental component of software maintenance. Version differencing information can be used to perform version merging, infer change characteristics, produce program documentation, and guide program re-validation. Existing techniques for characterizing code changes, however, are imprecise leading to unnecessary maintenance efforts.\n In this paper, we introduce a novel extension and application of symbolic execution techniques that computes a precise behavioral characterization of a program change. This technique, which we call differential symbolic execution (DSE), exploits the fact that program versions are largely similar to reduce cost and improve the quality of analysis results. We define the foundational concepts of DSE, describe cost-effective tool support for DSE, and illustrate its potential benefit through an exploratory study that considers version histories of two Java code bases.",
                "paper_link": "https://www.semanticscholar.org/paper/38a0e40c5b04ebe9154390ccde800dc6cebbd6cb"
            },
            {
                "title": "Selecting a cost-effective test case prioritization technique",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/0214307ac5be9f37a1d09dbda93ceb25acee13c7"
            },
            {
                "title": "Leveraging user-session data to support web application testing",
                "abstract": "Web applications are vital components of the global information infrastructure, and it is important to ensure their dependability. Many techniques and tools for validating Web applications have been created, but few of these have addressed the need to test Web application functionality and none have attempted to leverage data gathered in the operation of Web applications to assist with testing. In this paper, we present several techniques for using user session data gathered as users operate Web applications to help test those applications from a functional standpoint. We report results of an experiment comparing these new techniques to existing white-box techniques for creating test cases for Web applications, assessing both the adequacy of the generated test cases and their ability to detect faults on a point-of-sale Web application. Our results show that user session data can be used to produce test suites more effective overall than those produced by the white-box techniques considered; however, the faults detected by the two classes of techniques differ, suggesting that the techniques are complementary.",
                "paper_link": "https://www.semanticscholar.org/paper/8b37d06ef31121f9ce40ef361ef94593b59cfc62"
            },
            {
                "title": "Improving web application testing with user session data",
                "abstract": "Web applications have become critical components of the global information infrastructure, and it is important that they be validated to ensure their reliability. Therefore, many techniques and tools for validating web applications have been created. Only a few of these techniques, however, have addressed problems of testing the functionality of web applications, and those that do have not fully considered the unique attributes of web applications. In this paper we explore the notion that user session data gathered as users operate web applications can be successfully employed in the testing of those applications, particularly as those applications evolve and experience different usage profiles. We report results of an experiment comparing new and existing test generation techniques for web applications, assessing both the adequacy of the generated tests and their ability to detect faults on a point-of-sale web application. Our results show that user session data can produce test suites as effective overall as those produced by existing white-box techniques, but at less expense. Moreover the classes of faults detected differ somewhat across approaches, suggesting that the techniques may be complimentary.",
                "paper_link": "https://www.semanticscholar.org/paper/417ebe6fa8356a9ff312cf927ee7df558e17afc0"
            },
            {
                "title": "How developers search for code: a case study",
                "abstract": "With the advent of large code repositories and sophisticated search capabilities, code search is increasingly becoming a key software development activity. In this work we shed some light into how developers search for code through a case study performed at Google, using a combination of survey and log-analysis methodologies. Our study provides insights into what developers are doing and trying to learn when per- forming a search, search scope, query properties, and what a search session under different contexts usually entails. Our results indicate that programmers search for code very frequently, conducting an average of five search sessions with 12 total queries each workday. The search queries are often targeted at a particular code location and programmers are typically looking for code with which they are somewhat familiar. Further, programmers are generally seeking answers to questions about how to use an API, what code does, why something is failing, or where code is located.",
                "paper_link": "https://www.semanticscholar.org/paper/6aaccef3a11988699ba29988cff3bc86da8462f8"
            },
            {
                "title": "Programmers' build errors: a case study (at google)",
                "abstract": "Building is an integral part of the software development process. However, little is known about the compiler errors that occur in this process. In this paper, we present an empirical study of 26.6 million builds produced during a period of nine months by thousands of developers. We describe the workflow through which those builds are generated, and we analyze failure frequency, compiler error types, and resolution efforts to fix those compiler errors. The results provide insights on how a large organization build process works, and pinpoints errors for which further developer support would be most effective.",
                "paper_link": "https://www.semanticscholar.org/paper/ff2aea3828175dbeba47eacf456f966845be3768"
            },
            {
                "title": "Quality assurance under the open source development model",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/a7d069b1653bdc03af9abdb652ef3ffa86019449"
            },
            {
                "title": "Code churn: A measure for estimating the impact of code change",
                "abstract": "This study presents a methodology that will produce a viable fault surrogate. The focus of the effort is on the precise measurement of software development process and product outcomes. Tools and processes for the static measurement of the source code have been installed and made operational in a large embedded software system. Source code measurements have been gathered unobtrusively for each build in the software evolution process. The measurements are synthesized to obtain the fault surrogate. The complexity of sequential builds is compared and a new measure, code churn, is calculated. This paper demonstrates the effectiveness of code complexity churn by validating it against the testing problem reports.",
                "paper_link": "https://www.semanticscholar.org/paper/c31ca6c0230b4d902e1fa8741fa82c171a149955"
            },
            {
                "title": "Autonomous aerial water sampling",
                "abstract": "Obtaining spatially separated, high\u2010frequency water samples from rivers and lakes is critical to enhance our understanding and effective management of freshwater resources. In this work, we present an aerial water sampler and assess the system through field experiments. The aerial water sampler has the potential to vastly increase the speed and range at which scientists obtain water samples while reducing cost and effort. The water sampling system includes 1) a mechanism to capture three 20 ml samples per mission, 2) sensors and algorithms for altitude approximation over water, and 3) software components that integrate and analyze sensor data, control the vehicle, drive the sampling mechanism, and manage risk. We validate the system in the lab, characterize key sensors, develop a framework for quantifying risk, and present results of outdoor experiments that characterize the performance of the system under windy conditions. In addition, we compare water samples from local lakes obtained by our system to samples obtained by traditional sampling techniques. We find that even winds of 5.8 m/s have little impact on the water sampling system and that the samples collected are consistent with traditional techniques for most properties. These experiments show that despite the challenges associated with flying precisely over water, it is possible to quickly obtain scientifically useful water samples with an unmanned aerial vehicle.",
                "paper_link": "https://www.semanticscholar.org/paper/dbe807cfe2c84475aefb47299d91e230c5e520ac"
            },
            {
                "title": "On crop height estimation with UAVs",
                "abstract": "Remote sensing by Unmanned Aerial Vehicles (UAVs) is changing the way agriculture operates by increasing the spatial-temporal resolution of data collection. Micro-UAVs have the potential to further improve and enrich the data collected by operating close to the crops, enabling the collection of higher spatio-temporal resolution data. In this paper, we present a UAV-mounted measurement system that utilizes a laser scanner to compute crop heights, a critical indicator of crop health. The system filters, transforms, and analyzes the cluttered range data in real-time to determine the distance to the ground and to the top of the crops. We assess the system in an indoor testbed and in a corn field. Our findings indicate that despite the dense canopy and highly variable sensor readings, we can precisely fly over crops and measure its height to within 5cm of measurements gathered using current measurement technology.",
                "paper_link": "https://www.semanticscholar.org/paper/88aaf745a02fc5ccc762ba91b86b2c3a3894de05"
            },
            {
                "title": "Dynamic software system intrusion detection",
                "abstract": "Intrusion detection system (IDS) is a combination of software application and hardware devices that monitors the network and filters activities for malicious or unauthorized access attempts. IDS are deployed for generating large volumes of stream data, which can be challenging to identify relevant features and reduce false positives. Feature selection is a candidate solution for preserving only relevant features and filtering out non-relevant features in IDS. The feature selection is performed using multi-objective optimization based meta-heuristic searching algorithms (MOO-MHS), minimizing two objective functions: error rate and memory usage. Traditional FS has limited suitability due to the stream nature of IDS data and the occurrence of concept drift. To solve this challenge, the MOO-MHS is promising to be updated to support dynamic feature selection (DFS) for serving IDS. Therefore, this paper proposes a novel framework for DFS in IDS using MOO-MHS. The proposed framework is an Enhanced Dynamic Filter-Based Feature Selection (E-DFBFS) that enables periodic call of non-dominated sorting genetic algorithm 2 (NSGA-II) that provides a novel solution selection algorithm using two selection types, namely, mean and median. The performance of the proposed E-DFBFS framework is compared with existing state-of-the-art benchmarking algorithms considering both synthetic and real-world data, providing superiority over DFBFS in terms of accuracy and F-measure and most classification metrics.\u2019",
                "paper_link": "https://www.semanticscholar.org/paper/03c87c869fcabdf82257baae78192cd350263f7e"
            },
            {
                "title": "Cost-cognizant test case prioritization",
                "abstract": "Test case prioritization techniques schedule test cases for regression testing in an order that increases their ability to meet some performance goal. One performance goal, rate of fault detection, measures how quickly faults are detected within the testing process. Previous work has provided a metric, AP F D, for measuring rate of fault detection, and techniques for prioritizing test cases in order to improve AP F D. This metric and these techniques, however, assume that all test case and fault costs are uniform. In practice, test case and fault costs can vary, and in such cases the previous AP F D metric and techniques designed to improve AP F D can be unsatisfactory. This paper presents a new metric for assessing the rate of fault detection of prioritized test cases, AP F DC, that incorporates varying test case and fault costs. The paper also describes adjustments to previous prioritization techniques that allow them, too, to be \\cognizant\" of these varying costs. These techniques enable practitioners to perform a new type of prioritization: cost-cognizant test case prioritization. Finally, the results of a formative case study are presented. This study was designed to investigate the cost-cognizant metric and techniques and how they compare to their non-cost-cognizant counterparts. The study\u2019s results provide insights regarding the use of cost-cognizant test case prioritization in a variety of real-world settings.",
                "paper_link": "https://www.semanticscholar.org/paper/6fcc95f5adb18e24fce5ba6688901eb5b1f05256"
            },
            {
                "title": "Predicting accurate and actionable static analysis warnings: an experimental approach",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/e65ded93a456ac5d51242c93e5f6fc6475272dc3"
            },
            {
                "title": "On test suite composition and cost-effective regression testing",
                "abstract": "Regression testing is an expensive testing process used to revalidate software as it evolves. Various methodologies for improving regression testing processes have been explored, but the cost-effectiveness of these methodologies has been shown to vary with characteristics of regression test suites. One such characteristic involves the way in which test inputs are composed into test cases within a test suite. This article reports the results of controlled experiments examining the effects of two factors in test suite composition---test suite granularity and test input grouping---on the costs and benefits of several regression-testing-related methodologies: retest-all, regression test selection, test suite reduction, and test case prioritization. These experiments consider the application of several specific techniques, from each of these methodologies, across ten releases each of two substantial software systems, using seven levels of test suite granularity and two types of test input grouping. The effects of granularity, technique, and grouping on the cost and fault-detection effectiveness of regression testing under the given methodologies are analyzed. This analysis shows that test suite granularity significantly affects several cost-benefit factors for the methodologies considered, while test input grouping has limited effects. Further, the results expose essential tradeoffs affecting the relationship between test suite design and regression testing cost-effectiveness, with several implications for practice.",
                "paper_link": "https://www.semanticscholar.org/paper/375c589a20efb65a1f415e9396a3b054f9e20107"
            },
            {
                "title": "Solving the search for source code",
                "abstract": "Programmers frequently search for source code to reuse using keyword searches. The search effectiveness in facilitating reuse, however, depends on the programmer's ability to specify a query that captures how the desired code may have been implemented. Further, the results often include many irrelevant matches that must be filtered manually. More semantic search approaches could address these limitations, yet existing approaches are either not flexible enough to find approximate matches or require the programmer to define complex specifications as queries. We propose a novel approach to semantic code search that addresses several of these limitations and is designed for queries that can be described using a concrete input/output example. In this approach, programmers write lightweight specifications as inputs and expected output examples. Unlike existing approaches to semantic search, we use an SMT solver to identify programs or program fragments in a repository, which have been automatically transformed into constraints using symbolic analysis, that match the programmer-provided specification. We instantiated and evaluated this approach in subsets of three languages, the Java String library, Yahoo! Pipes mashup language, and SQL select statements, exploring its generality, utility, and trade-offs. The results indicate that this approach is effective at finding relevant code, can be used on its own or to filter results from keyword searches to increase search precision, and is adaptable to find approximate matches and then guide modifications to match the user specifications when exact matches do not already exist. These gains in precision and flexibility come at the cost of performance, for which underlying factors and mitigation strategies are identified.",
                "paper_link": "https://www.semanticscholar.org/paper/38cb9546c19be6b94cf760d48dcd9c3c9ae1bed7"
            }
        ]
    },
    {
        "Professor": "David Evans",
        "Papers": [
            {
                "title": "Advances and Open Problems in Federated Learning",
                "abstract": "Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.",
                "paper_link": "https://www.semanticscholar.org/paper/07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b"
            },
            {
                "title": "Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks",
                "abstract": "Although deep neural networks (DNNs) have achieved great success in many tasks, they can often be fooled by \\emph{adversarial examples} that are generated by adding small but purposeful distortions to natural examples. Previous studies to defend against adversarial examples mostly focused on refining the DNN models, but have either shown limited success or required expensive computation. We propose a new strategy, \\emph{feature squeezing}, that can be used to harden DNN models by detecting adversarial examples. Feature squeezing reduces the search space available to an adversary by coalescing samples that correspond to many different feature vectors in the original space into a single sample. By comparing a DNN model's prediction on the original input with that on squeezed inputs, feature squeezing detects adversarial examples with high accuracy and few false positives. This paper explores two feature squeezing methods: reducing the color bit depth of each pixel and spatial smoothing. These simple strategies are inexpensive and complementary to other defenses, and can be combined in a joint detection framework to achieve high detection rates against state-of-the-art attacks.",
                "paper_link": "https://www.semanticscholar.org/paper/9fec45e1ff97ffb0e0cf9f039e39b46043430301"
            },
            {
                "title": "Localization for Mobile Sensor Networks",
                "abstract": "Many sensor network applications require location awareness, but it is often too expensive to include a GPS receiver in a sensor network node. Hence, localization schemes for sensor networks typically use a small number of seed nodes that know their location and protocols whereby other nodes estimate their location from the messages they receive. Several such localization techniques have been proposed, but none of them consider mobile nodes and seeds. Although mobility would appear to make localization more difficult, in this paper we introduce the sequential Monte Carlo Localization method and argue that it can exploit mobility to improve the accuracy and precision of localization. Our approach does not require additional hardware on the nodes and works even when the movement of seeds and nodes is uncontrollable. We analyze the properties of our technique and report experimental results from simulations. Our scheme outperforms the best known static localization schemes under a wide range of conditions.",
                "paper_link": "https://www.semanticscholar.org/paper/63bf17e4f0a0137d5eea3bffb0f5f953914e068d"
            },
            {
                "title": "Faster Secure Two-Party Computation Using Garbled Circuits",
                "abstract": "Secure two-party computation enables two parties to evaluate a function cooperatively without revealing to either party anything beyond the function\u2019s output. The garbled-circuit technique, a generic approach to secure two-party computation for semi-honest participants, was developed by Yao in the 1980s, but has been viewed as being of limited practical significance due to its inefficiency. We demonstrate several techniques for improving the running time and memory requirements of the garbled-circuit technique, resulting in an implementation of generic secure two-party computation that is significantly faster than any previously reported while also scaling to arbitrarily large circuits. We validate our approach by demonstrating secure computation of circuits with over 10 9 gates at a rate of roughly 10 ms per garbled gate, and showing order-of-magnitude improvements over the best previous privacy-preserving protocols for computing Hamming distance, Levenshtein distance, Smith-Waterman genome alignment, and AES.",
                "paper_link": "https://www.semanticscholar.org/paper/801efba3069e08f26658a5ec49f27f442b3ef80d"
            },
            {
                "title": "Using Directional Antennas to Prevent Wormhole Attacks",
                "abstract": "Wormhole attacks enable an attacker with limited resources and no cryptographic material to wreak havoc on wireless networks. To date, no general defenses against wormhole attacks have been proposed. This paper presents an analysis of wormhole attacks and proposes a countermeasure using directional antennas. We present a cooperative protocol whereby nodes share directional information to prevent wormhole endpoints from masquerading as false neighbors. Our defense greatly diminishes the threat of wormhole attacks and requires no location information or clock synchronization.",
                "paper_link": "https://www.semanticscholar.org/paper/36b4004efa1465a9f25ed925321be8980a31cb55"
            },
            {
                "title": "Improving Security using Extensible Lightweight Static Analysis",
                "abstract": "Most security attacks exploit instances of well-known classes of implementation flaws. Developers could detect and eliminate many of these flaws before deploying the software, yet these problems persist with disturbing frequency-not because the security community doesn't sufficiently understand them but because techniques for preventing them have not been integrated into the software development process. This article describes an extensible tool that uses lightweight static analysis to detect common security vulnerabilities (including buffer overflows and format string vulnerabilities).",
                "paper_link": "https://www.semanticscholar.org/paper/6119e9eedf900a2b47fc52ffa186e5bceeb76a8b"
            },
            {
                "title": "Secure Aggregation for Wireless Networks",
                "abstract": "An emerging class of important applications uses ad hoc wireless networks of low-power sensor devices to monitor and send information about a possibly hostile environment to a powerful base station connected to a wired network. To conserve power, intermediate network nodes should aggregate results from individual sensors. However, this opens the risk that a single compromised sensor device can render the network useless, or worse, mislead the operator into trusting a false reading. We present a protocol that provides a secure aggregation mechanism for wireless networks that is resilient to both intruder devices and single device key compromises. Our protocol is designed to work within the computation, memory and power consumption limits of inexpensive sensor devices, but takes advantage of the properties of wireless networking, as well as the power asymmetry between the devices and the base station.",
                "paper_link": "https://www.semanticscholar.org/paper/e6c637d87088df3a68e908608a8cdf85b600b6a8"
            },
            {
                "title": "Statically Detecting Likely Buffer Overflow Vulnerabilities",
                "abstract": "Network security has been paid more attention to and hackers often take advantage of buffer overflow to attack other systems or spread virus.Safe code will decrease the risk if programmers focus on the question strictly.A method to statically detecting likely buffer overflow vulnerabilities is introduced and it may help programmers to improve the security of software.",
                "paper_link": "https://www.semanticscholar.org/paper/c8db640c7d7dd77f18d6e3c56d94b3b00e04ff93"
            },
            {
                "title": "Evaluating Differentially Private Machine Learning in Practice",
                "abstract": "Differential privacy is a strong notion for privacy that can be used to prove formal guarantees, in terms of a privacy budget, $\\epsilon$, about how much information is leaked by a mechanism. However, implementations of privacy-preserving machine learning often select large values of $\\epsilon$ in order to get acceptable utility of the model, with little understanding of the impact of such choices on meaningful privacy. Moreover, in scenarios where iterative learning procedures are used, differential privacy variants that offer tighter analyses are used which appear to reduce the needed privacy budget but present poorly understood trade-offs between privacy and utility. In this paper, we quantify the impact of these choices on privacy in experiments with logistic regression and neural network models. Our main finding is that there is a huge gap between the upper bounds on privacy loss that can be guaranteed, even with advanced mechanisms, and the effective privacy loss that can be measured using current inference attacks. Current mechanisms for differentially private machine learning rarely offer acceptable utility-privacy trade-offs with guarantees for complex learning tasks: settings that provide limited accuracy loss provide meaningless privacy guarantees, and settings that provide strong privacy guarantees result in useless models. Code for the experiments can be found here: this https URL",
                "paper_link": "https://www.semanticscholar.org/paper/8f8542a6aa8c76e8a4441d1ca722e230aa5d6c9e"
            },
            {
                "title": "Two Halves Make a Whole: Reducing Data Transfer in Garbled Circuits using Half Gates",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/a4d2b85346dc87ae1b1f4529fbf13c9d53fe6f2d"
            },
            {
                "title": "Automatically Hardening Web Applications using Precise Tainting",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/adcd4d715ab68dafcc7678a873d4c1a8217232e8"
            },
            {
                "title": "Perracotta: Mining Temporal API Rules from Imperfect Traces",
                "abstract": "Dynamic inference techniques have been demonstrated to provide useful support for various software engineering tasks including bug finding, test suite evaluation and improvement, and specification generation. To date, however, dynamic inference has only been used effectively on small programs under controlled conditions. In this paper, we identify reasons why scaling dynamic inference techniques has proven difficult, and introduce solutions that enable a dynamic inference technique to scale to large programs and work effectively with the imperfect traces typically available in industrial scenarios. We describe our approximate inference algorithm, present and evaluate heuristics for winnowing the large number of inferred properties to a manageable set of interesting properties, and report on experiments using inferred properties. We evaluate our techniques on JBoss and the Windows kernel. Our tool is able to infer many of the properties checked by the Static Driver Verifier and leads us to discover a previously unknown bug in Windows.",
                "paper_link": "https://www.semanticscholar.org/paper/98614fb574f1592b0dae3b45ca9f13506d14dd86"
            },
            {
                "title": "Automatically Evading Classifiers",
                "abstract": "Machine learning is widely used to develop classifiers for security tasks. However, the robustness of these methods against motivated adversaries is uncertain. In this work, we propose a generic method to evaluate the robustness of classifiers under attack. The key idea is to stochastically manipulate a malicious sample to find a variant that preserves the malicious behavior but is classified as benign by the classifier. We present a general approach to search for evasive variants and report on results from experiments using our techniques against two PDF malware classifiers, PDFrate and Hidost. Our method is able to automatically find evasive variants for both classifiers for all of the 500 malicious seeds in our study. Our results suggest a general method for evaluating classifiers used in security applications, and raise serious doubts about the effectiveness of classifiers based on superficial features in the presence of adversaries.",
                "paper_link": "https://www.semanticscholar.org/paper/a0833bf06cadb612a2600f4c4442d003d521d78e"
            },
            {
                "title": "Private Set Intersection: Are Garbled Circuits Better than Custom Protocols?",
                "abstract": "Cryptographic protocols for Private Set Intersection (PSI) are the basis for many important privacy-preserving applications. Over the past few years, intensive research has been devoted to designing custom protocols for PSI based on homomorphic encryption and other public-key techniques, apparently due to the belief that solutions using generic approaches would be impractical. This paper explores the validity of that belief. We develop three classes of protocols targeted to different set sizes and domains, all based on Yao\u2019s generic garbled-circuit method. We then compare the performance of our protocols to the fastest custom PSI protocols in the literature. Our results show that a careful application of garbled circuits leads to solutions that can run on million-element sets on typical desktops, and that can be competitive with the fastest custom protocols. Moreover, generic protocols like ours can be used directly for performing more complex secure computations, something we demonstrate by adding a simple information-auditing mechanism to our PSI protocols.",
                "paper_link": "https://www.semanticscholar.org/paper/14720266a35ced804438cdf06bc8d151e7e9903c"
            },
            {
                "title": "A Pragmatic Introduction to Secure Multi-Party Computation",
                "abstract": "Since its introduction by Andrew Yao in the 1980s, multi-party computation has developed from a theoretical curiosity to an important tool for building large-scale privacy-preserving applications. Secure multi-party computation (MPC) enables a group to jointly perform a computation without disclosing any participant\u2019s private inputs. The participants agree on a function to compute, and then can use an MPC protocol to jointly compute the output of that function on their secret inputs without revealing them. This monograph provides an introduction to multi-party computation for practitioners interested in building privacy-preserving applications and researchers who want to work in the area. The authors introduce the foundations of MPC and describe the current state of the art. The goal is to enable readers to understand what is possible today, and what may be possible in the future. It provides a starting point for building applications using MPC and for developing MPC protocols, implementations, tools, and applications. Those seeking a concise, accessible introduction to the topic which quickly enables them to build practical systems or conduct further research will find this essential reading.",
                "paper_link": "https://www.semanticscholar.org/paper/ae3af798af4e74096fa1637e221343d596bdb68d"
            },
            {
                "title": "N-Variant Systems: A Secretless Framework for Security through Diversity",
                "abstract": "We present an architectural framework for systematically using automated diversity to provide high assurance detection and disruption for large classes of attacks. The framework executes a set of automatically diversified variants on the same inputs, and monitors their behavior to detect divergences. The benefit of this approach is that it requires an attacker to simultaneously compromise all system variants with the same input. By constructing variants with disjoint exploitation sets, we can make it impossible to carry out large classes of important attacks. In contrast to previous approaches that use automated diversity for security, our approach does not rely on keeping any secrets. In this paper, we introduce the N-variant systems framework, present a model for analyzing security properties of N-variant systems, define variations that can be used to detect attacks that involve referencing absolute memory addresses and executing injected code, and describe and present performance results from a prototype implementation.",
                "paper_link": "https://www.semanticscholar.org/paper/ca185be3b1d835e02701e58e525f1a3ae5f2bc5f"
            },
            {
                "title": "Reverse-Engineering a Cryptographic RFID Tag",
                "abstract": "The security of embedded devices often relies on the secrecy of proprietary cryptographic algorithms. These algorithms and their weaknesses are frequently disclosed through reverse-engineering software, but it is commonly thought to be too expensive to reconstruct designs from a hardware implementation alone. This paper challenges that belief by presenting an approach to reverse-engineering a cipher from a silicon implementation. Using this mostly automated approach, we reveal a cipher from an RFID tag that is not known to have a software or micro-code implementation. We reconstruct the cipher from the widely used Mifare Classic RFID tag by using a combination of image analysis of circuits and protocol analysis. Our analysis reveals that the security of the tag is even below the level that its 48-bit key length suggests due to a number of design flaws. Weak random numbers and a weakness in the authentication protocol allow for pre-computed rainbow tables to be used to find any key in a matter of seconds. Our approach of deducing functionality from circuit images is mostly automated, hence it is also feasible for large chips. The assumption that algorithms can be kept secret should therefore to be avoided for any type of silicon chip.",
                "paper_link": "https://www.semanticscholar.org/paper/2e8b48935688da9a20ff9b456bab581d0c1339f2"
            },
            {
                "title": "EnviroTrack: Towards an Environmental Computing Paradigm for Distributed Sensor Networks",
                "abstract": "Distributed sensor networks are quickly gaining recognition as viable embedded computing platforms. Current techniques for programming sensor networks are cumbersome, inflexible, and low-level. We introduce EnviroTrack, an object-based distributed middleware system that raises the level of programming abstraction by providing a convenient and powerful interface to the application developer geared towards tracking the physical environment. EnviroTrack is novel in its seamless integration of objects that live in physical time and space into the computational environment of the application. Performance results demonstrate the ability of the middleware to track realistic targets.",
                "paper_link": "https://www.semanticscholar.org/paper/5db1aef15f138d48a38873616970d9f15ff31362"
            },
            {
                "title": "LCLint: A Tool for Using Specifications to Check Code",
                "abstract": "This paper describes LCLint, an efficient and flexible tool that accepts as input programs (written in ANSI C) and various levels of formal specification. Using this information, LCLint reports inconsistencies between a program and its specification. We also describe our experience using LCLint to help understand, document, and re-engineer legacy code.",
                "paper_link": "https://www.semanticscholar.org/paper/4286f14f5c5d06685070b82719132e90e7c8136f"
            },
            {
                "title": "Privacy-Preserving Distributed Linear Regression on High-Dimensional Data",
                "abstract": "Abstract We propose privacy-preserving protocols for computing linear regression models, in the setting where the training dataset is vertically distributed among several parties. Our main contribution is a hybrid multi-party computation protocol that combines Yao\u2019s garbled circuits with tailored protocols for computing inner products. Like many machine learning tasks, building a linear regression model involves solving a system of linear equations. We conduct a comprehensive evaluation and comparison of different techniques for securely performing this task, including a new Conjugate Gradient Descent (CGD) algorithm. This algorithm is suitable for secure computation because it uses an efficient fixed-point representation of real numbers while maintaining accuracy and convergence rates comparable to what can be obtained with a classical solution using floating point numbers. Our technique improves on Nikolaenko et al.\u2019s method for privacy-preserving ridge regression (S&P 2013), and can be used as a building block in other analyses. We implement a complete system and demonstrate that our approach is highly scalable, solving data analysis problems with one million records and one hundred features in less than one hour of total running time.",
                "paper_link": "https://www.semanticscholar.org/paper/367191902e70f2259d2874de79c08c5bf01891cd"
            }
        ]
    },
    {
        "Professor": "Farzad Farnoud",
        "Papers": [
            {
                "title": "Reliable broadcast of safety messages in vehicular ad hoc networks",
                "abstract": "Broadcast communications is critically important in vehicular networks. Many safety applications need safety warning messages to be broadcast to all vehicles present in an area. Design of a medium access control (MAC) protocol for vehicular networks is an interesting problem because of challenges posed by broadcast traffic, high mobility, high reliability and low delay requirements of these networks. In this article, we propose a topology-transparent broadcast protocol and present a detailed mathematical analysis for obtaining the probability of success and the average delay. We show, by analysis and simulations, that the proposed protocol outperforms two existing protocols for vehicular networks with topology-transparent properties and provides reliable broadcast communications for delivering safety messages under load conditions deemed to be common in vehicular environments.",
                "paper_link": "https://www.semanticscholar.org/paper/4b0ca3d3c4f8dc1eed4cfa4a2a9877b5c281ef26"
            },
            {
                "title": "Duplication-correcting codes for data storage in the DNA of living organisms",
                "abstract": "The ability to store data in the DNA of a living organism has applications in a variety of areas including synthetic biology and watermarking of patented genetically-modified organisms. Data stored in this medium is subject to errors arising from various mutations, such as point mutations, indels, and tandem duplication, which need to be corrected to maintain data integrity. In this paper, we provide error-correcting codes for errors caused by tandem duplications, which create a copy of a block of the sequence and insert it in a tandem manner, i.e., next to the original. In particular, we present a family of codes for correcting errors due to tandem-duplications of a fixed length and any number of errors. We also study codes for correcting tandem duplications of length up to a given constant k, where we are primarily focused on the cases of k = 2, 3.",
                "paper_link": "https://www.semanticscholar.org/paper/09138168ec5cfeb2393446e7debe191f6e8727cf"
            },
            {
                "title": "Error-Correction in Flash Memories via Codes in the Ulam Metric",
                "abstract": "We consider rank modulation codes for flash memories that allow for handling arbitrary charge-drop errors. Unlike classical rank modulation codes used for correcting errors that manifest themselves as swaps of two adjacently ranked elements, the proposed translocation rank codes account for more general forms of errors that arise in storage systems. Translocations represent a natural extension of the notion of adjacent transpositions and as such may be analyzed using related concepts in combinatorics and rank modulation coding. Our results include derivation of the asymptotic capacity of translocation rank codes, construction techniques for asymptotically good codes, as well as simple decoding methods for one class of constructed codes. As part of our exposition, we also highlight the close connections between the new code family and permutations with short common subsequences, deletion and insertion error-correcting codes for permutations, and permutation codes in the Hamming distance.",
                "paper_link": "https://www.semanticscholar.org/paper/f6c0da02a569851beb8ab562ffb29fb90cb1ada7"
            },
            {
                "title": "Capacity and expressiveness of genomic tandem duplication",
                "abstract": "The majority of the human genome consists of repeated sequences. An important type of repeated sequences common in the human genome are tandem repeats, where identical copies appear next to each other. For example, in the sequence <inline-formula> <tex-math notation=\"LaTeX\">$AGTC\\underline {TGTG}C$ </tex-math></inline-formula>, <inline-formula> <tex-math notation=\"LaTeX\">$TGTG$ </tex-math></inline-formula> is a tandem repeat, that may be generated from <inline-formula> <tex-math notation=\"LaTeX\">$AGTCTGC$ </tex-math></inline-formula> by a tandem duplication of length 2. In this paper, we investigate the possibility of generating a large number of sequences from a <italic>seed</italic>, i.e. a small initial string, by tandem duplications of bounded length. We study the capacity of such a system, a notion that quantifies the system\u2019s generating power. Our results include <italic>exact capacity</italic> values for certain tandem duplication string systems. In addition, motivated by the role of DNA sequences in expressing proteins via RNA and the genetic code, we define the notion of the <italic>expressiveness</italic> of a tandem duplication system as the capability of expressing arbitrary substrings. We then <italic>completely</italic> characterize the expressiveness of tandem duplication systems for general alphabet sizes and duplication lengths. In particular, based on a celebrated result by Axel Thue from 1906, presenting a construction for ternary squarefree sequences, we show that for alphabets of size 4 or larger, bounded tandem duplication systems, regardless of the seed and the bound on duplication length, are not fully expressive, i.e. they cannot generate all strings even as substrings of other strings. Note that the alphabet of size 4 is of particular interest as it pertains to the genomic alphabet. Building on this result, we also show that these systems do not have full capacity. In general, our results illustrate that duplication lengths play a more significant role than the seed in generating a large number of sequences for these systems.",
                "paper_link": "https://www.semanticscholar.org/paper/8711059cbc8921ff84c45d898df292cfbb927917"
            },
            {
                "title": "The Capacity of String-Duplication Systems",
                "abstract": "It is known that the majority of the human genome consists of repeated sequences. Furthermore, it is believed that a significant part of the rest of the genome also originated from repeated sequences and has mutated to its current form. In this paper, we investigate the possibility of constructing an exponentially large number of sequences from a short initial sequence and simple duplication rules, including those resembling genomic duplication processes. In other words, our goal is to find out the capacity, or the expressive power, of these string-duplication systems. Our results include the exact capacities, and bounds on the capacities, of four fundamental string-duplication systems.",
                "paper_link": "https://www.semanticscholar.org/paper/c924721070aa56034e7649b0544e1789ba2dc681"
            },
            {
                "title": "The capacity of string-duplication systems",
                "abstract": "It is known that the majority of the human genome consists of repeated sequences. Furthermore, it is believed that a significant part of the rest of the genome also originated from repeated sequences and has mutated to its current form. In this paper, we investigate the possibility of constructing an exponentially large number of sequences from a short initial sequence and simple duplication rules, including those resembling genomic duplication processes. In other words, our goal is to find out the capacity, or the expressive power, of these string-duplication systems. Our results include the exact capacities, and bounds on the capacities, of four fundamental string-duplication systems.",
                "paper_link": "https://www.semanticscholar.org/paper/c924721070aa56034e7649b0544e1789ba2dc681"
            },
            {
                "title": "Repetition-based broadcast in vehicular ad hoc networks in Rician channel with capture",
                "abstract": "In this paper we study the performance of different vehicular wireless broadcast schemes that rely on repetition as a means for providing reliable communications in Rician environment with capture effect. We investigate different patterns for retransmission and show that the one based on Optical Orthogonal Codes (OOC) performs better than others in terms of probability of success and delay. We propose using different numbers of repetitions for providing different Quality of Service (QoS) priority levels and show this method can effectively provide different QoS classes for different types of data without throughput loss. Probability of success and delay are obtained via simulation for three broadcast schemes in the presence of capture effect in Rician fading environment. Furthermore, analytical solutions are compared to simulation for transmission with no capture as a special case.",
                "paper_link": "https://www.semanticscholar.org/paper/7b81042f923e50472c72950a383596f73ecc010b"
            },
            {
                "title": "HyDRA: gene prioritization via hybrid distance-score rank aggregation",
                "abstract": "UNLABELLED\nGene prioritization refers to a family of computational techniques for inferring disease genes through a set of training genes and carefully chosen similarity criteria. Test genes are scored based on their average similarity to the training set, and the rankings of genes under various similarity criteria are aggregated via statistical methods. The contributions of our work are threefold: (i) first, based on the realization that there is no unique way to define an optimal aggregate for rankings, we investigate the predictive quality of a number of new aggregation methods and known fusion techniques from machine learning and social choice theory. Within this context, we quantify the influence of the number of training genes and similarity criteria on the diagnostic quality of the aggregate and perform in-depth cross-validation studies; (ii) second, we propose a new approach to genomic data aggregation, termed HyDRA (Hybrid Distance-score Rank Aggregation), which combines the advantages of score-based and combinatorial aggregation techniques. We also propose incorporating a new top-versus-bottom (TvB) weighting feature into the hybrid schemes. The TvB feature ensures that aggregates are more reliable at the top of the list, rather than at the bottom, since only top candidates are tested experimentally; (iii) third, we propose an iterative procedure for gene discovery that operates via successful augmentation of the set of training genes by genes discovered in previous rounds, checked for consistency.\n\n\nMOTIVATION\nFundamental results from social choice theory, political and computer sciences, and statistics have shown that there exists no consistent, fair and unique way to aggregate rankings. Instead, one has to decide on an aggregation approach using predefined set of desirable properties for the aggregate. The aggregation methods fall into two categories, score- and distance-based approaches, each of which has its own drawbacks and advantages. This work is motivated by the observation that merging these two techniques in a computationally efficient manner, and by incorporating additional constraints, one can ensure that the predictive quality of the resulting aggregation algorithm is very high.\n\n\nRESULTS\nWe tested HyDRA on a number of gene sets, including autism, breast cancer, colorectal cancer, endometriosis, ischaemic stroke, leukemia, lymphoma and osteoarthritis. Furthermore, we performed iterative gene discovery for glioblastoma, meningioma and breast cancer, using a sequentially augmented list of training genes related to the Turcot syndrome, Li-Fraumeni condition and other diseases. The methods outperform state-of-the-art software tools such as ToppGene and Endeavour. Despite this finding, we recommend as best practice to take the union of top-ranked items produced by different methods for the final aggregated list.\n\n\nAVAILABILITY AND IMPLEMENTATION\nThe HyDRA software may be downloaded from: http://web.engr.illinois.edu/\u223cmkim158/HyDRA.zip.\n\n\nSUPPLEMENTARY INFORMATION\nSupplementary data are available at Bioinformatics online.",
                "paper_link": "https://www.semanticscholar.org/paper/3292f0c10ebbdfc0c61d17afb20c085a9d20b31a"
            },
            {
                "title": "An axiomatic approach to constructing distances for rank comparison and aggregation",
                "abstract": "We propose a new family of distance measures on rankings, derived through an axiomatic approach, that consider the nonuniform relevance of the top and bottom of ordered lists and similarities between candidates. The proposed distance functions include specialized weighted versions of the Kendall \u03c4 distance and the Cayley distance, and are suitable for comparing rankings in a number of applications, including information retrieval and rank aggregation. In addition to proposing the distance measures and providing the theoretical underpinnings for their applications, we also analyze algorithmic and computational aspects of weighted distance-based rank aggregation. We present an aggregation method based on approximating weighted distance measures by a generalized version of Spearman's footrule distance as well as a Markov chain method inspired by PageRank, where transition probabilities of the Markov chain reflect the chosen weighted distances.",
                "paper_link": "https://www.semanticscholar.org/paper/b405bea2b8d76364abcb5be2ba8edb372a348cb7"
            },
            {
                "title": "Codes correcting erasures and deletions for rank modulation",
                "abstract": "Error-correcting codes for permutations have received a considerable attention in the past few years, especially in applications of the rank modulation scheme for flash memories. While several metrics have been studied like the Kendall's \u03c4, Ulam, and Hamming distances, no recent research has been carried for erasures and deletions over permutations. The problems studied in this paper are motivated by a hardware implementation of the rank modulation codes. If the flash memory cells represent a permutation, which is modulated by their relative charge levels, then we explore the problems arise when some of the cells are either erased or deleted. In each case we study how these erasures and deletions affect the information carried by the remaining cells. In particular, the cells can either be stable and do not change their values in the permutation or unstable where the remaining cells form an induced permutation with less symbols. Yet another erasure model, called here soft erasures, assumes that all cells can be read, however the relative levels between some of the cells is not known. Our main approach in tackling these problems is to build upon the existing works of error-correcting codes in the three metrics mentioned above and leverage them in order to construct codes in each model of deletions and erasures. Lastly, we follow up on codes in the Ulam distance and improve upon the state of the art results.",
                "paper_link": "https://www.semanticscholar.org/paper/d480f6b8d025deda12176a7160a3009f4af1d479"
            },
            {
                "title": "Multipermutation codes in the Ulam metric for nonvolatile memories",
                "abstract": "We address the problem of multipermutation code design in the Ulam metric for novel storage applications. Multipermutation codes are suitable for flash memory where cell charges may share the same rank. Changes in the charges of cells manifest themselves as errors whose effects on the retrieved signal may be measured via the Ulam distance. As part of our analysis, we study multipermutation codes in the Hamming metric, known as constant composition codes. We then present bounds on the size of multipermutation codes and their capacity, for both the Ulam and the Hamming metrics. Finally, we present constructions and accompanying decoders for multipermutation codes in the Ulam metric.",
                "paper_link": "https://www.semanticscholar.org/paper/797895e217d7c2d415c01b5d15394e4acd60ab65"
            },
            {
                "title": "Message broadcast using optical orthogonal codes in vehicular communication systems",
                "abstract": "Broadcast communication is considered to be especially important in delivering safety messages in vehicular environments. In this paper, we introduce and explore a new method for message broadcast based on repetition. The proposed method uses optical orthogonal codes in vehicular broadcast communications to increase the probability of detection and reduce reception delay. We formulate a general framework in which we can examine and evaluate the performance of broadcast schemes based on repetition. This framework is used to compare our method to other proposed broadcast methods.",
                "paper_link": "https://www.semanticscholar.org/paper/8c309b9d725a1084292307fc74690cff5ddaced6"
            },
            {
                "title": "Sorting of Permutations by Cost-Constrained Transpositions",
                "abstract": "The problem of finding a minimum decomposition of a permutation in terms of transpositions with predetermined non-uniform and non-negative costs is addressed. Alternatively, computing the transposition distance between two permutations, where transpositions are endowed with arbitrary non-negative costs, is studied. For such cost functions, polynomial-time, constant-approximation decomposition algorithms are described. For metric-path costs, exact polynomial-time decomposition algorithms are presented. The algorithms in this paper represent a combination of Viterbi-type algorithms and graph-search techniques for minimizing the cost of individual transpositions, and dynamic programing algorithms for finding minimum cost decompositions of cycles. The presented algorithms have a myriad of applications in information theory, bioinformatics, and algebra.",
                "paper_link": "https://www.semanticscholar.org/paper/30ea01c830ff39fedad4ec9908135cf447e685ef"
            },
            {
                "title": "Single-error detection and correction for duplication and substitution channels",
                "abstract": "Motivated by mutation processes occurring in in-vivo DNA-storage applications, a channel that mutates stored strings by duplicating substrings as well as substituting symbols is studied. Two models of such a channel are considered: one in which the substitutions occur only within the duplicated substrings, and one in which the location of substitutions is unrestricted. Both error-detecting and error-correcting codes are constructed, which can handle correctly any number of tandem duplications of a fixed length k, and at most a single substitution occurring at any time during the mutation process.",
                "paper_link": "https://www.semanticscholar.org/paper/9566bf50740b81332eeb64b40959b04e954ebae4"
            },
            {
                "title": "Building consensus via iterative voting",
                "abstract": "In networked systems comprised of many agents, it is often required to reach a common operating point of all agents, termed the network consensus. We consider two iterative methods for reaching a ranking (ordering) consensus over a voter network, where the initial preference of every voter is of the form of a full ranking of candidates. The voters are allowed, one at a time and based on some random scheme, to change their votes to bring them \u201ccloser\u201d to the opinions of selected subsets of peers. The first consensus method is based on changing votes one adjacent swap at a time; the second method is based on changing votes via averaging with the votes of peers, potentially leading to many adjacent swaps at a given time. For the first model, we characterize convergence points and conditions for convergence. For the second model, we prove convergence to a global ranking and derive the rate of convergence to this consensus.",
                "paper_link": "https://www.semanticscholar.org/paper/defe4d064a2f878c407d2bbf6f66238936e2d6bc"
            },
            {
                "title": "Coding for optimized writing rate in DNA storage",
                "abstract": "A method for encoding information in DNA sequences is described. The method is based on the precisionresolution framework, and is aimed to work in conjunction with a recently suggested terminator-free template independent DNA synthesis method. The suggested method optimizes the amount of information bits per synthesis time unit, namely, the writing rate. Additionally, the encoding scheme studied here takes into account the existence of multiple copies of the DNA sequence, which are independently distorted. Finally, quantizers for various run-length distributions are designed.",
                "paper_link": "https://www.semanticscholar.org/paper/68d95fb6427c8525d2b5c5458ffd72e9b3ec9b9a"
            },
            {
                "title": "Estimation of duplication history under a stochastic model for tandem repeats",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/c73927fdf6ecb59934a5cb24e087936f37462832"
            },
            {
                "title": "Error-correcting codes for noisy duplication channels",
                "abstract": "Because of its high data density and longevity, DNA is emerging as a promising candidate for satisfying increasing data storage needs. Compared to conventional storage media, however, data stored in DNA is subject to a wider range of errors resulting from various processes involved in the data storage pipeline. In this article, we consider correcting duplication errors for both exact and noisy tandem duplications of a given length <inline-formula> <tex-math notation=\"LaTeX\">$k$ </tex-math></inline-formula>. An exact duplication inserts a copy of a substring of length <inline-formula> <tex-math notation=\"LaTeX\">$k$ </tex-math></inline-formula> of the sequence immediately after that substring, e.g., <inline-formula> <tex-math notation=\"LaTeX\">$\\mathsf {ACGT} \\to \\mathsf {ACG\\underline {ACG}T}$ </tex-math></inline-formula>, where <inline-formula> <tex-math notation=\"LaTeX\">$k=3$ </tex-math></inline-formula>, while a noisy duplication inserts a copy suffering from substitution noise, e.g., <inline-formula> <tex-math notation=\"LaTeX\">$\\mathsf {ACGT} \\to \\mathsf {ACG\\underline {A \\color {Red}{T}}GT}$ </tex-math></inline-formula>. Specifically, we design codes that can correct any number of exact duplication and one noisy duplication errors, where in the noisy duplication case the copy is at Hamming distance 1 from the original. Our constructions rely upon recovering the duplication root of the stored codeword. We characterize the ways in which duplication errors manifest in the root of affected sequences and design efficient codes for correcting these error patterns. We show that the proposed construction is asymptotically optimal, in the sense that it has the same asymptotic rate as optimal codes correcting exact duplications only.",
                "paper_link": "https://www.semanticscholar.org/paper/c619a005c4cd913b76e6a571e0e85c6446db3b68"
            },
            {
                "title": "Rank aggregation via heterogeneous thurstone preference models",
                "abstract": "We propose the Heterogeneous Thurstone Model (HTM) for aggregating ranked data, which can take the accuracy levels of different users into account. By allowing different noise distributions, the proposed HTM model maintains the generality of Thurstone's original framework, and as such, also extends the Bradley-Terry-Luce (BTL) model for pairwise comparisons to heterogeneous populations of users. Under this framework, we also propose a rank aggregation algorithm based on alternating gradient descent to estimate the underlying item scores and accuracy levels of different users simultaneously from noisy pairwise comparisons. We theoretically prove that the proposed algorithm converges linearly up to a statistical error which matches that of the state-of-the-art method for the single-user BTL model. We evaluate the proposed HTM model and algorithm on both synthetic and real data, demonstrating that it outperforms existing methods.",
                "paper_link": "https://www.semanticscholar.org/paper/1a26628140819fd4d378b4f129f8749df6a15379"
            },
            {
                "title": "A packet-based photonic label switching router for a multirate all-optical CDMA-based GMPLS switch",
                "abstract": "A novel packet-based photonic label switching router for a multirate all-optical switch using generalized multiprotocol label switching is proposed. The idea is based on using optical code-division multiple access (OCDMA) as multiplexing technique and treating OCDMA codes as labels. The system can coexist with current wavelength division multiplexing systems on the same infrastructure. The concept of switch fabric is introduced. Label processing and label swapping functionalities of the switch are discussed. In-depth analyses are made for spectrally phase-encoded OCDMA (SPE-OCDMA) due to its capabilities of supporting high data rates, large code cardinality, and its secure transmission. Packet loss rate for a multirate SPE-OCDMA system is derived. Some performance metrics are derived for a typical network via simulation, and the results are discussed.",
                "paper_link": "https://www.semanticscholar.org/paper/282dfc44c13c88b55c24de3d19b69ed916dd2c89"
            }
        ]
    },
    {
        "Professor": "Lu Feng",
        "Papers": [
            {
                "title": "Synthesis of human-in-the-loop control protocols for autonomous systems",
                "abstract": "We propose an approach to synthesize control protocols for autonomous systems that account for uncertainties and imperfections in interactions with human operators. As an illustrative example, we consider a scenario involving road network surveillance by an unmanned aerial vehicle (UAV) that is controlled remotely by a human operator but also has a certain degree of autonomy. Depending on the type (i.e., probabilistic and/or nondeterministic) of knowledge about the uncertainties and imperfections in the human-automation interactions, we use abstractions based on Markov decision processes and augment these models to stochastic two-player games. Our approach enables the synthesis of operator-dependent optimal mission plans for the UAV, highlighting the effects of operator characteristics (e.g., workload, proficiency, and fatigue) on UAV mission performance. It can also provide informative feedback (e.g., Pareto curves showing the tradeoffs between multiple mission objectives), potentially assisting the operator in decision-making. We demonstrate the applicability of our approach via a detailed UAV mission planning case study.",
                "paper_link": "https://www.semanticscholar.org/paper/b81edb05aaba154dd34ec18e2a17994b46832e3a"
            },
            {
                "title": "Safe multi-agent reinforcement learning via shielding",
                "abstract": "Multi-agent reinforcement learning (MARL) has been increasingly used in a wide range of safety-critical applications, which require guaranteed safety (e.g., no unsafe states are ever visited) during the learning process.Unfortunately, current MARL methods do not have safety guarantees. Therefore, we present two shielding approaches for safe MARL. In centralized shielding, we synthesize a single shield to monitor all agents' joint actions and correct any unsafe action if necessary. In factored shielding, we synthesize multiple shields based on a factorization of the joint state space observed by all agents; the set of shields monitors agents concurrently and each shield is only responsible for a subset of agents at each step.Experimental results show that both approaches can guarantee the safety of agents during learning without compromising the quality of learned policies; moreover, factored shielding is more scalable in the number of agents than centralized shielding.",
                "paper_link": "https://www.semanticscholar.org/paper/e52fe67096e4feec853a440c6a18bd8f2cb6a5ee"
            },
            {
                "title": "Deeptake: Prediction of driver takeover behavior using multimodal data",
                "abstract": "Automated vehicles promise a future where drivers can engage in non-driving tasks without hands on the steering wheels for a prolonged period. Nevertheless, automated vehicles may still need to occasionally hand the control back to drivers due to technology limitations and legal requirements. While some systems determine the need for driver takeover using driver context and road condition to initiate a takeover request, studies show that the driver may not react to it. We present DeepTake, a novel deep neural network-based framework that predicts multiple aspects of takeover behavior to ensure that the driver is able to safely take over the control when engaged in non-driving tasks. Using features from vehicle data, driver biometrics, and subjective measurements, DeepTake predicts the driver\u2019s intention, time, and quality of takeover. We evaluate DeepTake performance using multiple evaluation metrics. Results show that DeepTake reliably predicts the takeover intention, time, and quality, with an accuracy of 96%, 93%, and 83%, respectively. Results also indicate that DeepTake outperforms previous state-of-the-art methods on predicting driver takeover time and quality. Our findings have implications for the algorithm development of driver monitoring and state detection.",
                "paper_link": "https://www.semanticscholar.org/paper/96636e0076fcea6e848f37481fd27b6af37e78fa"
            },
            {
                "title": "Automated learning of probabilistic assumptions for compositional reasoning",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/61ec8d9ab0bbe4ee5285a3e0bc065cd8abc44094"
            },
            {
                "title": "Compositional verification of probabilistic systems using learning",
                "abstract": "We present a fully automated technique for compositional verification of probabilistic systems. Our approach builds upon a recently proposed assume-guarantee framework for probabilistic automata, in which assumptions and guarantees are probabilistic safety properties, represented using finite automata. A limitation of this work is that the assumptions need to be created manually. To overcome this, we propose a novel learning technique based on the L* algorithm, which automatically generates probabilistic assumptions using the results of queries executed by a probabilistic model checker. Learnt assumptions either establish satisfaction of the verification problem or are used to generate a probabilistic counterexample that refutes it. In the case where an assumption cannot be generated, lower and upper bounds on the probability of satisfaction are produced. We illustrate the applicability of the approach on a range of case studies.",
                "paper_link": "https://www.semanticscholar.org/paper/73c03b0e41e043ac2a59e3c1cef5b8356c05fb63"
            },
            {
                "title": "Controller synthesis for autonomous systems interacting with human operators",
                "abstract": "We propose an approach to synthesize control protocols for autonomous systems that account for uncertainties and imperfections in interactions with human operators. As an illustrative example, we consider a scenario involving road network surveillance by an unmanned aerial vehicle (UAV) that is controlled remotely by a human operator but also has a certain degree of autonomy. Depending on the type (i.e., probabilistic and/or nondeterministic) of knowledge about the uncertainties and imperfections in the operator-autonomy interactions, we use abstractions based on Markov decision processes and augment these models to stochastic two-player games. Our approach enables the synthesis of operator-dependent optimal mission plans for the UAV, highlighting the effects of operator characteristics (e.g., workload, proficiency, and fatigue) on UAV mission performance; it can also provide informative feedback (e.g., Pareto curves showing the trade-offs between multiple mission objectives), potentially assisting the operator in decision-making.",
                "paper_link": "https://www.semanticscholar.org/paper/25c81c0ff458ec782e4ec06eb9aa6df7bfd5f026"
            },
            {
                "title": "Medirl: Predicting the visual attention of drivers via maximum entropy deep inverse reinforcement learning",
                "abstract": "Inspired by human visual attention, we propose a novel inverse reinforcement learning formulation using Maximum Entropy Deep Inverse Reinforcement Learning (MEDIRL) for predicting the visual attention of drivers in accident-prone situations. MEDIRL predicts fixation locations that lead to maximal rewards by learning a task-sensitive reward function from eye fixation patterns recorded from attentive drivers. Additionally, we introduce EyeCar, a new driver attention dataset in accident-prone situations. We conduct comprehensive experiments to evaluate our proposed model on three common benchmarks: (DR(eye)VE, BDD-A, DADA-2000), and our EyeCar dataset. Results indicate that MEDIRL outperforms existing models for predicting attention and achieves state-of-the-art performance. We present extensive ablation studies to provide more insights into different features of our proposed model.1",
                "paper_link": "https://www.semanticscholar.org/paper/7983b849d0b8378e030644cf3b1c92308ffc52e9"
            },
            {
                "title": "Learning-based compositional verification for synchronous probabilistic systems",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/41f4f10f5457f8b1bcb68690da52bac253090b2b"
            },
            {
                "title": "Cityresolver: a decision support system for conflict resolution in smart cities",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "STLnet: Signal temporal logic enforced multivariate recurrent neural networks",
                "abstract": "Recurrent Neural Networks (RNNs) have made great achievements for sequential prediction tasks. In practice, the target sequence often follows certain model properties or patterns (e.g., reasonable ranges, consecutive changes, resource constraint, temporal correlations between multiple variables, existence, unusual cases, etc.). However, RNNs cannot guarantee their learned distributions satisfy these properties. It is even more challenging for the prediction of large-scale and complex Cyber-Physical Systems. Failure to produce outcomes that meet these properties will result in inaccurate and even meaningless results. In this paper, we develop a new temporal logic-based learning framework, STLnet, which guides the RNN learning process with auxiliary knowledge of model properties, and produces a more robust model for improved future predictions. Our framework can be applied to general sequential deep learning models, and trained in an end-to-end manner with back-propagation. We evaluate the performance of STLnet using large-scale real-world city data. The experimental results show STLnet not only improves the accuracy of predictions, but importantly also guarantees the satisfaction of model properties and increases the robustness of RNNs.",
                "paper_link": "https://www.semanticscholar.org/paper/121a8f2f296eeb6579c4d00eeda0dc33b78af61c"
            },
            {
                "title": "A Case Study of Trust on Autonomous Driving*",
                "abstract": "As autonomous vehicles have benefited the society, understanding the dynamic change of humans\u2019 trust during human-autonomous vehicle interaction can help to improve the safety and performance of autonomous driving. We designed and conducted a human subjects study involving 19 participants. Each participant was asked to enter their trust level in a Likert scale in real-time during experiments on a driving simulator. We also collected physiological data (e.g., heart rate, pupil size) of participants as complementary indicators of trust. We used analysis of variance (ANOVA) and Signal Temporal Logic (STL) to analyze the experimental data. Our results show the influence of different factors (e.g., automation alarms, weather conditions) on trust, and the individual variability in human reaction time and trust change.",
                "paper_link": "https://www.semanticscholar.org/paper/cd130dfbc0d88b9b5691f0ec53d98e6bdc1a5b9a"
            },
            {
                "title": "A novel spatial\u2013temporal specification-based monitoring system for smart cities",
                "abstract": "With the development of the Internet of Things, millions of sensors are being deployed in cities to collect real-time data. This leads to a need for checking city states against city requirements at runtime. In this article, we develop a novel spatial\u2013temporal specification-based monitoring system for smart cities. We first describe a study of over 1000 smart city requirements, some of which cannot be specified using the existing logic, such as the signal temporal logic (STL) and its variants. To tackle this limitation, we develop spatial aggregation STL (SaSTL)\u2014a novel spatial aggregation STL\u2014for the efficient runtime monitoring of safety and performance requirements in smart cities. We develop two new logical operators in SaSTL to augment STL for expressing spatial aggregation and spatial counting characteristics that are commonly found in real city requirements. We define the Boolean and quantitative semantics for SaSTL in support of the analysis of city performance across different periods and locations. We also develop efficient monitoring algorithms that can check the SaSTL requirement in parallel over multiple data streams (e.g., generated by multiple sensors distributed spatially in a city). Additionally, we build an SaSTL-based monitoring tool to support decision making of different stakeholders to specify and runtime monitor their requirements in smart cities. We evaluate our SaSTL monitor by applying it to three case studies with large-scale real city sensing data (e.g., up to 10 000 sensors in one study). The results show that SaSTL has a much higher coverage expressiveness than other spatial\u2013temporal logics, and with a significant reduction of computation time for monitoring requirements. We also demonstrate that the SaSTL monitor improves the safety and performance of smart cities via simulated experiments.",
                "paper_link": "https://www.semanticscholar.org/paper/6c1d1a4ec900bb50fa8eb7aef49358c78469c259"
            },
            {
                "title": "SaSTL: Spatial aggregation signal temporal logic for runtime monitoring in smart cities",
                "abstract": "We present SaSTL\u2014a novel Spatial Aggregation Signal Temporal Logic\u2014for the efficient runtime monitoring of safety and performance requirements in smart cities. We first describe a study of over 1,000 smart city requirements, some of which can not be specified using existing logic such as Signal Temporal Logic (STL) and its variants. To tackle this limitation, we develop two new logical operators in SaSTL to augment STL for expressing spatial aggregation and spatial counting characteristics that are commonly found in real city requirements. We also develop efficient monitoring algorithms that can check a SaSTL requirement in parallel over multiple data streams (e.g., generated by multiple sensors distributed spatially in a city). We evaluate our SaSTL monitor by applying to two case studies with large-scale real city sensing data (e.g., up to 10,000 sensors in one requirement). The results show that SaSTL has a much higher coverage expressiveness than other spatial-temporal logics, and with a significant reduction of computation time for monitoring requirements. We also demonstrate that the SaSTL monitor can help improve the safety and performance of smart cities via simulated experiments.",
                "paper_link": "https://www.semanticscholar.org/paper/8ac6014b175e6558d0e15ffa7d1e16f8c84ceb9d"
            },
            {
                "title": "Predictive monitoring with logic-calibrated uncertainty for cyber-physical systems",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Assuring the safety of on-demand medical cyber-physical systems",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Toward Policy Explanations for Multi-Agent Reinforcement Learning",
                "abstract": "Advances in multi-agent reinforcement learning (MARL) enable sequential decision making for a range of exciting multi-agent applications such as cooperative AI and autonomous driving. Explaining agent decisions is crucial for improving system transparency, increasing user satisfaction, and facilitating human-agent collaboration. However, existing works on explainable reinforcement learning mostly focus on the single-agent setting and are not suitable for addressing challenges posed by multi-agent environments. We present novel methods to generate two types of policy explanations for MARL: (i) policy summarization about the agent cooperation and task sequence, and (ii) language explanations to answer queries about agent behavior. Experimental results on three MARL domains demonstrate the scalability of our methods. A user study shows that the generated explanations significantly improve user performance and increase subjective ratings on metrics such as user satisfaction.",
                "paper_link": "https://www.semanticscholar.org/paper/036b7c0546691eda0258945bf38cea2140c560f7"
            },
            {
                "title": "The effect of whole-body haptic feedback on driver\u2019s perception in negotiating a curve",
                "abstract": "It remains uncertain regarding the safety of driving in autonomous vehicles that, after a long, passive control and inattention to the driving situation, how the drivers will be effectively informed to take-over the control in emergency. In particular, the active role of vehicle force feedback on the driver\u2019s risk perception on curves has not been fully explored. To investigate it, the current paper examined the driver\u2019s cognitive and visual responses to the whole-body haptic feedback during curve negotiations. The effects of force feedback on drivers\u2019 responses on curves were investigated in a high-fidelity driving simulator while measuring EEG and visual gaze over ten participants. The preliminary analyses of the first two participants revealed that pupil diameter and fixation time on the curves were significantly longer when the driver received whole-body feedback, compared to none. The findings suggest that whole-body feedback can be used as an effective \u201cadvance notification\u201d of hazards.",
                "paper_link": "https://www.semanticscholar.org/paper/df705592330aaab42b5df9ef24b90d948220d08d"
            },
            {
                "title": "Trust-based route planning for automated vehicles",
                "abstract": "Several recent works consider the personalized route planning based on user profiles, none of which accounts for human trust. We argue that human trust is an important factor to consider when planning routes for automated vehicles. This paper presents the first trust-based route planning approach for automated vehicles. We formalize the human-vehicle interaction as a partially observable Markov decision process (POMDP) and model trust as a partially observable state variable of the POMDP, representing human's hidden mental state. We designed and conducted an online user study with 100 participants on the Amazon Mechanical Turk platform to collect data of users' trust in automated vehicles. We build data-driven models of trust dynamics and takeover decisions, which are incorporated in the POMDP framework. We compute optimal routes for automated vehicles by solving optimal policies in the POMDP planning. We evaluated the resulting routes via human subject experiments with 22 participants on a driving simulator. The experimental results show that participants taking the trust-based route generally resulted in higher cumulative POMDP rewards and reported more positive responses in the after-driving survey than those taking the baseline trust-free route.",
                "paper_link": "https://www.semanticscholar.org/paper/12ba6185ab1d45525a6b2be4453e34da752b8b24"
            },
            {
                "title": "Toward minimum startle after take-over request: A preliminary study of physiological data",
                "abstract": "In this work, we introduce the preliminary analysis of driver\u2019s physiological data after receiving take-over request (TOR). Studies have shown that physiological measurements on drivers may provide better insights into the cognitive behavior and performance of drivers. Our goal is to examine the effect of two common TOR modalities (visual-auditory and generic auditory), in the limited take-over time budget, on psychophysical states and take-over behavior of the drivers. We applied multimodal physiological data streams -i.e. eye-tracker, EEG, GSR and PPG to have a comprehensive overview of driver\u2019s workload, stress and reaction time for each TOR modality. The preliminary results suggest that visual-auditory modality leads to a safer take-over behavior than generic auditory tone. EEG and heart rate variability results showed a significantly greater engagement on the visual-auditory TOR than for the auditory TOR. Results of this study can be used to investigate the safer modality inducing the least startle reactions.",
                "paper_link": "https://www.semanticscholar.org/paper/8578ef6f26bb8b1280e7664f45054c211c127625"
            },
            {
                "title": "Attack-resilient sensor fusion for cooperative adaptive cruise control",
                "abstract": "Cooperative adaptive cruise control (CACC) has the potential to enable vehicle platooning and achieve benefits including improved highway throughput and reduced energy consumption. However, malicious attacks such as sensor jamming or data injection can lead to security vulnerabilities of vehicle platooning and cause catastrophic crashes. We present a novel attack-resilience sensor fusion method for vehicle platooning with CACC, which exploits spatial information provided by multiple vehicles and combines sensor readings to achieve more precise estimation. We demonstrate the feasibility of our method in a set of simulated vehicle platooning experiments with different CACC controllers and malicious attacks.",
                "paper_link": "https://www.semanticscholar.org/paper/a7e4485481f4b118d92e3b7b8bb5674c6589907a"
            }
        ]
    },
    {
        "Professor": "Matheus Venturyne Xavier Ferreira",
        "Papers": [
            {
                "title": "Dynamic posted-price mechanisms for the blockchain transaction-fee market",
                "abstract": "In recent years, prominent blockchain systems such as Bitcoin and Ethereum have experienced explosive growth in transaction volume, leading to frequent surges in demand for limited block space and causing transaction fees to fluctuate by orders of magnitude. The status quo auctions sell space using a first-price auction [27]; however, users find it difficult to estimate how much they need to bid in order to get their transactions accepted onto the chain. If they bid too low, their transactions can have long confirmation times. If they bid too high, they pay larger fees than necessary. In light of these issues, new transaction fee mechanisms have been proposed, most notably EIP-1559 [4], aiming to provide better usability. EIP-1559 is a history-dependent mechanism that relies on block utilization to adjust a base fee. We propose an alternative design - a dynamic posted-price mechanism - which uses not only block utilization but also observable bids from past blocks to compute a posted-price for subsequent blocks. We show its potential to reduce price volatility by providing examples for which the prices of EIP-1559 are unstable while the prices of the proposed mechanism are stable. More generally, whenever the demand for the blockchain stabilizes, we ask if our mechanism is able to converge to a stable state. Our main result provides sufficient conditions in a probabilistic setting for which the proposed mechanism is approximately welfare optimal and the prices are stable. Our main technical contribution towards establishing stability is an iterative algorithm that, given oracle access to a Lipschitz continuous and strictly concave function f, converges to a fixed point of f.",
                "paper_link": "https://www.semanticscholar.org/paper/95677e1dafc1e4253e2297682176a360cc68d790"
            },
            {
                "title": "Credible Decentralized Exchange Design via Verifiable Sequencing Rules",
                "abstract": "Trading on decentralized exchanges has been one of the primary use cases for permissionless blockchains with daily trading volume exceeding billions of U.S.\u2004dollars. In the status quo, users broadcast transactions they wish to execute in the exchange and miners are responsible for composing a block of transactions and picking an execution ordering \u2014 the order in which transactions execute in the exchange. Due to the lack of a regulatory framework, it is common to observe miners exploiting their privileged position by front-running transactions and obtaining risk-fee profits. Indeed, the Flashbots service institutionalizes this exploit, with miners auctioning the right to front-run transactions. In this work, we propose to modify the interaction between miners and users and initiate the study of verifiable sequencing rules. As in the status quo, miners can determine the content of a block; however, they commit to respecting a sequencing rule that constrains the execution ordering and is verifiable (there is a polynomial time algorithm that can verify if the execution ordering satisfies such constraints). Thus in the event a miner deviates from the sequencing rule, anyone can generate a proof of non-compliance. We ask if there are sequencing rules that limit price manipulation from miners in a two-token liquidity pool exchange. Our first result is an impossibility theorem: for any sequencing rule, there is an instance of user transactions where the miner can obtain non-zero risk-free profits. In light of this impossibility result, our main result is a verifiable sequencing rule that provides execution price guarantees for users. In particular, for any user transaction A, it ensures that either (1) the execution price of A is at least as good as if A was the only transaction in the block, or (2) the execution price of A is worse than this \u201cstandalone\u201d price and the miner does not gain when including A in the block. Our framework does not require users to use countermeasures against predatory trading strategies, for example, set limit prices or split large transactions into smaller ones. This is likely to improve user experience relative to the status quo.",
                "paper_link": "https://www.semanticscholar.org/paper/cc34f5903b913f7ec513b7a54a5f5255169de1b8"
            },
            {
                "title": "Credible, truthful, and two-round (optimal) auctions via cryptographic commitments",
                "abstract": "We consider the sale of a single item to multiple buyers by a revenue-maximizing seller. Recent work of Akbarpour and Li formalizes credibility as an auction desideratum, and prove that the only optimal, credible, strategyproof auction is the ascending price auction with reserves. In contrast, when buyers' valuations are MHR, we show that the mild additional assumption of a cryptographically secure commitment scheme suffices for a simple two-round auction which is optimal, strategyproof, and credible (even when the number of bidders is only known by the auctioneer). We extend our analysis to the case when buyer valuations are \u03b1-strongly regular for any \u03b1 > 0, up to arbitrary \u03b5 in credibility. Interestingly, we also prove that this construction cannot be extended to regular distributions, nor can the \u03b5 be removed with multiple bidders.",
                "paper_link": "https://www.semanticscholar.org/paper/33b42aff9dd7e02087e1a7fede5664758c3b9468"
            },
            {
                "title": "Proof-of-stake mining games with perfect randomness",
                "abstract": "Proof-of-Stake blockchains based on a longest-chain consensus protocol are an attractive energy-friendly alternative to the Proof-of-Work paradigm. However, formal barriers to \"getting the incentives right\" were recently discovered, driven by the desire to use the blockchain itself as a source of pseudorandomness. We consider instead a longest-chain Proof-of-Stake protocol with perfect, trusted, external randomness (e.g. a randomness beacon). We produce two main results. First, we show that a strategic miner can strictly outperform an honest miner with just 32.8% of the total stake. Note that a miner of this size cannot outperform an honest miner in the Proof-of-Work model. This establishes that even with access to a perfect randomness beacon, incentives in Proof-of-Work and Proof-of-Stake longest-chain protocols are fundamentally different. Second, we prove that a strategic miner cannot outperform an honest miner with 30.8% of the total stake. This means that, while not quite as secure as the Proof-of-Work regime, desirable incentive properties of Proof-of-Work longest-chain protocols can be approximately recovered via Proof-of-Stake with a perfect randomness beacon. The space of possible strategies in a Proof-of-Stake mining game is significantly richer than in a Proof-of-Work game. Our main technical contribution is a characterization of potentially optimal strategies for a strategic miner, and in particular a proof that the corresponding infinite-state MDP admits an optimal strategy that is positive recurrent.",
                "paper_link": "https://www.semanticscholar.org/paper/c358834843ad9053c0f50443f6327ce646164dca"
            },
            {
                "title": "Credible, optimal auctions via blockchains",
                "abstract": "We study auction design in a setting where agents can communicate over a censorship-resistant broadcast channel like the ones we can implement over a public blockchain. We seek to design credible, strategyproof auctions in a model that differs from the traditional mechanism design framework because communication is not centralized via the auctioneer. We prove this allows us to design a larger class of credible auctions where the auctioneer has no incentive to be strategic. Intuitively, a decentralized communication model weakens the auctioneer's adversarial capabilities because they can only inject messages into the communication channel but not delete, delay, or modify the messages from legitimate buyers. Our main result is a separation in the following sense: we give the first instance of an auction that is credible only if communication is decentralized. Moreover, we construct the first two-round auction that is credible, strategyproof, and optimal when bidder valuations are $\\alpha$-strongly regular, for $\\alpha>0$. Our result relies on mild assumptions -- namely, the existence of a broadcast channel and cryptographic commitments.",
                "paper_link": "https://www.semanticscholar.org/paper/73c09981a2143e1611f89da550dde88148a11f57"
            },
            {
                "title": "Credible, strategyproof, optimal, and bounded expected-round single-item auctions for all distributions",
                "abstract": "We consider a revenue-maximizing seller with a single item for sale to multiple buyers with i.i.d. valuations. Akbarpour and Li (2020) show that the only optimal, credible, strategyproof auction is the ascending price auction with reserves which has unbounded communication complexity. Recent work of Ferreira and Weinberg (2020) circumvents their impossibility result assuming the existence of cryptographically secure commitment schemes, and designs a two-round credible, strategyproof, optimal auction. However, their auction is only credible when buyers' valuations are MHR or $\\alpha$-strongly regular: they show their auction might not be credible even when there is a single buyer drawn from a non-MHR distribution. In this work, under the same cryptographic assumptions, we identify a new single-item auction that is credible, strategyproof, revenue optimal, and terminates in constant rounds in expectation for all distributions with finite monopoly price.",
                "paper_link": "https://www.semanticscholar.org/paper/0c883850e04a4df803bd1e003226dad79de2e9bf"
            },
            {
                "title": "Selling a Single Item with Negative Externalities",
                "abstract": "We consider the problem of regulating products with negative externalities to a third party that is neither the buyer nor the seller, but where both the buyer and seller can take steps to mitigate the externality. The motivating example to have in mind is the sale of Internet-of-Things (IoT) devices, many of which have historically been compromised for DDoS attacks that disrupted Internet-wide services such as Twitter [5, 26]. Neither the buyer (i.e., consumers) nor seller (i.e., IoT manufacturers) was known to suffer from the attack, but both have the power to expend effort to secure their devices. We consider a regulator who regulates payments (via fines if the device is compromised, or market prices directly), or the product directly via mandatory security requirements. Both regulations come at a cost-implementing security requirements increases production costs, and the existence of fines decreases consumers' values-thereby reducing the seller's profits. The focus of this paper is to understand the efficiency of various regulatory policies. That is, policy A is more efficient than policy B if A more successfully minimizes negatives externalities, while both A and B reduce seller's profits equally. We develop a simple model to capture the impact of regulatory policies on a buyer's behavior. In this model, we show that for homogeneous markets-where the buyer's ability to follow security practices is always high or always low-the optimal (externality-minimizing for a given profit constraint) regulatory policy need regulate only payments or production. In arbitrary markets, by contrast, we show that while the optimal policy may require regulating both aspects, there is always an approximately optimal policy which regulates just one.",
                "paper_link": "https://www.semanticscholar.org/paper/a173ffa5662d779ca4b4ac25253c5f4d5baecacb"
            },
            {
                "title": "Optimal strategic mining against cryptographic self-selection in proof-of-stake",
                "abstract": "Cryptographic Self-Selection is a subroutine used to select a leader for modern proof-of-stake consensus protocols. In cryptographic self-selection, each round r has a seed Qr. In round r, each account owner is asked to digitally sign Qr, hash their digital signature to produce a credential, and then broadcast this credential to the entire network. A publicly-known function scores each credential in a manner so that the distribution of the lowest scoring credential is identical to the distribution of stake owned by each account. The user who broadcasts the lowest-scoring credential is the leader for round r, and their credential becomes the seed Qr+1. Such protocols leave open the possibility of manipulation: a user who owns multiple accounts that each produce low-scoring credentials in round r can selectively choose which ones to broadcast in order to influence the seed for round r+1. Indeed, the user can pre-compute their credentials for round r+1 for each potential seed, and broadcast only the credential (among those with low enough score to be leader) that produces the most favorable seed. We consider an adversary who wishes to maximize the expected fraction of rounds in which an account they own is the leader. We show such an adversary always benefits from deviating from the intended protocol, regardless of the fraction of the stake controlled. We characterize the optimal strategy; first by proving the existence of optimal positive recurrent strategies whenever the adversary owns last than 3-5/2 ~38% of the stake. Then, we provide a Markov Decision Process formulation to compute the optimal strategy.",
                "paper_link": "https://www.semanticscholar.org/paper/5b4f037f46e8bbc77be85879e70b0ca94345a958"
            },
            {
                "title": "Catherine Yu. Optimal strategic mining against cryptographic self-selection in proof-of-stake",
                "abstract": "Cryptographic Self-Selection is a subroutine used to select a leader for modern proof-of-stake consensus protocols. In cryptographic self-selection, each round r has a seed Qr. In round r, each account owner is asked to digitally sign Qr, hash their digital signature to produce a credential, and then broadcast this credential to the entire network. A publicly-known function scores each credential in a manner so that the distribution of the lowest scoring credential is identical to the distribution of stake owned by each account. The user who broadcasts the lowest-scoring credential is the leader for round r, and their credential becomes the seed Qr+1. Such protocols leave open the possibility of manipulation: a user who owns multiple accounts that each produce low-scoring credentials in round r can selectively choose which ones to broadcast in order to influence the seed for round r+1. Indeed, the user can pre-compute their credentials for round r+1 for each potential seed, and broadcast only the credential (among those with low enough score to be leader) that produces the most favorable seed. We consider an adversary who wishes to maximize the expected fraction of rounds in which an account they own is the leader. We show such an adversary always benefits from deviating from the intended protocol, regardless of the fraction of the stake controlled. We characterize the optimal strategy; first by proving the existence of optimal positive recurrent strategies whenever the adversary owns last than 3-5/2 ~38% of the stake. Then, we provide a Markov Decision Process formulation to compute the optimal strategy.",
                "paper_link": "https://www.semanticscholar.org/paper/5b4f037f46e8bbc77be85879e70b0ca94345a958"
            },
            {
                "title": "I See You! Robust Measurement of Adversarial Behavior",
                "abstract": "We introduce the study of non-manipulable measures of manipulative behavior in multi-agent systems. We do this through a case study of decentralized finance (DeFi) and blockchain systems, which are salient as real-world, rapidly emerging multi-agent systems with financial incentives for malicious behavior, for the participation in algorithmic and AI systems, and for the need for new methods with which to measure levels of manipulative behavior. We introduce a new surveillance metric for measuring malicious behavior and demonstrate its effectiveness in a natural experiment to the Uniswap DeFi ecosystem. Find code and data here",
                "paper_link": "https://www.semanticscholar.org/paper/c681f92aa77897ab8f524eebe9bcdaa4aaddc411"
            },
            {
                "title": "How to force mechanisms to commit",
                "abstract": "We consider the mechanism design problem of a single item auction with multiple bidders where the auctioneer is sequentially rational and unable to commit to the rules of the auction. Bidders do not share any information and can only communicate privately with the auctioneer, who is free to manipulate the auction as long as deviations are undetectable by bidders. In this setting, strategy-proof direct revelation mechanisms are not credible since the auctioneer can easily mischaracterize the history of the game to obtain higher revenue. We introduce the Deferred Revelation Auction (DRA), a two-round mechanism that is strategy-proof not only for bidders but also for the auctioneer under distributional and computational hardness assumptions circumventing a known impossibility result due to Akbarpour and Li [1].",
                "paper_link": "https://www.semanticscholar.org/paper/b4b597f2f7fa2daac15f53649a06fcd65938391d"
            },
            {
                "title": "PRISM-guardian: A Enhancing Data Privacy in Devices with Sound Collection, Recognition, and Sharing through Blockchain Technology",
                "abstract": "The proliferation of voice-activated devices, such as virtual assistants and voice-controlled systems, has changed how people interact with technology and the environment. These devices collect data that can be sent to servers to process sound, returning responses or suggestions to the user. However, the widespread use of these devices has led to intensive data collection, exposing sensitive information, such as conversations and intimate audio. In this context, we developed PRISM-guardian, a technique for sharing and tracking sound data without revealing its origin, thus preserving privacy. Transparently, audio generators, such as residential users, can track who accessed their information and why. We collected 1000 audio samples, each lasting 10 s, to recognize short-duration cough and sneeze sounds. We achieved average sound recognition processing times of 3.78 s, 6.78 ms to encapsulate the data in the API, and an average of 48 ms to save the data on the blockchain. Besides, we present a mathematical formalization of PRISM and conduct tests to identify the origin of the sound. The results showed that the identity of the sound source is preserved while this source can view and track the data.",
                "paper_link": "https://www.semanticscholar.org/paper/5c5ca105a8e56daf338ca6f61b3dd112d02c4cbc"
            },
            {
                "title": "Computing Optimal Manipulations in Cryptographic Self-Selection Proof-of-Stake Protocols",
                "abstract": "Cryptographic Self-Selection is a paradigm employed by modern Proof-of-Stake consensus protocols to select a block-proposing\"leader.\"Algorand [Chen and Micali, 2019] proposes a canonical protocol, and Ferreira et al. [2022] establish bounds $f(\\alpha,\\beta)$ on the maximum fraction of rounds a strategic player can lead as a function of their stake $\\alpha$ and a network connectivity parameter $\\beta$. While both their lower and upper bounds are non-trivial, there is a substantial gap between them (for example, they establish $f(10\\%,1) \\in [10.08\\%, 21.12\\%]$), leaving open the question of how significant of a concern these manipulations are. We develop computational methods to provably nail $f(\\alpha,\\beta)$ for any desired $(\\alpha,\\beta)$ up to arbitrary precision, and implement our method on a wide range of parameters (for example, we confirm $f(10\\%,1) \\in [10.08\\%, 10.15\\%]$). Methodologically, estimating $f(\\alpha,\\beta)$ can be phrased as estimating to high precision the value of a Markov Decision Process whose states are countably-long lists of real numbers. Our methodological contributions involve (a) reformulating the question instead as computing to high precision the expected value of a distribution that is a fixed-point of a non-linear sampling operator, and (b) provably bounding the error induced by various truncations and sampling estimations of this distribution (which appears intractable to solve in closed form). One technical challenge, for example, is that natural sampling-based estimates of the mean of our target distribution are \\emph{not} unbiased estimators, and therefore our methods necessarily go beyond claiming sufficiently-many samples to be close to the mean.",
                "paper_link": "https://www.semanticscholar.org/paper/9d61cc06831322bcd66470fe689fac329d7cc046"
            },
            {
                "title": "Credible, Optimal Auctions via Public Broadcast",
                "abstract": "We study auction design in a setting where agents can communicate over a censorship-resistant broadcast channel like the ones we can implement over a public blockchain. We seek to design credible, strategyproof auctions in a model that differs from the traditional mechanism design framework because communication is not centralized via the auctioneer. We prove this allows us to design a larger class of credible auctions where the auctioneer has no incentive to be strategic. Intuitively, a decentralized communication model weakens the auctioneer's adversarial capabilities because they can only inject messages into the communication channel but not delete, delay, or modify the messages from legitimate buyers. Our main result is a separation in the following sense: we give the first instance of an auction that is credible only if communication is decentralized. Moreover, we construct the first two-round auction that is credible, strategyproof, and optimal when bidder valuations are $\\alpha$-strongly regular, for $\\alpha>0$. Our result relies on mild assumptions -- namely, the existence of a broadcast channel and cryptographic commitments.",
                "paper_link": "https://www.semanticscholar.org/paper/73c09981a2143e1611f89da550dde88148a11f57"
            },
            {
                "title": "Credibility and Incentives in Gradual Dutch Auctions",
                "abstract": "Gradual dutch auctions (GDAs) are a class of auctions that have been proposed when an auctioneer would like to sell a batch of illiquid items. They function by making available a fraction of the items for sale at every time, and starting a new auction whose price decays as time passes. This has the effect of allowing a seller to cater to buyers who may not want to purchase the entire batch of items at a single time. We analyze the incentives of participating in GDAs. First, we consider the seller\u2019s incentives in running a GDA. We show that the seller can deviate from truthfully running a GDA via an attack in which she initially buys a fraction of the supply available in each dutch auction, forcing buyers to fill their demand with later (more expensive) auctions. This attack is a form of multi-block maximal extractable value (MEV), extending previous work on lending protocols and decentralized exchanges. Next, we consider buyers\u2019 incentives in participating in a GDA, and consider an interdependent values setting in which the history of the auction is allowed to affect buyers\u2019 values in the future. We show conditions in which GDAs are ex post incentive compatible and individually rational for buyers.",
                "paper_link": "https://www.semanticscholar.org/paper/d16c0b922c2d2dc989d693d3ebfdbbcd8d7a6a6a"
            },
            {
                "title": "Dynamic Posted-Price Mechanisms for the Blockchain Transaction Fee Market (Invited Talk)",
                "abstract": "In recent years, prominent blockchain systems such as Bitcoin and Ethereum have experienced explosive growth in transaction volume, leading to frequent surges in demand for limited block space and causing transaction fees to fluctuate by orders of magnitude. The status quo auctions sell space using a first-price auction [27]; however, users find it difficult to estimate how much they need to bid in order to get their transactions accepted onto the chain. If they bid too low, their transactions can have long confirmation times. If they bid too high, they pay larger fees than necessary. In light of these issues, new transaction fee mechanisms have been proposed, most notably EIP-1559 [4], aiming to provide better usability. EIP-1559 is a history-dependent mechanism that relies on block utilization to adjust a base fee. We propose an alternative design - a dynamic posted-price mechanism - which uses not only block utilization but also observable bids from past blocks to compute a posted-price for subsequent blocks. We show its potential to reduce price volatility by providing examples for which the prices of EIP-1559 are unstable while the prices of the proposed mechanism are stable. More generally, whenever the demand for the blockchain stabilizes, we ask if our mechanism is able to converge to a stable state. Our main result provides sufficient conditions in a probabilistic setting for which the proposed mechanism is approximately welfare optimal and the prices are stable. Our main technical contribution towards establishing stability is an iterative algorithm that, given oracle access to a Lipschitz continuous and strictly concave function f, converges to a fixed point of f.",
                "paper_link": "https://www.semanticscholar.org/paper/95677e1dafc1e4253e2297682176a360cc68d790"
            },
            {
                "title": "Economics and Computation in Decentralized Systems",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/47e65cfdcb4e3ea9d5bfc666b40b8b1879848ff8"
            },
            {
                "title": "Report-Analytical Methods in TCS Constructive Discrepancy Minimization for Convex Sets",
                "abstract": "This report relates the main results and methods of Constructive Discrepancy Minimization for Convex Sets [6] by Thomas Rothvo\u00df, a beautiful and largely self-contained paper in algorithmic discrepancy theory. This paper ties together or extends many previous results, including those of Gluskin [4], Giannopoulos [3], Bansal [1], and Lovett and Meka [5]. The primary motivation and application for this work is in solving the discrepancy minimization problem optimally, up to a constant factor. The problem is as follows: Given sets S1, . . . , Sn \u2286 [n], find a discrepancy function \u03c7 : [n]\u2192 {\u22121, 1} that minimizes the discrepancy, maxj\u2208[n]|\u03c7(Sj)|",
                "paper_link": "https://www.semanticscholar.org/paper/9c110d81a9341425fb8f671f85e60e33fd071d91"
            },
            {
                "title": "To Mitigate Insecure IoT Devices, Regulate Manufacturers or Consumers?",
                "abstract": "Recent years have seen the emergence and proliferation of Internet-of-Things (IoT) devices. However, many of them are found to contain security vulnerabilities that adversaries could exploit remotely to launch high-profile attacks [1]. For example, the Mirai botnet compromised a few hundred thousand Internet-connected cameras, before it launched a distributed denial-of-service (DDoS) attack on a DNS provider used by Twitter and Reddit, causing widespread disruption on these major platforms [2]. As the total number of IoT devices is projected to reach a billion in the next five to ten years [3], attacks from insecure devices are likely to cause higher damages.",
                "paper_link": "https://www.semanticscholar.org/paper/22289338def75570e51f337f87dcc2aa8de594d8"
            },
            {
                "title": "Dolphin: Dataplane Load-balancing in Programmable Hybrid Networks",
                "abstract": "Programmability allows networks to adopt flexible policies without requiring hardware changes every time a new policy is deployed. However, the first step to transition to such a network today is to first replace all existing devices with programmable ones. This is often a major hurdle for enterprises to upgrade due to the high upfront operational costs and inherent risks involved. In this paper, we explore how to deploy programmable networks incrementally, such that networks can still reap the benefits of programmability by upgrading only a small fraction of their switches. Our work targets the use case of loadbalancing at dataplane time scales. Existing schemes, such as HULA [9] and CONGA [3], require devices in the network to be homogeneous and capable of functionality not available on legacy switches. We propose two schemes to approximate the same behavior on hybrid networks where only a fraction of the switches are programmable.",
                "paper_link": "https://www.semanticscholar.org/paper/720f7773cb0287df2c7468336fca8b7428b8c826"
            }
        ]
    },
    {
        "Professor": "Ferdinando Fioretto",
        "Papers": [
            {
                "title": "Learning to Optimize meets Neural-ODE: Real-Time, Stability-Constrained AC OPF",
                "abstract": "Recent developments in applying machine learning to address Alternating Current Optimal Power Flow (AC OPF) problems have demonstrated significant potential in providing close to optimal solutions for generator dispatch in near real-time. While these learning to optimize methods have demonstrated remarkable performance on steady-state operations, practical applications often demand compliance with dynamic constraints when used for fast-timescale optimization. This paper addresses this gap and develops a real-time stability-constrained OPF model (DynOPF-Net) that simultaneously addresses both optimality and dynamical stability within learning-assisted grid operations. The model is a unique integration of learning to optimize that learns a mapping from load conditions to OPF solutions, capturing the OPF's physical and engineering constraints, with Neural Ordinary Differential Equations, capturing generator dynamics, enabling the inclusion of a subset of stability constraints. Numerical results on the WSCC 9-bus and IEEE 57-bus benchmark systems demonstrate that DynOPF-Net can produce highly accurate AC-OPF solutions while also ensuring system stability, contrasting the unstable results obtained by state-of-the-art LtO methods.",
                "paper_link": "https://www.semanticscholar.org/paper/dc71e67d4408b563123346acd6c7790aabb223a9"
            },
            {
                "title": "End-to-End Optimization and Learning of Fair Court Schedules",
                "abstract": "Criminal courts across the United States handle millions of cases every year, and the scheduling of those cases must accommodate a diverse set of constraints, including the preferences and availability of courts, prosecutors, and defense teams. When criminal court schedules are formed, defendants' scheduling preferences often take the least priority, although defendants may face significant consequences (including arrest or detention) for missed court dates. Additionally, studies indicate that defendants' nonappearances impose costs on the courts and other system stakeholders. To address these issues, courts and commentators have begun to recognize that pretrial outcomes for defendants and for the system would be improved with greater attention to court processes, including \\emph{court scheduling practices}. There is thus a need for fair criminal court pretrial scheduling systems that account for defendants' preferences and availability, but the collection of such data poses logistical challenges. Furthermore, optimizing schedules fairly across various parties' preferences is a complex optimization problem, even when such data is available. In an effort to construct such a fair scheduling system under data uncertainty, this paper proposes a joint optimization and learning framework that combines machine learning models trained end-to-end with efficient matching algorithms. This framework aims to produce court scheduling schedules that optimize a principled measure of fairness, balancing the availability and preferences of all parties.",
                "paper_link": "https://www.semanticscholar.org/paper/80c2cd27958b47d3bcff45474dc60725230fef60"
            },
            {
                "title": "Learning To Solve Differential Equation Constrained Optimization Problems",
                "abstract": "Differential equations (DE) constrained optimization plays a critical role in numerous scientific and engineering fields, including energy systems, aerospace engineering, ecology, and finance, where optimal configurations or control strategies must be determined for systems governed by ordinary or stochastic differential equations. Despite its significance, the computational challenges associated with these problems have limited their practical use. To address these limitations, this paper introduces a learning-based approach to DE-constrained optimization that combines techniques from proxy optimization and neural differential equations. The proposed approach uses a dual-network architecture, with one approximating the control strategies, focusing on steady-state constraints, and another solving the associated DEs. This combination enables the approximation of optimal strategies while accounting for dynamic constraints in near real-time. Experiments across problems in energy optimization and finance modeling show that this method provides full compliance with dynamic constraints and it produces results up to 25 times more precise than other methods which do not explicitly model the system's dynamic equations.",
                "paper_link": "https://www.semanticscholar.org/paper/8c3c2b08c4a54688132cba3ca9cbab43cd2390b9"
            },
            {
                "title": "Learning Joint Models of Prediction and Optimization",
                "abstract": "The Predict-Then-Optimize framework uses machine learning models to predict unknown parameters of an optimization problem from exogenous features before solving. This setting is common to many real-world decision processes, and recently it has been shown that decision quality can be substantially improved by solving and differentiating the optimization problem within an end-to-end training loop. However, this approach requires significant computational effort in addition to handcrafted, problem-specific rules for backpropagation through the optimization step, challenging its applicability to a broad class of optimization problems. This paper proposes an alternative method, in which optimal solutions are learned directly from the observable features by joint predictive models. The approach is generic, and based on an adaptation of the Learning-to-Optimize paradigm, from which a rich variety of existing techniques can be employed. Experimental evaluations show the ability of several Learning-to-Optimize methods to provide efficient and accurate solutions to an array of challenging Predict-Then-Optimize problems.",
                "paper_link": "https://www.semanticscholar.org/paper/51009856ba14bc75c9ff38d86e6f104d3492e12f"
            },
            {
                "title": "Decision-focused learning: Foundations, state of the art, benchmark and future opportunities",
                "abstract": "Decision-focused learning (DFL) is an emerging paradigm that integrates machine learning (ML) and constrained optimization to enhance decision quality by training ML models in an end-to-end system. This approach shows significant potential to revolutionize combinatorial decision-making in real-world applications that operate under uncertainty, where estimating unknown parameters within decision models is a major challenge. This paper presents a comprehensive review of DFL, providing an in-depth analysis of both gradient-based and gradient-free techniques used to combine ML and constrained optimization. It evaluates the strengths and limitations of these techniques and includes an extensive empirical evaluation of eleven methods across seven problems. The survey also offers insights into recent advancements and future research directions in DFL.",
                "paper_link": "https://www.semanticscholar.org/paper/70a958f7123b3a15f19bdd98fd0bbcf622c9b30c"
            },
            {
                "title": "Fairness Issues and Mitigations in (Differentially Private) Socio-demographic Data Processes",
                "abstract": "Statistical agencies rely on sampling techniques to collect socio-demographic data crucial for policy-making and resource allocation. This paper shows that surveys of important societal relevance introduce sampling errors that unevenly impact group-level estimates, thereby compromising fairness in downstream decisions. To address these issues, this paper introduces an optimization approach modeled on real-world survey design processes, ensuring sampling costs are optimized while maintaining error margins within prescribed tolerances. Additionally, privacy-preserving methods used to determine sampling rates can further impact these fairness issues. The paper explores the impact of differential privacy on the statistics informing the sampling process, revealing a surprising effect: not only the expected negative effect from the addition of noise for differential privacy is negligible, but also this privacy noise can in fact reduce unfairness as it positively biases smaller counts. These findings are validated over an extensive analysis using datasets commonly applied in census statistics.",
                "paper_link": "https://www.semanticscholar.org/paper/3388ce2d12087eb8050a7883d6e0e44ac987c622"
            },
            {
                "title": "Speculative diffusion decoding: Accelerating language generation through diffusion",
                "abstract": "Speculative decoding has emerged as a widely adopted method to accelerate large language model inference without sacrificing the quality of the model outputs. While this technique has facilitated notable speed improvements by enabling parallel sequence verification, its efficiency remains inherently limited by the reliance on incremental token generation in existing draft models. To overcome this limitation, this paper proposes an adaptation of speculative decoding which uses discrete diffusion models to generate draft sequences. This allows parallelization of both the drafting and verification steps, providing significant speed-ups to the inference process. Our proposed approach, Speculative Diffusion Decoding (SpecDiff), is validated on standard language generation benchmarks and empirically demonstrated to provide a up to 8.7x speed-up over standard generation processes and up to 2.5x speed-up over existing speculative decoding approaches.",
                "paper_link": "https://www.semanticscholar.org/paper/0adf7d0d104f59189ee082442d595c4eaa094904"
            },
            {
                "title": "Differentially Private Data Release on Graphs: Inefficiencies and Unfairness",
                "abstract": "Networks are crucial components of many sectors, including telecommunications, healthcare, finance, energy, and transportation.The information carried in such networks often contains sensitive user data, like location data for commuters and packet data for online users. Therefore, when considering data release for networks, one must ensure that data release mechanisms do not leak information about individuals, quantified in a precise mathematical sense. Differential Privacy (DP) is the widely accepted, formal, state-of-the-art technique, which has found use in a variety of real-life settings including the 2020 U.S. Census, Apple users' device data, or Google's location data. Yet, the use of DP comes with new challenges, as the noise added for privacy introduces inaccuracies or biases and further, DP techniques can also distribute these biases disproportionately across different populations, inducing fairness issues. The goal of this paper is to characterize the impact of DP on bias and unfairness in the context of releasing information about networks, taking a departure from previous work which has studied these effects in the context of private population counts release (such as in the U.S. Census). To this end, we consider a network release problem where the network structure is known to all, but the weights on edges must be released privately. We consider the impact of this private release on a simple downstream decision-making task run by a third-party, which is to find the shortest path between any two pairs of nodes and recommend the best route to users. This setting is of highly practical relevance, mirroring scenarios in transportation networks, where preserving privacy while providing accurate routing information is crucial. Our work provides theoretical foundations and empirical evidence into the bias and unfairness arising due to privacy in these networked decision problems.",
                "paper_link": "https://www.semanticscholar.org/paper/8c514750c4a2b57158964e036fc28acb2c1200c1"
            },
            {
                "title": "The Data Minimization Principle in Machine Learning",
                "abstract": "The principle of data minimization aims to reduce the amount of data collected, processed or retained to minimize the potential for misuse, unauthorized access, or data breaches. Rooted in privacy-by-design principles, data minimization has been endorsed by various global data protection regulations. However, its practical implementation remains a challenge due to the lack of a rigorous formulation. This paper addresses this gap and introduces an optimization framework for data minimization based on its legal definitions. It then adapts several optimization algorithms to perform data minimization and conducts a comprehensive evaluation in terms of their compliance with minimization objectives as well as their impact on user privacy. Our analysis underscores the mismatch between the privacy expectations of data minimization and the actual privacy benefits, emphasizing the need for approaches that account for multiple facets of real-world privacy risks.",
                "paper_link": "https://www.semanticscholar.org/paper/7065a2b503865ec978cebe83fb0ab8142411ad7a"
            },
            {
                "title": "Low-rank finetuning for LLMs: A fairness perspective",
                "abstract": "Low-rank approximation techniques have become the de facto standard for fine-tuning Large Language Models (LLMs) due to their reduced computational and memory requirements. This paper investigates the effectiveness of these methods in capturing the shift of fine-tuning datasets from the initial pre-trained data distribution. Our findings reveal that there are cases in which low-rank fine-tuning falls short in learning such shifts. This, in turn, produces non-negligible side effects, especially when fine-tuning is adopted for toxicity mitigation in pre-trained models, or in scenarios where it is important to provide fair models. Through comprehensive empirical evidence on several models, datasets, and tasks, we show that low-rank fine-tuning inadvertently preserves undesirable biases and toxic behaviors. We also show that this extends to sequential decision-making tasks, emphasizing the need for careful evaluation to promote responsible LLMs development.",
                "paper_link": "https://www.semanticscholar.org/paper/5626d531fbb4afc3eb1cf47910c85031f7369138"
            },
            {
                "title": "Metric learning to accelerate convergence of operator splitting methods for differentiable parametric programming",
                "abstract": "Recent work has shown a variety of ways in which machine learning can be used to accelerate the solution of constrained optimization problems. Increasing demand for real-time decision-making capabilities in applications such as artificial intelligence and optimal control has led to a variety of approaches, based on distinct strategies. This work proposes a novel approach to learning optimization, in which the underlying metric space of a proximal operator splitting algorithm is learned so as to maximize its convergence rate. While prior works in optimization theory have derived optimal metrics for limited classes of problems, the results do not extend to many practical problem forms including general Quadratic Programming (QP). This paper shows how differentiable optimization can enable the end-to-end learning of proximal metrics, enhancing the convergence of proximal algorithms for QP problems beyond what is possible based on known theory. Additionally, the results illustrate a strong connection between the learned proximal metrics and active constraints at the optima, leading to an interpretation in which the learning of proximal metrics can be viewed as a form of active set learning.",
                "paper_link": "https://www.semanticscholar.org/paper/ef9e9b5a25df49d547c7d8883144219f5ec46cd7"
            },
            {
                "title": "Learning Constrained Optimization with Deep Augmented Lagrangian Methods",
                "abstract": "Learning to Optimize (LtO) is a problem setting in which a machine learning (ML) model is trained to emulate a constrained optimization solver. Learning to produce optimal and feasible solutions subject to complex constraints is a difficult task, but is often made possible by restricting the input space to a limited distribution of related problems. Most LtO methods focus on directly learning solutions to the primal problem, and applying correction schemes or loss function penalties to encourage feasibility. This paper proposes an alternative approach, in which the ML model is trained instead to predict dual solution estimates directly, from which primal estimates are constructed to form dual-feasible solution pairs. This enables an end-to-end training scheme is which the dual objective is maximized as a loss function, and solution estimates iterate toward primal feasibility, emulating a Dual Ascent method. First it is shown that the poor convergence properties of classical Dual Ascent are reflected in poor convergence of the proposed training scheme. Then, by incorporating techniques from practical Augmented Lagrangian methods, we show how the training scheme can be improved to learn highly accurate constrained optimization solvers, for both convex and nonconvex problems.",
                "paper_link": "https://www.semanticscholar.org/paper/e774dd532db5c25dfd94c7d516ac367ce045faaf"
            },
            {
                "title": "End-to-End Learning for Fair Multiobjective Optimization Under Uncertainty",
                "abstract": "Many decision processes in artificial intelligence and operations research are modeled by parametric optimization problems whose defining parameters are unknown and must be inferred from observable data. The Predict-Then-Optimize (PtO) paradigm in machine learning aims to maximize downstream decision quality by training the parametric inference model end-to-end with the subsequent constrained optimization. This requires backpropagation through the optimization problem using approximation techniques specific to the problem's form, especially for nondifferentiable linear and mixed-integer programs. This paper extends the PtO methodology to optimization problems with nondifferentiable Ordered Weighted Averaging (OWA) objectives, known for their ability to ensure properties of fairness and robustness in decision models. Through a collection of training techniques and proposed application settings, it shows how optimization of OWA functions can be effectively integrated with parametric prediction for fair and robust optimization under uncertainty.",
                "paper_link": "https://www.semanticscholar.org/paper/609562bb77838fbb888d5c89f437eff6cdbbb3b5"
            },
            {
                "title": "Learning Fair Ranking Policies via Differentiable Optimization of Ordered Weighted Averages",
                "abstract": "Learning to Rank (LTR) is one of the most widely used machine learning applications. It is a key component in platforms with profound societal impacts, including job search, healthcare information retrieval, and social media content feeds. Conventional LTR models have been shown to produce biases results, stimulating a discourse on how to address the disparities introduced by ranking systems that solely prioritize user relevance. However, while several models of fair learning to rank have been proposed, they suffer from deficiencies either in accuracy or efficiency, thus limiting their applicability to real-world ranking platforms. This paper shows how efficiently-solvable fair ranking models, based on the optimization of Ordered Weighted Average (OWA) functions, can be integrated into the training loop of an LTR model to achieve favorable balances between fairness, user utility, and runtime efficiency. In particular, this paper is the first to show how to backpropagate through constrained optimizations of OWA objectives, enabling their use in integrated prediction and decision models.",
                "paper_link": "https://www.semanticscholar.org/paper/76a1a4466f32347a49be5a6df0b70a3af7263940"
            },
            {
                "title": "Disparate Impact on Group Accuracy of Linearization for Private Inference",
                "abstract": "Ensuring privacy-preserving inference on cryptographically secure data is a well-known computational challenge. To alleviate the bottleneck of costly cryptographic computations in non-linear activations, recent methods have suggested linearizing a targeted portion of these activations in neural networks. This technique results in significantly reduced runtimes with often negligible impacts on accuracy. In this paper, we demonstrate that such computational benefits may lead to increased fairness costs. Specifically, we find that reducing the number of ReLU activations disproportionately decreases the accuracy for minority groups compared to majority groups. To explain these observations, we provide a mathematical interpretation under restricted assumptions about the nature of the decision boundary, while also showing the prevalence of this problem across widely used datasets and architectures. Finally, we show how a simple procedure altering the fine-tuning step for linearized models can serve as an effective mitigation strategy.",
                "paper_link": "https://www.semanticscholar.org/paper/18a226b794e89d3cf8d93734724b42275c9c4c0d"
            },
            {
                "title": "On The Fairness Impacts of Hardware Selection in Machine Learning",
                "abstract": "In the machine learning ecosystem, hardware selection is often regarded as a mere utility, overshadowed by the spotlight on algorithms and data. This oversight is particularly problematic in contexts like ML-as-a-service platforms, where users often lack control over the hardware used for model deployment. How does the choice of hardware impact generalization properties? This paper investigates the influence of hardware on the delicate balance between model performance and fairness. We demonstrate that hardware choices can exacerbate existing disparities, attributing these discrepancies to variations in gradient flows and loss surfaces across different demographic groups. Through both theoretical and empirical analysis, the paper not only identifies the underlying factors but also proposes an effective strategy for mitigating hardware-induced performance imbalances.",
                "paper_link": "https://www.semanticscholar.org/paper/f3d096d0e299fd79d0f4364992dc849741123c80"
            },
            {
                "title": "Constrained Synthesis with Projected Diffusion Models",
                "abstract": "This paper introduces an approach to endow generative diffusion processes the ability to satisfy and certify compliance with constraints and physical principles. The proposed method recast the traditional sampling process of generative diffusion models as a constrained optimization problem, steering the generated data distribution to remain within a specified region to ensure adherence to the given constraints. These capabilities are validated on applications featuring both convex and challenging, non-convex, constraints as well as ordinary differential equations, in domains spanning from synthesizing new materials with precise morphometric properties, generating physics-informed motion, optimizing paths in planning scenarios, and human motion synthesis.",
                "paper_link": "https://www.semanticscholar.org/paper/f3a52f0e5be52d06a1bf1851fe2d0ad8dc5ef35e"
            },
            {
                "title": "Finding\u2208 and \u03b4 of Statistical Disclosure Control Systems",
                "abstract": "This paper analyzes the privacy of traditional Statistical Dis-closure Control (SDC) systems under a differential privacy interpretation. SDCs, such as cell suppression and swapping, promise to safeguard the confidentiality of data and are routinely adopted in data analyses with profound societal and economic impacts. Through a formal analysis and empirical evaluation on demographic data from real household in the U.S., the paper shows that widely adopted SDC systems not only induce vastly larger privacy losses than classical differential privacy mechanisms, but, they may also come at a cost of larger accuracy and fairness.",
                "paper_link": "https://www.semanticscholar.org/paper/5327527f9fc36a086030729fae246da8f9597296"
            },
            {
                "title": "Data Minimization at Inference Time",
                "abstract": "In domains with high stakes such as law, recruitment, and healthcare, learning models frequently rely on sensitive user data for inference, necessitating the complete set of features. This not only poses significant privacy risks for individuals but also demands substantial human effort from organizations to verify information accuracy. This paper asks whether it is necessary to use \\emph{all} input features for accurate predictions at inference time. The paper demonstrates that, in a personalized setting, individuals may only need to disclose a small subset of their features without compromising decision-making accuracy. The paper also provides an efficient sequential algorithm to determine the appropriate attributes for each individual to provide. Evaluations across various learning tasks show that individuals can potentially report as little as 10\\% of their information while maintaining the same accuracy level as a model that employs the full set of user information.",
                "paper_link": "https://www.semanticscholar.org/paper/d4b2137a38c7b50bd81acf53dcd30f7173ed9996"
            },
            {
                "title": "Fairness Increases Adversarial Vulnerability",
                "abstract": "The remarkable performance of deep learning models and their applications in consequential domains (e.g., facial recognition) introduces important challenges at the intersection of equity and security. Fairness and robustness are two desired notions often required in learning models. Fairness ensures that models do not disproportionately harm (or benefit) some groups over others, while robustness measures the models' resilience against small input perturbations. This paper shows the existence of a dichotomy between fairness and robustness, and analyzes when achieving fairness decreases the model robustness to adversarial samples. The reported analysis sheds light on the factors causing such contrasting behavior, suggesting that distance to the decision boundary across groups as a key explainer for this behavior. Extensive experiments on non-linear models and different architectures validate the theoretical findings in multiple vision domains. Finally, the paper proposes a simple, yet effective, solution to construct models achieving good tradeoffs between fairness and robustness.",
                "paper_link": "https://www.semanticscholar.org/paper/939f6d5d17ec2b07602782858c47111473da8835"
            }
        ]
    },
    {
        "Professor": "Tom Fletcher",
        "Papers": [
            {
                "title": "Principal geodesic analysis for the study of nonlinear statistics of shape",
                "abstract": "A primary goal of statistical shape analysis is to describe the variability of a population of geometric objects. A standard technique for computing such descriptions is principal component analysis. However, principal component analysis is limited in that it only works for data lying in a Euclidean vector space. While this is certainly sufficient for geometric models that are parameterized by a set of landmarks or a dense collection of boundary points, it does not handle more complex representations of shape. We have been developing representations of geometry based on the medial axis description or m-rep. While the medial representation provides a rich language for variability in terms of bending, twisting, and widening, the medial parameters are not elements of a Euclidean vector space. They are in fact elements of a nonlinear Riemannian symmetric space. In this paper, we develop the method of principal geodesic analysis, a generalization of principal component analysis to the manifold setting. We demonstrate its use in describing the variability of medially-defined anatomical objects. Results of applying this framework on a population of hippocampi in a schizophrenia study are presented.",
                "paper_link": "https://www.semanticscholar.org/paper/11647c4d6867daaa90e67850f36d2cd8e614acdc"
            },
            {
                "title": "DeformableM-Repsfor 3D Medical Image Segmentation",
                "abstract": "Fully Convolutional Neural Networks (FCNNs) with contracting and expanding paths have shown prominence for the majority of medical image segmentation applications since the past decade. In FCNNs, the encoder plays an integral role by learning both global and local features and contextual representations which can be utilized for semantic output prediction by the decoder. Despite their success, the locality of convolutional layers in FCNNs, limits the capability of learning long-range spatial dependencies. Inspired by the recent success of transformers for Natural Language Processing (NLP) in long-range sequence learning, we reformulate the task of volumetric (3D) medical image segmentation as a sequence-to-sequence prediction problem. We introduce a novel architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information, while also following the successful \"U-shaped\" network design for the encoder and decoder. The transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the final semantic segmentation output. We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation tasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV leaderboard.",
                "paper_link": "https://www.semanticscholar.org/paper/7519a1e9e7371df79bd8a21cee871feb0ec597a5"
            },
            {
                "title": "Functional connectivity magnetic resonance imaging classification of autism",
                "abstract": "Group differences in resting state functional magnetic resonance imaging connectivity between individuals with autism and typically developing controls have been widely replicated for a small number of discrete brain regions, yet the whole-brain distribution of connectivity abnormalities in autism is not well characterized. It is also unclear whether functional connectivity is sufficiently robust to be used as a diagnostic or prognostic metric in individual patients with autism. We obtained pairwise functional connectivity measurements from a lattice of 7266 regions of interest covering the entire grey matter (26.4 million connections) in a well-characterized set of 40 male adolescents and young adults with autism and 40 age-, sex- and IQ-matched typically developing subjects. A single resting state blood oxygen level-dependent scan of 8 min was used for the classification in each subject. A leave-one-out classifier successfully distinguished autism from control subjects with 83% sensitivity and 75% specificity for a total accuracy of 79% (P = 1.1 \u00d7 10(-7)). In subjects <20 years of age, the classifier performed at 89% accuracy (P = 5.4 \u00d7 10(-7)). In a replication dataset consisting of 21 individuals from six families with both affected and unaffected siblings, the classifier performed at 71% accuracy (91% accuracy for subjects <20 years of age). Classification scores in subjects with autism were significantly correlated with the Social Responsiveness Scale (P = 0.05), verbal IQ (P = 0.02) and the Autism Diagnostic Observation Schedule-Generic's combined social and communication subscores (P = 0.05). An analysis of informative connections demonstrated that region of interest pairs with strongest correlation values were most abnormal in autism. Negatively correlated region of interest pairs showed higher correlation in autism (less anticorrelation), possibly representing weaker inhibitory connections, particularly for long connections (Euclidean distance >10 cm). Brain regions showing greatest differences included regions of the default mode network, superior parietal lobule, fusiform gyrus and anterior insula. Overall, classification accuracy was better for younger subjects, with differences between autism and control subjects diminishing after 19 years of age. Classification scores of unaffected siblings of individuals with autism were more similar to those of the control subjects than to those of the subjects with autism. These findings indicate feasibility of a functional connectivity magnetic resonance imaging diagnostic assay for autism.",
                "paper_link": "https://www.semanticscholar.org/paper/f4dfb03dea61cbb2e1db07472d7547abc1b6ec86"
            },
            {
                "title": "Riemannian geometry for the statistical analysis of diffusion tensor data",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/22f281ac734b5cf032a091818c395c5ed7109900"
            },
            {
                "title": "Longitudinal changes in cortical thickness in autism and typical development",
                "abstract": "The natural history of brain growth in autism spectrum disorders remains unclear. Cross-sectional studies have identified regional abnormalities in brain volume and cortical thickness in autism, although substantial discrepancies have been reported. Preliminary longitudinal studies using two time points and small samples have identified specific regional differences in cortical thickness in the disorder. To clarify age-related trajectories of cortical development, we examined longitudinal changes in cortical thickness within a large mixed cross-sectional and longitudinal sample of autistic subjects and age- and gender-matched typically developing controls. Three hundred and forty-five magnetic resonance imaging scans were examined from 97 males with autism (mean age = 16.8 years; range 3-36 years) and 60 males with typical development (mean age = 18 years; range 4-39 years), with an average interscan interval of 2.6 years. FreeSurfer image analysis software was used to parcellate the cortex into 34 regions of interest per hemisphere and to calculate mean cortical thickness for each region. Longitudinal linear mixed effects models were used to further characterize these findings and identify regions with between-group differences in longitudinal age-related trajectories. Using mean age at time of first scan as a reference (15 years), differences were observed in bilateral inferior frontal gyrus, pars opercularis and pars triangularis, right caudal middle frontal and left rostral middle frontal regions, and left frontal pole. However, group differences in cortical thickness varied by developmental stage, and were influenced by IQ. Differences in age-related trajectories emerged in bilateral parietal and occipital regions (postcentral gyrus, cuneus, lingual gyrus, pericalcarine cortex), left frontal regions (pars opercularis, rostral middle frontal and frontal pole), left supramarginal gyrus, and right transverse temporal gyrus, superior parietal lobule, and paracentral, lateral orbitofrontal, and lateral occipital regions. We suggest that abnormal cortical development in autism spectrum disorders undergoes three distinct phases: accelerated expansion in early childhood, accelerated thinning in later childhood and adolescence, and decelerated thinning in early adulthood. Moreover, cortical thickness abnormalities in autism spectrum disorders are region-specific, vary with age, and may remain dynamic well into adulthood.",
                "paper_link": "https://www.semanticscholar.org/paper/9f196b929bb8d0367dcfdd0cc1a298edf1461d7d"
            },
            {
                "title": "Multisite functional connectivity MRI classification of autism: ABIDE results",
                "abstract": "Background: Systematic differences in functional connectivity MRI metrics have been consistently observed in autism, with predominantly decreased cortico-cortical connectivity. Previous attempts at single subject classification in high-functioning autism using whole brain point-to-point functional connectivity have yielded about 80% accurate classification of autism vs. control subjects across a wide age range. We attempted to replicate the method and results using the Autism Brain Imaging Data Exchange (ABIDE) including resting state fMRI data obtained from 964 subjects and 16 separate international sites. Methods: For each of 964 subjects, we obtained pairwise functional connectivity measurements from a lattice of 7266 regions of interest covering the gray matter (26.4 million \u201cconnections\u201d) after preprocessing that included motion and slice timing correction, coregistration to an anatomic image, normalization to standard space, and voxelwise removal by regression of motion parameters, soft tissue, CSF, and white matter signals. Connections were grouped into multiple bins, and a leave-one-out classifier was evaluated on connections comprising each set of bins. Age, age-squared, gender, handedness, and site were included as covariates for the classifier. Results: Classification accuracy significantly outperformed chance but was much lower for multisite prediction than for previous single site results. As high as 60% accuracy was obtained for whole brain classification, with the best accuracy from connections involving regions of the default mode network, parahippocampaland fusiform gyri, insula, Wernicke Area, and intraparietal sulcus. The classifier score was related to symptom severity, social function, daily living skills, and verbal IQ. Classification accuracy was significantly higher for sites with longer BOLD imaging times. Conclusions: Multisite functional connectivity classification of autism outperformed chance using a simple leave-one-out classifier, but exhibited poorer accuracy than for single site results. Attempts to use multisite classifiers will likely require improved classification algorithms, longer BOLD imaging times, and standardized acquisition parameters for possible future clinical utility.",
                "paper_link": "https://www.semanticscholar.org/paper/371b52114394c7ab5f767de1495a009484a3d6de"
            },
            {
                "title": "Principal geodesic analysis on symmetric spaces: Statistics of diffusion tensors",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/c53a49522e963e15a582408633d4c6d90ac083c5"
            },
            {
                "title": "Population shape regression from random design data",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/163088df67df32b51aedded945195666db2ce8d0"
            },
            {
                "title": "Geodesic regression and the theory of least squares on Riemannian manifolds",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/82ee1271f59cc376d69a22251e55c2c61803f805"
            },
            {
                "title": "The geometric median on Riemannian manifolds with application to robust atlas estimation",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/d972a17d33a7a64def027685fca01c43059991e5"
            },
            {
                "title": "Rician noise removal in diffusion tensor MRI",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/12af09aaa2e8897f30c693f777919294e162c61c"
            },
            {
                "title": "Statistics of shape via principal geodesic analysis on Lie groups",
                "abstract": "Principal component analysis has proven to be useful for understanding geometric variability in populations of parameterized objects. The statistical framework is well understood when the parameters of the objects are elements of a Euclidean vector space. This is certainly the case when the objects are described via landmarks or as a dense collection of boundary points. We have been developing representations of geometry based on the medial axis description or m-rep. Although this description has proven to be effective, the medial parameters are not naturally elements of a Euclidean space. In this paper we show that medial descriptions are in fact elements of a Lie group. We develop methodology based on Lie groups for the statistical analysis of medially-defined anatomical objects.",
                "paper_link": "https://www.semanticscholar.org/paper/bac7a2af3d39395a6e9e5a12edfdcf96d4bb82c7"
            },
            {
                "title": "Longitudinal volumetric brain changes in autism spectrum disorder ages 6\u201335 years",
                "abstract": "Since the impairments associated with autism spectrum disorder (ASD) tend to persist or worsen from childhood into adulthood, it is of critical importance to examine how the brain develops over this growth epoch. We report initial findings on whole and regional longitudinal brain development in 100 male participants with ASD (226 high\u2010quality magnetic resonance imaging [MRI] scans; mean inter\u2010scan interval 2.7 years) compared to 56 typically developing controls (TDCs) (117 high\u2010quality scans; mean inter\u2010scan interval 2.6 years) from childhood into adulthood, for a total of 156 participants scanned over an 8\u2010year period. This initial analysis includes between one and three high\u2010quality scans per participant that have been processed and segmented to date, with 21% having one scan, 27% with two scans, and 52% with three scans in the ASD sample; corresponding percentages for the TDC sample are 30%, 30%, and 40%. The proportion of participants with multiple scans (79% of ASDs and 68% of TDCs) was high in comparison to that of large longitudinal neuroimaging studies of typical development. We provide volumetric growth curves for the entire brain, total gray matter (GM), frontal GM, temporal GM, parietal GM, occipital GM, total cortical white matter (WM), corpus callosum, caudate, thalamus, total cerebellum, and total ventricles. Mean volume of cortical WM was reduced significantly. Mean ventricular volume was increased in the ASD sample relative to the TDCs across the broad age range studied. Decreases in regional mean volumes in the ASD sample most often were due to decreases during late adolescence and adulthood. The growth curve of whole brain volume over time showed increased volumes in young children with autism, and subsequently decreased during adolescence to meet the TDC curve between 10 and 15 years of age. The volume of many structures continued to decline atypically into adulthood in the ASD sample. The data suggest that ASD is a dynamic disorder with complex changes in whole and regional brain volumes that change over time from childhood into adulthood. Autism Res 2015, 8: 82\u201393. \u00a9 2014 International Society for Autism Research, Wiley Periodicals, Inc.",
                "paper_link": "https://www.semanticscholar.org/paper/463cb687318eb70c8d6a535b74848e9ccd94e00d"
            },
            {
                "title": "Microstructural connectivity of the arcuate fasciculus in adolescents with high-functioning autism",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/12a876877b3e106ae11db54c0a13eb581b3c45e7"
            },
            {
                "title": "Semi-supervised learning with GANs: Manifold invariance with improved inference",
                "abstract": "Semi-supervised learning methods using Generative Adversarial Networks (GANs) have shown promising empirical success recently. Most of these methods use a shared discriminator/classifier which discriminates real examples from fake while also predicting the class label. Motivated by the ability of the GANs generator to capture the data manifold well, we propose to estimate the tangent space to the data manifold using GANs and employ it to inject invariances into the classifier. In the process, we propose enhancements over existing methods for learning the inverse mapping (i.e., the encoder) which greatly improves in terms of semantic similarity of the reconstructed sample with the input sample. We observe considerable empirical gains in semi-supervised learning over baselines, particularly in the cases when the number of labeled examples is low. We also provide insights into how fake examples influence the semi-supervised learning procedure.",
                "paper_link": "https://www.semanticscholar.org/paper/1c035dea4d61b51fd6352932b628fecd3c009023"
            },
            {
                "title": "Group analysis of DTI fiber tract statistics with application to neurodevelopment",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/f52b2729c869deffe19e6c09b6ec7fc349e414f4"
            },
            {
                "title": "Multiscale deformable model segmentation and statistical shape analysis using medial descriptions",
                "abstract": "This paper presents a multiscale framework based on a medial representation for the segmentation and shape characterization of anatomical objects in medical imagery. The segmentation procedure is based on a Bayesian deformable templates methodology in which the prior information about the geometry and shape of anatomical objects is incorporated via the construction of exemplary templates. The anatomical variability is accommodated in the Bayesian framework by defining probabilistic transformations on these templates. The transformations, thus, defined are parameterized directly in terms of natural shape operations, such as growth and bending, and their locations. A preliminary validation study of the segmentation procedure is presented. We also present a novel statistical shape analysis approach based on the medial descriptions that examines shape via separate intuitive categories, such as global variability at the coarse scale and localized variability at the fine scale. We show that the method can be used to statistically describe shape variability in intuitive terms such as growing and bending.",
                "paper_link": "https://www.semanticscholar.org/paper/3418627fbdc0eb3a92ebe5c168c167882e67e8ed"
            },
            {
                "title": "Shape modeling and analysis with entropy-based particle systems",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/dca09388490a37e2bb684365475fb35e0323f9af"
            },
            {
                "title": "Fiber tract-oriented statistics for quantitative diffusion tensor MRI analysis",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/0469093ffb6af960378ee12fe5b18f2d8ba0f183"
            },
            {
                "title": "The Riemannian geometry of deep generative models",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            }
        ]
    },
    {
        "Professor": "Geoffrey C. Fox",
        "Papers": [
            {
                "title": "Solving problems on concurrent processors: vol. 2",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/70c23cadb086e7a718c459c4d11d622a150619af"
            },
            {
                "title": "Overview of the book: grid computing\u2013making the global infrastructure a reality",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/a27ee801650e372a832539c31654622cade24373"
            },
            {
                "title": "Observables for the analysis of event shapes in e+ e\u2212 annihilation and other processes",
                "abstract": "We present a set of rotationally invariant observables which characterizes the ''shapes'' of events, and is calculable in quantum-chromodynamics perturbation theory for final states consisting of quarks and gluons (G). We include the effects of fragmentation to hadrons in comparing the shapes of events from the processes e/sup +/e/sup -/ ..-->.. qq-bar, e/sup +/e/sup -/ ..-->.. qq-barG, and e/sup +/e/sup -/ ..-->.. heavy resonance ..-->.. GGG, and from heavy-quark and lepton production. We indicate how our analysis may be extended to deep-inelastic lepton-hadron interactions and hadron-hadron collisions involving large transverse momenta.",
                "paper_link": "https://www.semanticscholar.org/paper/5cddcda379f48f7005c54aa9d24a026df1e69c9a"
            },
            {
                "title": "Twister: a runtime for iterative mapreduce",
                "abstract": "MapReduce programming model has simplified the implementation of many data parallel applications. The simplicity of the programming model and the quality of services provided by many implementations of MapReduce attract a lot of enthusiasm among distributed computing communities. From the years of experience in applying MapReduce to various scientific applications we identified a set of extensions to the programming model and improvements to its architecture that will expand the applicability of MapReduce to more classes of applications. In this paper, we present the programming model and the architecture of Twister an enhanced MapReduce runtime that supports iterative MapReduce computations efficiently. We also show performance comparisons of Twister with other similar runtimes such as Hadoop and DryadLINQ for large scale data parallel applications.",
                "paper_link": "https://www.semanticscholar.org/paper/09a180d9d410d8e551f42401d6453d57406b6d29"
            },
            {
                "title": "Examining the challenges of scientific workflows",
                "abstract": "Workflows have emerged as a paradigm for representing and managing complex distributed computations and are used to accelerate the pace of scientific progress. A recent National Science Foundation workshop brought together domain, computer, and social scientists to discuss requirements of future scientific applications and the challenges they present to current workflow technologies.",
                "paper_link": "https://www.semanticscholar.org/paper/e45d4aedd10229cbbeef8b2ec009f87ae1a4065e"
            },
            {
                "title": "Fortran D language specification",
                "abstract": "This paper presents Fortran D, a version of Fortran enhanced with data decomposition spec-iications. It is designed to support two fundamental stages of writing a data-parallel program: problem mapping using sophisticated array alignments, and machine mapping through a rich set of data distribution functions. We believe that Fortran D provides a simple machine-independent programming model for most numerical computations. We intend to evaluate its usefulness for both programmers and advanced compilers on a variety of parallel architectures.",
                "paper_link": "https://www.semanticscholar.org/paper/423424b5d8f517d2ecc90de18a82d0529e985066"
            },
            {
                "title": "Sourcebook of parallel computing",
                "abstract": "I. Parallelism 1. Introduction 2. Parallel Computer Architectures 3. Parallel Programming Considerations II. Applications 4. General Application Issues 5. Parallel Computing in CFD 6. Parallel Computing in Environment and Energy 7. Parallel Computational Chemistry 8. Application Overviews III. Software technologies 9. Software Technologies 10. Message Passing and Threads 11. Parallel I/O 12. Languages and Compilers 13. Parallel Object-Oriented Libraries 14. Problem-Solving Environments 15. Tools for Performance Tuning and Debugging 16. The 2-D Poisson Problem IV. Enabling Technologies and Algorithms 17. Reusable Software and Algorithms 18. Graph Partitioning for Scientific Simulations 19. Mesh Generation 20. Templates and Numerical Linear Algebra 21. Software for the Scalable Solutions of PDEs 22. Parallel Continuous Optimization 23. Path Following in Scientific Computing 24. Automatic Differentiation V. Conclusion 25. Wrap-up and Features",
                "paper_link": "https://www.semanticscholar.org/paper/cacc2e74839ac55029f3aeb4db32a52b30069cdb"
            },
            {
                "title": "Distributed and Cloud Computing: Clusters, Grids, Clouds and The Internet of Things",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "The Perfect Club benchmarks: Effective performance evaluation of supercomputers",
                "abstract": "This report presents a methodology for measuring the performance of supercomputers. It includes 13 Fortran programs that total over 50,000 lines of source code. They represent applications in several areas of engi neering and scientific computing, and in many cases the codes are currently being used by computational re search and development groups. We also present the PERFECT Fortran standard, a set of guidelines that allow portability to several types of machines. Furthermore, we present some performance measures and a method ology for recording and sharing results among diverse users on different machines. The results presented in this paper should not be used to compare machines, except in a preliminary sense. Rather, they are presented to show how the methodology has been applied, and to encourage others to join us in this effort. The results should be regarded as the first step toward our objec tive, which is to develop a publicly accessible data base of performance information of this type.",
                "paper_link": "https://www.semanticscholar.org/paper/4dcb177e99948060aedbfaed076c4623cae89b38"
            },
            {
                "title": "Statistical mechanics and phase transitions in clustering",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/3da017fac82eb805c751c3653e323a47f8ee5eb4"
            },
            {
                "title": "Event shapes in e+ e\u2212 annihilation",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/6ad434eb11aa4be5de8a9c25d5ce83fae8964c14"
            },
            {
                "title": "Mapreduce for data intensive scientific analyses",
                "abstract": "Most scientific data analyses comprise analyzing voluminous data collected from various instruments. Efficient parallel/concurrent algorithms and frameworks are the key to meeting the scalability and performance requirements entailed in such scientific data analyses. The recently introduced MapReduce technique has gained a lot of attention from the scientific community for its applicability in large parallel data analyses. Although there are many evaluations of the MapReduce technique using large textual data collections, there have been only a few evaluations for scientific data analyses. The goals of this paper are twofold. First, we present our experience in applying the MapReduce technique for two scientific data analyses: (i) high energy physics data analyses; (ii) K-means clustering. Second, we present CGL-MapReduce, a streaming-based MapReduce implementation and compare its performance with Hadoop.",
                "paper_link": "https://www.semanticscholar.org/paper/cc7f20f2ad4db10ea58facc10c265236a2811755"
            },
            {
                "title": "A deterministic annealing approach to clustering",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/a7984546a154eae54375fcda9bc9bcde08c096d0"
            },
            {
                "title": "Quantum-chromodynamic approach for the large-transverse-momentum production of particles and jets",
                "abstract": "It is shown that if, in a calculation of high-transverse-momentum (p\u22a5) meson production in hadron-hadron collisions, one includes not only the scale-breaking effects that might be expected from asymptotically free theories but also the effects due to the transverse momentum of quarks in hadrons and further adds contributions from quark-gluon and gluon-gluon scattering to those of quark-quark scattering then the results are not inconsistent with the data. The approach yields the correct magnitude and an apparent approximate 1/p\u22a58 behavior in accord with single-particle data for the energy range currently observed. Two-particle correlations are examined. Because of scale-breaking effects and the presence of gluons, the theory does not have the problem of predicting too many away-side hadrons at large p\u22a5 as did an earlier quark-quark scattering \"black-box\" approach. We conclude that the quantum-chromodynamics approach is in reasonable accord with the data although theoretical uncertainties (especially at low p\u22a5) make incontrovertible conclusions impossible at present. Crucial tests of the theory require higher p\u22a5 than are now available; estimates for this region are made.",
                "paper_link": "https://www.semanticscholar.org/paper/7b4d3eb5b39887d02101acc88266554326de7ad8"
            },
            {
                "title": "Parallel computing works!",
                "abstract": "Parallel Computing Works! by G.C. Fox, R.D. Williams, and P.C. Messina 977 pp. $69.95 Morgan Kaufmann San Francisco 1994 ISBN 1-55860-253-4",
                "paper_link": "https://www.semanticscholar.org/paper/443f7f227467287ed4f6625de0e9d48dc714f897"
            },
            {
                "title": "A model for parton showers in QCD",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/78967b86855fd5ef3cee12e2605e52c954a9a78b"
            },
            {
                "title": "Matrix algorithms on a hypercube I: Matrix multiplication",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/f185fd93fdda5244280805f7d5de2c2a68cce826"
            },
            {
                "title": "The semantic grid: A future e\u2010science infrastructure",
                "abstract": "e-Science offers a promising vision of how computer and communication technology can support and enhance the scientific process. It does this by enabling scientists to generate, analyse, share and discuss their insights, experiments and results in an effective manner. The underlying computer infrastructure that provides these facilities is commonly referred to as the Grid. At this time, there are a number of grid applications being developed and there is a whole raft of computer technologies that provide fragments of the necessary functionality. However there is currently a major gap between these endeavours and the vision of e-Science in which there is a high degree of easy-to-use and seamless automation and in which there are flexible collaborations and computations on a global scale. To bridge this practice\u2013aspiration divide, this paper presents a research agenda whose aim is to move from the current state of the art in e-Science infrastructure, to the future infrastructure that is needed to support the full richness of the e-Science vision. Here the future e-Science research infrastructure is termed the Semantic Grid (Semantic Grid to Grid is meant to connote a similar relationship to the one that exists between the Semantic Web and the Web). In particular, we present a conceptual architecture for the Semantic Grid. This architecture adopts a service-oriented perspective in which distinct stakeholders in the scientific process, represented as software agents, provide services to one another, under various service level agreements, in various forms of marketplace. We then focus predominantly on the issues concerned with the way that knowledge is acquired and used in such environments since we believe this is the key differentiator between current grid endeavours and those envisioned for the Semantic Grid.",
                "paper_link": "https://www.semanticscholar.org/paper/73d34cabcb6049d68b206b84fce55fdcfb5251f4"
            },
            {
                "title": "Vector quantization by deterministic annealing",
                "abstract": "A deterministic annealing approach is suggested to search for the optimal vector quantizer given a set of training data. The problem is reformulated within a probabilistic framework. No prior knowledge is assumed on the source density, and the principle of maximum entropy is used to obtain the association probabilities at a given average distortion. The corresponding Lagrange multiplier is inversely related to the 'temperature' and is used to control the annealing process. In this process, as the temperature is lowered, the system undergoes a sequence of phase transitions when existing clusters split naturally, without use of heuristics. The resulting codebook is independent of the codebook used to initialize the iterations. >",
                "paper_link": "https://www.semanticscholar.org/paper/2ee14dd35886c44c87d66f8490528fa58c19fc25"
            },
            {
                "title": "Solving Problems in Concurrent Processors-Volume 1",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/2d0fb8d6f68552ed616978e4f107fc9174bdbfe2"
            }
        ]
    },
    {
        "Professor": "Daniel G. Graham",
        "Papers": []
    },
    {
        "Professor": "Wajih Ul Hassan",
        "Papers": [
            {
                "title": "NoDoze: Combatting Threat Alert Fatigue with Automated Provenance Triage",
                "abstract": "\u2014Large enterprises are increasingly relying on threat detection softwares (e.g., Intrusion Detection Systems) to allow them to spot suspicious activities. These softwares generate alerts which must be investigated by cyber analysts to \ufb01gure out if they are true attacks. Unfortunately, in practice, there are more alerts than cyber analysts can properly investigate. This leads to a \u201cthreat alert fatigue\u201d or information overload problem where cyber analysts miss true attack alerts in the noise of false alarms. In this paper, we present N O D OZE to combat this challenge using contextual and historical information of generated threat alert. N O D OZE \ufb01rst generates a causal dependency graph of an alert event. Then, it assigns an anomaly score to each edge in the dependency graph based on the frequency with which related events have happened before in the enterprise. N O D OZE then propagates those scores along the neighboring edges of the graph using a novel network diffusion algorithm and generates an aggregate anomaly score which is used for triaging. We deployed and evaluated N O D OZE at NEC Labs America. Evaluation on our dataset of 364 threat alerts shows that N O D OZE consistently ranked the true alerts higher than the false alerts based on aggregate anomaly scores. Further, through the introduction of a cutoff threshold for anomaly scores, we estimate that our system decreases the volume of false alarms by 84%, saving analysts\u2019 more than 90 hours of investigation time per week. N O D OZE generates alert dependency graphs that are two orders of magnitude smaller than those generated by traditional tools without sacri\ufb01cing the vital information needed for the investigation. Our system has a low average runtime overhead and can be deployed with any threat detection software.",
                "paper_link": "https://www.semanticscholar.org/paper/dfe5a726609c318baf509af7cf41bf3840884c0b"
            },
            {
                "title": "Fear and logging in the internet of things",
                "abstract": "\u2014As the Internet of Things (IoT) continues to proliferate, diagnosing incorrect behavior within increasingly-automated homes becomes considerably more dif\ufb01cult. Devices and apps may be chained together in long sequences of trigger-action rules to the point that from an observable symptom (e.g., an unlocked door) it may be impossible to identify the distantly removed root cause (e.g., a malicious app). This is because, at present, IoT audit logs are siloed on individual devices, and hence cannot be used to reconstruct the causal relationships of complex work\ufb02ows. In this work, we present ProvThings, a platform-centric approach to centralized auditing in the Internet of Things. ProvThings performs ef\ufb01cient automated instrumentation of IoT apps and device APIs in order to generate data provenance that provides a holistic explanation of system activities, including malicious behaviors. We prototype ProvThings for the Samsung SmartThings platform, and benchmark the ef\ufb01cacy of our approach against a corpus of 26 IoT attacks. Through the introduction of a selective code instrumentation optimization, we demonstrate in evaluation that ProvThings imposes just 5% overhead on physical IoT devices while enabling real time querying of system behaviors, and further consider how ProvThings can be leveraged to meet the needs of a variety of stakeholders in the IoT ecosystem.",
                "paper_link": "https://www.semanticscholar.org/paper/5152f79d0c80186690947de4704a51b22852d3c9"
            },
            {
                "title": "Tactical Provenance Analysis for Endpoint Detection and Response Systems",
                "abstract": "Endpoint Detection and Response (EDR) tools provide visibility into sophisticated intrusions by matching system events against known adversarial behaviors. However, current solutions suffer from three challenges: 1) EDR tools generate a high volume of false alarms, creating backlogs of investigation tasks for analysts; 2) determining the veracity of these threat alerts requires tedious manual labor due to the overwhelming amount of low-level system logs, creating a \"needle-in-a-haystack\" problem; and 3) due to the tremendous resource burden of log retention, in practice the system logs describing long-lived attack campaigns are often deleted before an investigation is ever initiated.This paper describes an effort to bring the benefits of data provenance to commercial EDR tools. We introduce the notion of Tactical Provenance Graphs (TPGs) that, rather than encoding low-level system event dependencies, reason about causal dependencies between EDR-generated threat alerts. TPGs provide compact visualization of multi-stage attacks to analysts, accelerating investigation. To address EDR\u2019s false alarm problem, we introduce a threat scoring methodology that assesses risk based on the temporal ordering between individual threat alerts present in the TPG. In contrast to the retention of unwieldy system logs, we maintain a minimally-sufficient skeleton graph that can provide linkability between existing and future threat alerts. We evaluate our system, RapSheet, using the Symantec EDR tool in an enterprise environment. Results show that our approach can rank truly malicious TPGs higher than false alarm TPGs. Moreover, our skeleton graph reduces the long-term burden of log retention by up to 87%.",
                "paper_link": "https://www.semanticscholar.org/paper/670f07252b9936b3a8b3d3b3c5ad3602c3550210"
            },
            {
                "title": "You Are What You Do: Hunting Stealthy Malware via Data Provenance Analysis.",
                "abstract": "\u2014To subvert recent advances in perimeter and host security, the attacker community has developed and employed various attack vectors to make a malware much stealthier than before to penetrate the target system and prolong its presence. Such advanced malware or \u201cstealthy malware\u201d makes use of various techniques to impersonate or abuse benign applications and legitimate system tools to minimize its footprints in the target system. It is thus dif\ufb01cult for traditional detection tools, such as malware scanners, to detect it, as the malware normally does not expose its malicious payload in a \ufb01le and hides its malicious behaviors among the benign behaviors of the processes. In this paper, we present P ROV D ETECTOR , a provenance-based approach for detecting stealthy malware. Our insight behind the P ROV D ETECTOR approach is that although a stealthy malware attempts to blend into benign processes, its malicious behaviors inevitably interact with the underlying operating system (OS), which will be exposed to and captured by provenance monitoring. Based on this intuition, P ROV D ETECTOR \ufb01rst employs a novel selection algorithm to identify possibly malicious parts in the OS-level provenance data of a process. It then applies a neural embedding and machine learning pipeline to automatically detect any behavior that deviates signi\ufb01cantly from normal behaviors. We evaluate our approach on a large provenance dataset from an enterprise network and demonstrate that it achieves very high detection performance of stealthy malware (an average F1 score of 0.974). Further, we conduct thorough interpretability studies to understand the internals of the learned machine learning models",
                "paper_link": "https://www.semanticscholar.org/paper/1b12fbe7d9f7cd251a40e099e66e65f635b873ff"
            },
            {
                "title": "Towards Scalable Cluster Auditing through Grammatical Inference over Provenance Graphs",
                "abstract": "Investigating the nature of system intrusions in large distributed systems remains a notoriously difficult challenge. While monitoring tools (e.g., Firewalls, IDS) provide preliminary alerts through easy-to-use administrative interfaces, attack reconstruction still requires that administrators sift through gigabytes of system audit logs stored locally on hundreds of machines. At present, two fundamental obstacles prevent synergy between system-layer auditing and modern cluster monitoring tools: 1) the sheer volume of audit data generated in a data center is prohibitively costly to transmit to a central node, and 2) systemlayer auditing poses a \u201cneedle-in-a-haystack\u201d problem, such that hundreds of employee hours may be required to diagnose a single intrusion. This paper presents Winnower, a scalable system for auditbased cluster monitoring that addresses these challenges. Our key insight is that, for tasks that are replicated across nodes in a distributed application, a model can be defined over audit logs to succinctly summarize the behavior of many nodes, thus eliminating the need to transmit redundant audit records to a central monitoring node. Specifically, Winnower parses audit records into provenance graphs that describe the actions of individual nodes, then performs grammatical inference over individual graphs using a novel adaptation of Deterministic Finite Automata (DFA) Learning to produce a behavioral model of many nodes at once. This provenance model can be efficiently transmitted to a central node and used to identify anomalous events in the cluster. We implement Winnower for Docker Swarm container clusters and evaluate our system against real-world applications and attacks. We show that Winnower dramatically reduces storage and network overhead associated with aggregating system audit logs, by as much as 98%, without sacrificing the important information needed for attack investigation. Winnower thus represents a significant step forward for security monitoring in distributed systems.",
                "paper_link": "https://www.semanticscholar.org/paper/348191516284b7ed3c21a4172976576c31c1d5cc"
            },
            {
                "title": "OmegaLog: High-fidelity attack investigation via transparent multi-layer log analysis",
                "abstract": "\u2014Recent advances in causality analysis have en- abled investigators to trace multi-stage attacks using provenance graphs. Based on system-layer audit logs (e.g., syscalls), these approaches omit vital sources of application context (e.g., email addresses, HTTP response codes) that can be found in higher layers of the system. Although such information is often essential to understanding attack behaviors, it is dif\ufb01cult to incorporate this evidence into causal analysis engines because of the semantic gap that exists between system layers. To address that short- coming, we propose the notion of universal provenance , which encodes all forensically relevant causal dependencies regardless of their layer of origin. To transparently realize that vision on commodity systems, we present OmegaLog, a provenance tracker that bridges the semantic gap between system and application logging contexts. OmegaLog analyzes program binaries to identify and model application-layer logging behaviors, enabling accurate reconciliation of application events with system-layer accesses. OmegaLog then intercepts applications\u2019 runtime logging activities and grafts those events onto the system-layer provenance graph, allowing investigators to reason more precisely about the nature of attacks. We demonstrate that our system is widely applicable to existing software projects and can transparently facilitate execution partitioning of provenance graphs without any training or developer intervention. Evaluation on real-world attack scenarios shows that our technique generates concise provenance graphs with rich semantic information relative to the state-of-the-art, with an average runtime overhead of 4%.",
                "paper_link": "https://www.semanticscholar.org/paper/36eb598c9cb9975dd4626f241822919f5fc67dfe"
            },
            {
                "title": "Custos: Practical tamper-evident auditing of operating systems using trusted execution",
                "abstract": "\u2014System auditing is a central concern when investigating and responding to security incidents. Unfortunately, attackers regularly engage in anti-forensic activities after a break-in, covering their tracks from the system logs in order to frustrate the efforts of investigators. While a variety of tamper-evident logging solutions have appeared throughout the industry and the literature, these techniques do not meet the operational and scalability requirements of system-layer audit frameworks. In this work, we introduce C USTOS , a practical framework for the detection of tampering in system logs. C USTOS consists of a tamper-evident logging layer and a decentralized auditing protocol. The former enables the veri\ufb01cation of log integrity with minimal changes to the underlying logging framework, while the latter enables near real-time detection of log integrity violations within an enterprise-class network. C USTOS is made practical by the observation that we can decouple the costs of cryptographic log commitments from the act of creating and storing log events, without trading off security, leveraging features of off-the-shelf trusted execution environments. Supporting over one million events per second, we show that C USTOS \u2019 tamper-evident logging protocol is three orders of magnitude (1000 \u00d7 ) faster than prior solutions and incurs only between 2% and 7% runtime overhead over insecure logging on intensive workloads. Further, we show that C USTOS \u2019 auditing protocol can detect violations in near real-time even in the presence of a powerful distributed adversary and with minimal (3%) network overhead. Our case study on a real-world APT attack scenario demonstrates that C USTOS forces anti-forensic attackers into a \u201close-lose\u201d situation, where they can either be covert and not tamper with logs (which can be used for forensics), or erase logs but then",
                "paper_link": "https://www.semanticscholar.org/paper/34ab3f31f842a6f06996da8fb7ca83d563e95812"
            },
            {
                "title": "How good are the specs? A study of the bug-finding effectiveness of existing Java API specifications",
                "abstract": "Runtime verification can be used to find bugs early, during software development, by monitoring test executions against formal specifications (specs). The quality of runtime verification depends on the quality of the specs. While previous research has produced many specs for the Java API, manually or through automatic mining, there has been no large-scale study of their bug-finding effectiveness. We present the first in-depth study of the bug-finding effectiveness of previously proposed specs. We used JavaMOP to monitor 182 manually written and 17 automatically mined specs against more than 18K manually written and 2.1M automatically generated tests in 200 open-source projects. The average runtime overhead was under 4.3x. We inspected 652 violations of manually written specs and (randomly sampled) 200 violations of automatically mined specs. We reported 95 bugs, out of which developers already fixed 74. However, most violations, 82.81% of 652 and 97.89% of 200, were false alarms. Our empirical results show that (1) runtime verification technology has matured enough to incur tolerable runtime overhead during testing, and (2) the existing API specifications can find many bugs that developers are willing to fix; however, (3) the false alarm rates are worrisome and suggest that substantial effort needs to be spent on engineering better specs and properly evaluating their effectiveness.",
                "paper_link": "https://www.semanticscholar.org/paper/daa6dc9d5dd90b4daab94efb78b2bf2a8ac38fe0"
            },
            {
                "title": "Transparent Web Service Auditing via Network Provenance Functions",
                "abstract": "Detecting and explaining the nature of attacks in distributed web services is often difficult -- determining the nature of suspicious activity requires following the trail of an attacker through a chain of heterogeneous software components including load balancers, proxies, worker nodes, and storage services. Unfortunately, existing forensic solutions cannot provide the necessary context to link events across complex workflows, particularly in instances where application layer semantics (e.g., SQL queries, RPCs) are needed to understand the attack. In this work, we present a transparent provenance-based approach for auditing web services through the introduction of Network Provenance Functions (NPFs). NPFs are a distributed architecture for capturing detailed data provenance for web service components, leveraging the key insight that mediation of an application's protocols can be used to infer its activities without requiring invasive instrumentation or developer cooperation. We design and implement NPF with consideration for the complexity of modern cloud-based web services, and evaluate our architecture against a variety of applications including DVDStore, RUBiS, and WikiBench to show that our system imposes as little as 9.3% average end-to-end overhead on connections for realistic workloads. Finally, we consider several scenarios in which our system can be used to concisely explain attacks. NPF thus enables the hassle-free deployment of semantically rich provenance-based auditing for complex applications workflows in the Cloud.",
                "paper_link": "https://www.semanticscholar.org/paper/3e4edfedc60dbd1fc7f13267fbba6c9189cb2afd"
            },
            {
                "title": "Analysis of Privacy Protections in Fitness Tracking Social Networks -or- You can run, but can you hide?",
                "abstract": "Mobile \ufb01tness tracking apps allow users to track their workouts and share them with friends through online social networks. Although the sharing of personal data is an inherent risk in all social networks, the dangers presented by sharing personal workouts comprised of geospatial and health data may prove especially grave. While \ufb01tness apps offer a variety of privacy features, at present it is unclear if these countermeasures are suf\ufb01-cient to thwart a determined attacker, nor is it clear how many of these services\u2019 users are at risk. In this work, we perform a systematic analysis of privacy behaviors and threats in \ufb01tness tracking social networks. Collecting a month-long snapshot of public posts of a popular \ufb01tness tracking service (21 million posts, 3 million users), we observe that 16.5% of users make use of Endpoint Privacy Zones (EPZs), which conceal \ufb01tness activity near user-designated sensitive locations (e.g., home, of\ufb01ce). We go on to develop an attack against EPZs that infers users\u2019 protected locations from the remaining available information in public posts, discovering that 95.1% of moderately active users are at risk of having their protected locations extracted by an attacker. Finally, we consider the ef\ufb01cacy of state-of-the-art privacy mechanisms through adapting geo-indistinguishability techniques as well as developing a novel EPZ fuzzing technique. The affected companies have been noti\ufb01ed of the discovered vulnerabilities and at the time of publication have incorporated our proposed countermeasures into their production systems.",
                "paper_link": "https://www.semanticscholar.org/paper/a8423a5040ae69c69ff8d4e32f3e406e65fe93b1"
            },
            {
                "title": "Sok: History is a vast early warning system: Auditing the provenance of system intrusions",
                "abstract": "Auditing, a central pillar of operating system security, has only recently come into its own as an active area of public research. This resurgent interest is due in large part to the notion of data provenance, a technique that iteratively parses audit log entries into a dependency graph that explains the history of system execution. Provenance facilitates precise threat detection and investigation through causal analysis of sophisticated intrusion behaviors. However, the absence of a foundational audit literature, combined with the rapid publication of recent findings, makes it difficult to gain a holistic picture of advancements and open challenges in the area.In this work, we survey and categorize the provenance-based system auditing literature, distilling contributions into a layered taxonomy based on the audit log capture and analysis pipeline. Recognizing that the Reduction Layer remains a key obstacle to the further proliferation of causal analysis technologies, we delve further on this issue by conducting an ambitious independent evaluation of 8 exemplar reduction techniques against the recently-released DARPA Transparent Computing datasets. Our experiments uncover that past approaches frequently prune an overlapping set of activities from audit logs, reducing the synergistic benefits from applying them in tandem; further, we observe an inverse relation between storage efficiency and anomaly detection performance. However, we also observe that log reduction techniques are able to synergize effectively with data compression, potentially reducing log retention costs by multiple orders of magnitude. We conclude by discussing promising future directions for the field.",
                "paper_link": "https://www.semanticscholar.org/paper/7598956efd3cdda714509f02aa1907984525c481"
            },
            {
                "title": "On the forensic validity of approximated audit logs",
                "abstract": "Auditing is an increasingly essential tool for the defense of computing systems, but the unwieldy nature of log data imposes significant burdens on administrators and analysts. To address this issue, a variety of techniques have been proposed for approximating the contents of raw audit logs, facilitating efficient storage and analysis. However, the security value of these approximated logs is difficult to measure\u2014relative to the original log, it is unclear if these techniques retain the forensic evidence needed to effectively investigate threats. Unfortunately, prior work has only investigated this issue anecdotally, demonstrating sufficient evidence is retained for specific attack scenarios. In this work, we address this gap in the literature through formalizing metrics for quantifying the forensic validity of an approximated audit log under differing threat models. In addition to providing quantifiable security arguments for prior work, we also identify a novel point in the approximation design space\u2014that log events describing typical (benign) system activity can be aggressively approximated, while events that encode anomalous behavior should be preserved with lossless fidelity. We instantiate this notion of Attack-Preserving forensic validity in LogApprox, a new approximation technique that eliminates the redundancy of voluminous file I/O associated with benign process activities. We evaluate LogApprox alongside a corpus of exemplar approximation techniques from prior work and demonstrate that LogApprox achieves comparable log reduction rates while retaining 100% of attack-identifying log events. Additionally, we utilize this evaluation to illuminate the inherent trade-off between performance and utility within existing approximation techniques. This work thus establishes trustworthy foundations for the design of the next generation of efficient auditing frameworks.",
                "paper_link": "https://www.semanticscholar.org/paper/acabdd08f7732747b4ab3c85aa92dd42979fbe1a"
            },
            {
                "title": "Don't cry over spilled records: Memory elasticity of data-parallel applications and its application to cluster scheduling",
                "abstract": "Understanding the performance of data-parallel workloads when resource-constrained has significant practical importance but unfortunately has received only limited attention. This paper identifies, quantifies and demonstrates memory elasticity, an intrinsic property of dataparallel tasks. Memory elasticity allows tasks to run with significantly less memory than they would ideally need while only paying a moderate performance penalty. For example, we find that given as little as 10% of ideal memory, PageRank and NutchIndexing Hadoop reducers become only 1.2\u00d7/1.75\u00d7 and 1.08\u00d7 slower. We show that memory elasticity is prevalent in the Hadoop, Spark, Tez and Flink frameworks. We also show that memory elasticity is predictable in nature by building simple models for Hadoop and extending them to Tez and Spark. \n \nTo demonstrate the potential benefits of leveraging memory elasticity, this paper further explores its application to cluster scheduling. In this setting, we observe that the resource vs. time trade-off enabled by memory elasticity becomes a task queuing time vs. task runtime trade-off. Tasks may complete faster when scheduled with less memory because their waiting time is reduced. We show that a scheduler can turn this task-level trade-off into improved job completion time and cluster-wide memory utilization. We have integrated memory elasticity into Apache YARN. We show gains of up to 60% in average job completion time on a 50-node Hadoop cluster. Extensive simulations show similar improvements over a large number of scenarios.",
                "paper_link": "https://www.semanticscholar.org/paper/36cc75ba7c0eb6f6a4f0aa95f258d5a540a7d43c"
            },
            {
                "title": "This is why we can\u2019t cache nice things: Lightning-fast threat hunting using suspicion-based hierarchical storage",
                "abstract": "Recent advances in the causal analysis can accelerate incident response time, but only after a causal graph of the attack has been constructed. Unfortunately, existing causal graph generation techniques are mainly offline and may take hours or days to respond to investigator queries, creating greater opportunity for attackers to hide their attack footprint, gain persistency, and propagate to other machines. To address that limitation, we present Swift, a threat investigation system that provides high-throughput causality tracking and real-time causal graph generation capabilities. We design an in-memory graph database that enables space-efficient graph storage and online causality tracking with minimal disk operations. We propose a hierarchical storage system that keeps forensically-relevant part of the causal graph in main memory while evicting rest to disk. To identify the causal graph that is likely to be relevant during the investigation, we design an asynchronous cache eviction policy that calculates the most suspicious part of the causal graph and caches only that part in the main memory. We evaluated Swift on a real-world enterprise to demonstrate how our system scales to process typical event loads and how it responds to forensic queries when security alerts occur. Results show that Swift is scalable, modular, and answers forensic queries in real-time even when analyzing audit logs containing tens of millions of events.",
                "paper_link": "https://www.semanticscholar.org/paper/5a66f47761a49bf3be7c3c7356821cbea00b4cd4"
            },
            {
                "title": "Can data provenance put an end to the data breach?",
                "abstract": "In September 2017, the world awoke to the news that Equifax, a consumer reporting agency and one of the pillars of the American credit system, fell prey to a data breach that led to the exposure of 147 million individuals' personal information. For Equifax, the coming weeks would include high-profile executive resignations, a steep drop in its stock prices, and an infamously ill-conceived public outreach effort; however, eventually the public's attention turned elsewhere. After all, Equifax was just the latest in a seemingly endless parade of data breach victims that included commercial titans like Target and eBay, political campaigns like Hillary Clinton's, and government agencies like the Office of Personnel Management. Today, the threat of the next data breach looms invisibly over every aspect of society.",
                "paper_link": "https://www.semanticscholar.org/paper/d1176dd05bd405842997fc997601bfd643d2d70b"
            },
            {
                "title": "Validating the integrity of audit logs against execution repartitioning attacks",
                "abstract": "Provenance-based causal analysis of audit logs has proven to be an invaluable method of investigating system intrusions. However, it also suffers from dependency explosion, whereby long-running processes accumulate many dependencies that are hard to unravel. Execution unit partitioning addresses this by segmenting dependencies into units of work, such as isolating the events that processed a single HTTP request. Unfortunately, we discover that current designs have a semantic gap problem due to how system calls and application log messages are used to infer complex internal program states. We demonstrate how attackers can modify existing code exploits to control event partitioning, breaking links in the attack and framing innocent users. We also show how our techniques circumvent existing program and log integrity defenses. We then propose a new design for execution unit partitioning that leverages additional runtime data to yield verified partitions that resist manipulation. Our design overcomes the technical challenges of minimizing additional overhead while accurately connecting low level code instructions to high level audit events, in part with the use of commodity hardware processor tracing. We implement a prototype of our design for Linux, MARSARA, and extensively evaluate it on 14 real-world programs, targeted with expertly crafted exploits. MARSARA's verified partitions successfully capture all the attack provenances while only reintroducing 2.82% of false dependencies, in the worst case, with an average overhead of 8.7%. Using a new metric called Partitioning Attack Surface, we show that MARSARA eliminates 47,642 more repartitioning gadgets per program than integrity defenses like CFI, demonstrating our prototype's effectiveness and the novelty of the attacks it prevents.",
                "paper_link": "https://www.semanticscholar.org/paper/9604d8cf10ba725e6fcf9bb9ba2c9d357ba329f4"
            },
            {
                "title": "Automated provenance analytics: A regular grammar based approach with applications in security",
                "abstract": "Provenance collection techniques have been carefully studied in the literature, and there are now several systems to automatically capture provenance data. However, the analysis of provenance data is often left \u201cas an exercise for the reader\u201d. The provenance community needs tools that allow users to quickly sort through large volumes of provenance data and identify records that require further investigation. By detecting anomalies in provenance data that deviate from established patterns, we hope to actively thwart security threats. In this paper, we discuss issues with current graph analysis techniques as applied to data provenance, particularly Frequent Subgraph Mining (FSM). Then we introduce Directed Acyclic Graph regular grammars (DAGr) as a model for provenance data and show how they can detect anomalies. These DAGr provide an expressive characterization of DAGs, and by using regular grammars as a formalism, we can apply results from formal language theory to learn the difference between \u201cgood\u201d and \u201cbad\u201d provenance. We propose a restricted subclass of DAGr called deterministic Directed Acyclic Graph automata (dDAGa) that guarantees parsing in linear time. Finally, we propose a learning algorithm for dDAGa, inspired by Minimum Description Length for Grammar Induction [1]. \u2217DISTRIBUTION STATEMENT A. Approved for public release: distribution unlimited. This material is based, in part, upon work supported by the Assistant Secretary of Defense for Research and Engineering under Air Force Contract No. FA8721-05-C-0002 and/or FA8702-15-D-0001. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Assistant Secretary of Defense for Research and Engineering. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. TaPP 2017, June 22-23, 2017, Seattle, Washington. Copyright remains with the owner/author(s).",
                "paper_link": "https://www.semanticscholar.org/paper/e848c12bce447af52827af4d8d83e3b824a886e1"
            },
            {
                "title": "FLASH: A Comprehensive Approach to Intrusion Detection via Provenance Graph Representation Learning",
                "abstract": "Recently, provenance-based Intrusion Detection Systems (IDSes) have gained popularity for their potential in detecting sophisticated Advanced Persistent Threat (APT) attacks. These IDSes employ provenance graphs created from system logs to identify potentially malicious activities. Despite their potential, they face challenges in accuracy, practicality, and scalability, particularly when dealing with large provenance graphs. We present Flash, a scalable IDS that leverages graph representation learning through Graph Neural Networks (GNNs) on data provenance graphs to overcome these limitations. Flash employs a Word2Vec-based semantic encoder to capture essential semantic attributes (e.g., process names and file paths) and the temporal ordering of events within the provenance graph. Furthermore, Flash incorporates a novel adaptation of a GNN-based contextual encoder to efficiently encode both local and global graph structures into expressive node embeddings. To learn benign node behaviors, we utilize a lightweight classifier that combines the GNN and Word2Vec embeddings. Recognizing the computational demands and slow processing times of GNN, particularly for large provenance graphs, we have developed an embedding recycling database to store the node embeddings generated during the training phase. During runtime, our lightweight classifier leverages the stored embeddings, obviating the need to regenerate GNN embeddings, thus facilitating real-time APT detection. Extensive evaluation of Flash on real-world datasets demonstrates superior detection accuracy compared to existing provenance-based IDSes. The results also illustrate Flash\u2019s scalability, robustness against mimicry attacks, and potential for accelerating the alert verification process.",
                "paper_link": "https://www.semanticscholar.org/paper/d3ad994d5504a87b4c79a1ca8485d71b6106be01"
            },
            {
                "title": "Forensic analysis of configuration-based attacks",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Automated threat alert triage via data provenance",
                "abstract": "\u2014Large enterprises are increasingly relying on threat detection softwares (e.g., Intrusion Detection Systems) to allow them to spot suspicious activities. These softwares generate alerts which must be investigated by cyber analysts to \ufb01gure out if they are true attacks. Unfortunately, in practice, there are more alerts than cyber analysts can properly investigate. This leads to a \u201cthreat alert fatigue\u201d or information overload problem where cyber analysts miss true attack alerts in the noise of false alarms. In this paper, we present N O D OZE to combat this challenge using contextual and historical information of generated threat alert. N O D OZE \ufb01rst generates a causal dependency graph of an alert event. Then, it assigns an anomaly score to each edge in the dependency graph based on the frequency with which related events have happened before in the enterprise. N O D OZE then propagates those scores along the neighboring edges of the graph using a novel network diffusion algorithm and generates an aggregate anomaly score which is used for triaging. We deployed and evaluated N O D OZE at NEC Labs America. Evaluation on our dataset of 364 threat alerts shows that N O D OZE consistently ranked the true alerts higher than the false alerts based on aggregate anomaly scores. Further, through the introduction of a cutoff threshold for anomaly scores, we estimate that our system decreases the volume of false alarms by 84%, saving analysts\u2019 more than 90 hours of investigation time per week. N O D OZE generates alert dependency graphs that are two orders of magnitude smaller than those generated by traditional tools without sacri\ufb01cing the vital information needed for the investigation. Our system has a low average runtime overhead and can be deployed with any threat detection software.",
                "paper_link": "https://www.semanticscholar.org/paper/dfe5a726609c318baf509af7cf41bf3840884c0b"
            }
        ]
    },
    {
        "Professor": "Seongkook Heo",
        "Papers": [
            {
                "title": "You watch, you give, and you engage: a study of live streaming practices in China",
                "abstract": "Despite gaining traction in North America, live streaming has not reached the popularity it has in China, where live- streaming has a tremendous impact on the social behaviors of users. To better understand this socio-technological phenomenon, we conducted a mixed methods study of live streaming practices in China. We present the results of an online survey of 527 live streaming users, focusing on their broadcasting or viewing practices and the experiences they find most engaging. We also interviewed 14 active users to explore their motivations and experiences. Our data revealed the different categories of content that was broadcasted and how varying aspects of this content engaged viewers. We also gained insight into the role reward systems and fan group-chat play in engaging users, while also finding evidence that both viewers and streamers desire deeper channels and mechanisms for interaction in addition to the commenting, gifting, and fan groups that are available today.",
                "paper_link": "https://www.semanticscholar.org/paper/1d8ad5af9bbcbf4266aa6d7c049084a4e6584373"
            },
            {
                "title": "Thor's hammer: An ungrounded force feedback device utilizing propeller-induced propulsive force",
                "abstract": "We present a new handheld haptic device, Thor's Hammer, which uses propeller propulsion to generate ungrounded, 3-DOF force feedback. Thor's Hammer has six motors and propellers that generates strong thrusts of air without the need for physical grounding or heavy air compressors. With its location and orientation tracked by an optimal tracking system, the system can exert forces in arbitrary directions regardless of the device's orientation. Our technical evaluation shows that Thor's Hammer can apply up to 4 N of force in arbitrary directions with less than 0.11 N and 3.9\u00b0 of average magnitude and orientation errors. We also present virtual reality applications that can benefit from the force feedback provided by Thor's Hammer. Using these applications, we conducted a preliminary user study and participants felt the experience more realistic and immersive with the force feedback.",
                "paper_link": "https://www.semanticscholar.org/paper/64a007280567943852816677ea2855d922133f77"
            },
            {
                "title": "Force gestures: augmenting touch screen gestures with normal and tangential forces",
                "abstract": "Force gestures are touch screen gestures augmented by the normal and tangential forces on the screen. In order to study the feasibility of the force gestures on a mobile touch screen, we implemented a prototype touch screen device that can sense the normal and tangential forces of a touch gesture on the screen. We also designed two example applications, a web browser and an e-book reader, that utilize the force gestures for their primary actions. We conducted a user study with the prototype and the applications to study the characteristics of the force gestures and the effectiveness of their mapping to the primary actions. In the user study we could also discover interesting usability issues and collect useful user feedback about the force gestures and their mapping to GUI actions.",
                "paper_link": "https://www.semanticscholar.org/paper/a1fad43168ff2e3a272a827784b3483e8047f02e"
            },
            {
                "title": "Pre-Touch Sensing for Mobile Interaction",
                "abstract": "Touchscreens continue to advance including progress towards sensing fingers proximal to the display. We explore this emerging pre-touch modality via a self-capacitance touchscreen that can sense multiple fingers above a mobile device, as well as grip around the screen's edges. This capability opens up many possibilities for mobile interaction. For example, using pre-touch in an anticipatory role affords an \"ad-lib interface\" that fades in a different UI--appropriate to the context--as the user approaches one-handed with a thumb, two-handed with an index finger, or even with a pinch or two thumbs. Or we can interpret pre-touch in a retroactive manner that leverages the approach trajectory to discern whether the user made contact with a ballistic vs. a finely-targeted motion. Pre-touch also enables hybrid touch + hover gestures, such as selecting an icon with the thumb while bringing a second finger into range to invoke a context menu at a convenient location. Collectively these techniques illustrate how pre-touch sensing offers an intriguing new back-channel for mobile interaction.",
                "paper_link": "https://www.semanticscholar.org/paper/4f5208043b35166a0351a89946070cbd7e2464d3"
            },
            {
                "title": "SplitBoard: A simple split soft keyboard for wristwatch-sized touch screens",
                "abstract": "Text entry on a smartwatch is a challenging problem due to the device's limited screen area. In this paper, we introduce the SplitBoard, which is a soft keyboard designed for a smartwatch. As the user flicks left or right on the keyboard, it switches between the left and right halves of a QWERTY keyboard. We report the results of two user experiments where the SplitBoard was compared to an ordinary QWERTY keyboard, the ZoomBoard, SlideBoard, and Qwerty-like keypad. We measured the initial performance with new users for each method. The SplitBoard outperformed all other techniques in the experiments. The SplitBoard is expected to be a viable option for smartwatch text entry because of its light processing requirements, good performance, and immediate learnability.",
                "paper_link": "https://www.semanticscholar.org/paper/5e98d34e50c65bb4438303070ac86eebebe81527"
            },
            {
                "title": "Forcetap: extending the input vocabulary of mobile touch screens by adding tap gestures",
                "abstract": "We introduce an interaction technique that increases the touch screen input vocabulary by distinguishing a strong tap from a gentle tap without the use of additional hardware. We have designed and validated an algorithm that detects different types of screen touches by combining data from the built-in accelerometer with position data from the touch screen. The proposed technique allows a touch screen input to contain not only the position of a finger contact, but also its type, i.e., whether the contact is a 'Tap' or a 'ForceTap.' To verify the feasibility of the proposed technique we have implemented our detection algorithm in experiments that test cases of single-handed, two-handed, immersive, and on the move usage. Based on the experimental results, we investigate the advantages of using two types of touch inputs and discuss emerging issues. Finally, we suggest a design guideline for applying the proposed technique to touch screen applications, and present possible application scenarios.",
                "paper_link": "https://www.semanticscholar.org/paper/242a26f3b0b8e6d5738194bc61444458705eefda"
            },
            {
                "title": "Deeptake: Prediction of driver takeover behavior using multimodal data",
                "abstract": "Automated vehicles promise a future where drivers can engage in non-driving tasks without hands on the steering wheels for a prolonged period. Nevertheless, automated vehicles may still need to occasionally hand the control back to drivers due to technology limitations and legal requirements. While some systems determine the need for driver takeover using driver context and road condition to initiate a takeover request, studies show that the driver may not react to it. We present DeepTake, a novel deep neural network-based framework that predicts multiple aspects of takeover behavior to ensure that the driver is able to safely take over the control when engaged in non-driving tasks. Using features from vehicle data, driver biometrics, and subjective measurements, DeepTake predicts the driver\u2019s intention, time, and quality of takeover. We evaluate DeepTake performance using multiple evaluation metrics. Results show that DeepTake reliably predicts the takeover intention, time, and quality, with an accuracy of 96%, 93%, and 83%, respectively. Results also indicate that DeepTake outperforms previous state-of-the-art methods on predicting driver takeover time and quality. Our findings have implications for the algorithm development of driver monitoring and state detection.",
                "paper_link": "https://www.semanticscholar.org/paper/96636e0076fcea6e848f37481fd27b6af37e78fa"
            },
            {
                "title": "Streamwiki: Enabling viewers of knowledge sharing live streams to collaboratively generate archival documentation for effective in-stream and post hoc learning",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "PseudoBend: Producing haptic illusions of stretching, bending, and twisting using grain vibrations",
                "abstract": "We present PseudoBend, a haptic feedback technique that creates the illusion that a rigid device is being stretched, bent, or twisted. The method uses a single 6-DOF force sensor and a vibrotactile actuator to render grain vibrations to simulate the vibrations produced during object deformation based on the changes in force or torque exerted on a device. Because this method does not require any moving parts aside from the vibrotactile actuator, devices designed using this method can be small and lightweight. Psychophysical studies conducted using a prototype that implements this method confirmed that the method could be used to successfully create the illusion of deformation and could also change users' perception of stiffness by changing the virtual stiffness parameters.",
                "paper_link": "https://www.semanticscholar.org/paper/e907d9b7d400780557730ec284ea1db8451fa0d0"
            },
            {
                "title": "Typing on a smartwatch for smart glasses",
                "abstract": "While smart glasses make information more accessible in mobile scenarios, entering text on these devices is still difficult. In this paper, we suggest using a smartwatch as an indirect input device for smart glasses text entry. With the watch-glasses combination, users do not need to lift the arm to touch the glasses nor need to carry a special external input device. To prove the feasibility of the suggested combination, we implemented two text entry methods: a modified version of SwipeBoard, which we adapted for the suggested combination, and HoldBoard, which we newly designed and implemented specifically for the suggested combination. We evaluated the performances of the two text entry methods through two user studies, and could show that they are faster than prior art for smart glasses text entry in a seated condition. A further study showed that they are competitive with the prior art also in a walking condition.",
                "paper_link": "https://www.semanticscholar.org/paper/22260611cc8e8f880dbf1e55c13ef2d7da6d0d28"
            },
            {
                "title": "Plane, ray, and point: Enabling precise spatial manipulations with shape constraints",
                "abstract": "We present Plane, Ray, and Point, a set of interaction techniques that utilizes shape constraints to enable quick and precise object alignment and manipulation in virtual reality. Users create the three types of shape constraints, Plane, Ray, and Point, by using symbolic gestures. The shape constraints are used like scaffoldings and limit and guide the movement of virtual objects that collide or intersect with them. The same set of gestures can be performed with the other hand, which allow users to further control the degrees of freedom for precise and constrained manipulation. The combination of shape constraints and bimanual gestures yield a rich set of interaction techniques to support object transformation. An exploratory study conducted with 3D design experts and novice users found the techniques to be useful in 3D scene design workflows and easy to learn and use.",
                "paper_link": "https://www.semanticscholar.org/paper/6302d830ea2c73d6e5c70724f3ea3f276d552a1c"
            },
            {
                "title": "Expanding touch input vocabulary by using consecutive distant taps",
                "abstract": "In recent years, touch screens have emerged and matured as the main input interface for mobile and tablet computers calling for extended touch input possibilities. In this paper, we explore the use of consecutive distant taps to expand the touch screen input vocabulary. We analyzed time intervals and distances between consecutive taps during common applications on a tablet and verified that consecutive distant taps can be used conflict-free with existing touch gestures. We designed the two interaction techniques Ta-tap and Ta-Ta-tap that utilize consecutive distant taps. Ta-tap uses two consecutive distant taps to invoke alternative touch operations for multi-touch emulation, whereas Ta-Ta-tap uses a series of consecutive distant taps to define a spatial gesture. We verified the feasibility of both interaction techniques through a series of experiments and a user study. The high recognition rate of Ta-tap and Ta-Ta-tap gestures, the few conflicts with existing gestures, and the positive feedback from the participants assert the potential of consecutive distant taps as a new design space to enrich touch screen interactions.",
                "paper_link": "https://www.semanticscholar.org/paper/42d60ff6397669df5238d8558a6c7f7c99022331"
            },
            {
                "title": "Indirect shear force estimation for multi-point shear force operations",
                "abstract": "The possibility of using shear forces is being explored recently as a method to enrich touch screen interaction. However, most of the related studies are restricted to the case of single-point shear forces, possibly owing to the difficulty of independently sensing shear forces at multiple touch points. In this paper, we propose indirect methods to estimate shear forces using the movement of contact areas. These methods enable multi-point shear force estimation, where the estimation is done for each finger independently. We show the feasibility of these methods through an informal user study with a demo application utilizing these methods.",
                "paper_link": "https://www.semanticscholar.org/paper/ac0c204eec1dd761d5eddfda4ade2dc9aec29354"
            },
            {
                "title": "No Need to Stop What You\u2019re Doing: Exploring No-Handed Smartwatch Interaction",
                "abstract": "Smartwatches have the potential to enable quick micro-interactions throughout daily life. However, because they require both hands to operate, their full potential is constrained, particularly in situations where the user is actively performing a task with their hands. We investigate the space of no-handed interaction with smartwatches in scenarios where one or both hands are not free. Specifically, we present a taxonomy of scenarios in which standard touchscreen interaction with smartwatches is not possible, and discuss the key constraints that limit such interaction. We then implement a set of interaction techniques and evaluate them via two user studies: one where participants viewed video clips of the techniques and another where participants used the techniques in simulated hand-constrained scenarios. Our results found a preference for foot-based interaction and reveal novel design considerations to be mindful of when designing for no-handed smartwatch interaction scenarios.",
                "paper_link": "https://www.semanticscholar.org/paper/e2696b9dd5cfbd72518d2324529ae229547c57fb"
            },
            {
                "title": "ForceDrag: using pressure as a touch input modifier",
                "abstract": "It is common to use modifier keys in a PC environment in order to change drag modes, but mobile devices with a touch screen do not provide this option. Thus, we present ForceDrag to address this issue, which is a touch drag operation where pressure is used as a modifier key to change touch drag modes. We also introduce the concept of force lock and we compare three selection techniques for performing a force lock in a user study. We also describe a prototype implementation and discuss early user feedback on ForceDrag.",
                "paper_link": "https://www.semanticscholar.org/paper/a461d20b085ac1b0494ee4bd45b632292cfb6b22"
            },
            {
                "title": "LongPad: a touchpad using the entire area below the keyboard of a laptop computer",
                "abstract": "In this paper, we explore the possibility of a long touchpad that utilizes the entire area below the keyboard of a laptop computer. An essential prerequisite for such a touchpad is a robust palm rejection method, which we satisfy using a proximity-sensing touchpad. We developed LongPad, a proximity-sensing optical touchpad that is as wide as a laptop keyboard, and implemented a palm rejection algorithm that utilizes proximity images from LongPad. In a user study conducted, we observed that LongPad rejected palm touches almost perfectly while participants were repeating typing and pointing tasks. We also summarize the new design space enabled by LongPad and demonstrate a few of the interaction techniques it facilitates.",
                "paper_link": "https://www.semanticscholar.org/paper/3e303a1c355a0e8f61beb1eb4cfb7524b6cf0f11"
            },
            {
                "title": "MagTouch: Robust Finger Identification for a Smartwatch Using a Magnet Ring and a Built-in Magnetometer",
                "abstract": "Completing tasks on smartwatches often requires multiple gestures due to the small size of the touchscreens and the lack of sufficient number of touch controls that are easily accessible with a finger. We propose to increase the number of functions that can be triggered with the touch gesture by enabling a smartwatch to identify which finger is being used. We developed MagTouch, a method that uses a magnetometer embedded in an off-the-shelf smartwatch. It measures the magnetic field of a magnet fixed to a ring worn on the middle finger. By combining the measured magnetic field and the touch location on the screen, MagTouch recognizes which finger is being used. The tests demonstrated that MagTouch can differentiate among the three fingers used to make contacts at a success rate of 95.03%.",
                "paper_link": "https://www.semanticscholar.org/paper/0ce127ebea937d1730bdb2a4625d6bd94a3bdef4"
            },
            {
                "title": "Mining social relationship types in an organization using communication patterns",
                "abstract": "Our goal is to show that it is possible to automatically infer social relationship types among people who stay together in an organization by analyzing communication patterns. We collected indoor co-location data and instant messenger data from 22 participants for one month. Based on the data, we designed and explored several indicators which are considered to be useful for mining social relationship types. We applied machine learning techniques using the indicators and found that it is possible to develop an intelligent method to infer social relationship types.",
                "paper_link": "https://www.semanticscholar.org/paper/67a81977ada8c6616aab552559e373c42b6a572e"
            },
            {
                "title": "FluidMeet: Enabling Frictionless Transitions Between In-Group, Between-Group, and Private Conversations During Virtual Breakout Meetings",
                "abstract": "People often form small conversation groups during physical gatherings to have ad-hoc and informal conversations. As these groups are loosely defined, others can often overhear and join the conversation. However, current video-conferencing tools only allow for strict boundaries between small conversation groups, inhibiting fluid group formations and between-group conversations. This isolates small-group conversations from others and leads to inefficient transitions between conversations. We present FluidMeet, a virtual breakout meeting system that employs flexible conversation boundaries and cross-group conversation visualizations to enable fluid conversation group formations and ad-hoc, informal conversations. FluidMeet enables out-group members to overhear group conversations while allowing conversation groups to control their shared level of context. Users within conversation groups can also quickly switch between in-group and private conversations. A study of FluidMeet showed that it encouraged users to break group boundaries, made them feel less isolated in group conversations, and facilitated communication across different groups.",
                "paper_link": "https://www.semanticscholar.org/paper/15a185dfec183b3d671a539868c37c2a15aeabaf"
            },
            {
                "title": "ThickPad: a hover-tracking touchpad for a laptop",
                "abstract": "We explored the use of a hover tracking touchpad in a laptop environment. In order to study the new experience, we implemented a prototype touchpad consisting of infrared LEDs and photo-transistors, which can track fingers as far as 10mm over the surface. We demonstrate here three major interaction techniques that would become possible when a hover-tracking touchpad meets a laptop",
                "paper_link": "https://www.semanticscholar.org/paper/f5ec46b31b56d4e94fc2f145f8861da8c4d4727d"
            }
        ]
    },
    {
        "Professor": "Thomas B. Horton",
        "Papers": [
            {
                "title": "Using LEGO MINDSTORMS NXT and LEJOS in an advanced software engineering course",
                "abstract": "This paper describes the benefits of using LeJOS and the Lego Mindstorms NXT set for teaching advanced software development. While Lego Mindstorms has been used in introduction to computer science courses, it is not reported to be widely used in a simulated production environment requiring such things as threading, network communications, and the implementation of command protocols. Additionally, because the Mindstorms NXT system supports Bluetooth communications with multiple devices, it is possible to use this system as the basis for a complex, communicating system requiring multiple software artifacts on different machines.",
                "paper_link": "https://www.semanticscholar.org/paper/b269756012f09938573caf76e8cfcf9f804afb16"
            },
            {
                "title": "From domain models to architecture frameworks",
                "abstract": "Thispaper &resses how domain analysis served to help create reusable architectures and components in the development of a real-time embealied system. The subject domain is portable winders communication devices. The paper discwrses this experience in terms of discovering and developing reusable @neworkr for this domain. Some interesting differences between this approach and what is usually suggested as a process for developing frameworks are described.",
                "paper_link": "https://www.semanticscholar.org/paper/cccaa39459a43bb84f02eb15776f5a0ba89601ff"
            },
            {
                "title": "Computing classification system 1998: current status and future maintenance report of the ccs update committee",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "The Effectiveness of the Stylometry of Function words in Discriminating between Shakespeare and Fletcher",
                "abstract": "A number of recent' successful authorship studies have relied on a statistical analysis of language features based on function words. However, stylometry has not been extensively applied to Elizabethan and Jacobean dramatic questions. To determine the effectiveness of such an approach in this field, language features are studied in twenty-four plays by Shakespeare and eight by Fletcher. The goal is to develop procedures that might be used to determine the authorship of individual scenes in The Two Noble Kinsmen and Henry VIII. Homonyms, spelling variants and contracted forms in old-spelling dramatic texts present problems for a computer analysis. A program that uses a system of pre-edit codes and replacement /expansion lists was developed to prepare versions of the texts in which all forms of common words can be recognized automatically. To evaluate some procedures for determining authorship developed by A. Q. Morton and his colleagues, occurrences of 30 common collocations and 5 proportional pairs are analyzed in the texts. Within-author variation for these features is greater than had been found in previous studies. Univariate chi-square tests are shown to be of limited usefulness because of the statistical distribution of these textual features and correlation between pairs of features. The best of the collocations do not discriminate as well as most of the individual words from which they are composed. Turning to the rate of occurrence of individual words and groups of words, distinctiveness ratios and t-tests are used to select variables that best discriminate between Shakespeare and Fletcher. Variation due to date of composition and genre within the Shakespeare texts is examined. A multivariate and distributionfree discriminant analysis procedure (using kernel estimation) is introduced. The classifiers based on the best marker words and the kernel method are not greatly affected by characterization and perform well for samples as short as 500 words. When the final procedure is used to assign the 459 scenes of known authorship (containing at least 500 words), \"\"*-` 1795% are assigned to C\"orrect author. Only two scenes are incorrectly classified, and 4.5% of the scenes cannot be assigned to either author by the procedure. When applied to individual scenes of at least 500 words in The Two Noble Kinsmen and Henry VIII, the procedure indicates that both plays are collaborations and generally supports the usual division. However, the marker words in a number of scenes often attributed to Fletcher are very much closer to Shakespeare's pattern of use. These scenes include TNK W.iii and H8 I.iii, IV.i\u2014ii and V.iv. To the memory of my mother, who always believed in the members of her family, even when we ourselves doubted.",
                "paper_link": "https://www.semanticscholar.org/paper/0c596b25dc18a651769171fa6393dd0e19284d3d"
            },
            {
                "title": "Architecting for domain variability",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/925106762340b0b8af06d5956d9eb87c9a412ef0"
            },
            {
                "title": "Applying domain analysis and modeling: an industrial experience",
                "abstract": "In this paper we describe our experience in applying domain analysis within a company that develops personal electronic devices. We describe how we tailored the DSSA method to suit our needs and then present the process and representations that we found most useful for this situation. The conclusions and lessons learned are useful because few studies published at this time provide details about applications of domain engineering in commercial development environments.",
                "paper_link": "https://www.semanticscholar.org/paper/c364ae9423e86fa187810a4a5875cd86fbf7e0f3"
            },
            {
                "title": "A domain model of WWW browsers",
                "abstract": "Domain analysis is the process of identifying and organizing knowledge about some class of problems to support the description and solution of those problems, WWW (World Wide Web) browsers was chosen as the domain to illustrate two different levels of domain analysis. In this work we illustrate a problem oriented domain analysis on the navigational features of WWW browsers and a solution oriented domain analysis on WWW browser caching.",
                "paper_link": "https://www.semanticscholar.org/paper/f3a7f477248de748bdba9eddf56317bc744c78b3"
            },
            {
                "title": "Teaching second-level Java and software engineering with Android",
                "abstract": "Over the past two years, second-year Java and software engineering courses have been taught at the University of Virginia and North Carolina State University utilizing the Android OS platform. Instructors taught a variety of traditional second-year topics, including abstraction, design, requirements, and testing, utilizing a variety of Android-based mobile devices. Anecdotal responses from student surveys and evaluations from five course sessions indicate that teaching lower-level courses with more advanced and current technology, even with a steeper learning curve, is beneficial. In this tutorial proposal, we outline our plan for presenting a session that would help educators incorporate the Android OS into their curriculum and how to use the system even if mobile devices are not available.",
                "paper_link": "https://www.semanticscholar.org/paper/277bfa561e609df00d2a298aa1b6dafab92728ae"
            },
            {
                "title": "Evaluating a software engineering project course model based on studio presentations",
                "abstract": "We present a model for a software engineering project course that has proven successful as the capstone of our required sequence of software courses in our computer science major. It differs from many similar courses because all teams work on the same project, defined by the instructor (not projects for \"real\" customers). The course's project centers on weekly \"studio presentations\" during which student teams present preliminary results and receive immediate feedback. This model allows us to efficiently offer a project course for over a hundred students. The nature of the project itself supports other course goals. Assessment of our program and the course indicates that the course successfully fulfills its role within our degree program",
                "paper_link": "https://www.semanticscholar.org/paper/253929282d2e29814b602c8d8e28281ba493be14"
            },
            {
                "title": "Panel on teaching faculty positions",
                "abstract": "1. Summary Many SIGCSE attendees have considered or are considering faculty positions focused on education and teaching that are not traditional tenure-track positions. In addition, many Computer Science departments have added or are considering adding such faculty positions [1]. This panel seeks to explore models for teaching faculty positions currently established at several schools and the experiences of panelists in those positions. Panelists will describe their current position in terms of job responsibilities, compensation and benefits, job security, and status within the department. In addition they will relate their personal experiences in what factors led them to seek out and stay in these positions.We also will present the results of a survey of teaching positions at other schools to be conducted prior to the conference. The panel should be of interest to educators already in teaching positions, those considering these positions, or departments that are considering adding such positions to their faculty.",
                "paper_link": "https://www.semanticscholar.org/paper/9b147744d8d0ccabaa0c947f62a727037f3a7574"
            },
            {
                "title": "An object-oriented methodology for the design of control software for flexible manufacturing systems",
                "abstract": "We describe here a systematic method for the design and modeling of flexible manufacturing systems, using object-oriented concepts and Petri nets. We first define the system components in terms of an object model consisting of hierarchical sets of classes and associations. Then, we model the dynamic aspects of the system using statecharts, including exceptions and abnormal behavior. As a third step, we derive Petri nets from those statecharts to realize the concurrency present in the system. Finally we develop a hierarchy of controllers, corresponding to the layers of the object model, for the independent components of the system based on the Petri nets obtained in the previous step. The approach is systematic and relatively easy to apply. It is illustrated through a realistic example.<<ETX>>",
                "paper_link": "https://www.semanticscholar.org/paper/52e9213f0482d8fc7257a00b1a7e63773efe7bdf"
            },
            {
                "title": "ACM Computing Classification System 1998: Current Status and Future Maintenance",
                "abstract": "The ACM Computing Classification System (CCS), which for 20 years has served as the primary and most generally used system for the classification and indexing of the published literature of computing, has been substantially revised to reflect the greatly changed nature of computing. In this article, the revision committee reports on its work, describes the changes in the CCS, proposes processes for making annual changes, and recommends a future total revision.",
                "paper_link": "https://www.semanticscholar.org/paper/9814039370e355d529ec08eb92f32c0b96470c58"
            },
            {
                "title": "Distinguishing Shakespeare from Fletcher through function words.",
                "abstract": "The article explores the function of Shakespeare\u2019s words as quoted by two characters of David Copper\ufb01eld \u2014 David and Micawber. Each showing excellent memory of Shakespeare\u2019s works, the two heroes embody opposing borrowing strategies. Whereas David carefully judges if the narrated subject matches a Shakespearean quote in its semantic and expressive power and may choose to adapt or altogether reject it upon reflection, Micawber borrows from Shakespeare almost unconsciously, at the same time showing a particular weakness for the most memorable and tragic lines. This inapt quoting o\ufb05en reduces Micawber to a bombastic thespian. The characters\u2019 dialogue with Shakespeare is, in turn, one in which Dickens, famously fond of his great predecessor, is engaged himself through his novel. It appears, therefore, that the Shakespearean \ufb01eld brings the author closer to his characters. The novel\u2019s references to Shakespeare\u2019s plays which are analysed in this article were mentioned in V. Gager\u2019s catalogue, yet remained hitherto unexplored in the comparative context and with regard to the novel.",
                "paper_link": "https://www.semanticscholar.org/paper/88428f151c881e89bd1451f9c8865bb0d701b7c9"
            },
            {
                "title": "Frequent Words, Authorship, and Characterization in Jacobean Drama",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Work in progress-reexamining closed laboratories in computer science courses",
                "abstract": "We present early results of a project that ultimately seeks to define how best to design, implement and deliver the next generation of laboratories for courses in computer science. In response to computing curriculum 1991 many computing programs adopted closed laboratories, especially for early courses. But advances in computing since 1991 raise questions about why closed laboratories should be used. The project has begun with a reexamination of the goals and strengths of the closed laboratory model. We have defined a classification of the types of activities students carry out in closed laboratories, along with set of rationales for why an activity might be included in a closed laboratory. These results were used to modify some closed laboratories in our CS1 course. We believe that the more complete understanding that our analysis of closed laboratories provides will help us design more effective practicum-oriented experiences for students in computing courses.",
                "paper_link": "https://www.semanticscholar.org/paper/1f9de79ea1418c08e0e13b128820e9af990441d6"
            },
            {
                "title": "Using commercial CASE environments to teach software design",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/469a70e8d860132e5dfd5673ac6d904d0ca9aeb7"
            },
            {
                "title": "Sequence Comparison and Old-Spelling Texts",
                "abstract": "Part One: Statistical Methods. 1: Christian Delcourt: Some Reflections about Current Practice in Lexicometry. 2: Philippe Thoiron: Topography of Terms and Structure of Techno-Scientific Texts. Part Two: Morphology and Syntax. 3: Harry Gaylord, Harry Overdijk, and Ferdinand de Haan: The AVTOSLOV Project: Encoding Russian Morphology. 4: Kimmo Kettunen: Evaluating FUNDPL, A Dependency Parsing Formalism for Finnish. Part Three: Textual Criticism. 5: Didier Xhardez: Computer-Assisted Study of a Textual Tradition. 6: Thomas B. Horton: Sequence Comparison and Old-Spelling Texts. Part Four: Language and Text Understanding. 7: Francesco Antonacci, Cecilia M. Calamani, and Mirella Schaerf: Deduction of Implicit Information in a Text Understanding System. 8: Jules Duchastel, Louis-Claude Paquin, and Jacques Beauchemin: Automated Syntactic Text Description Enhancement: Determination Analysis. 9: Nancy Ide and Jean Veronis: Refining Taxonomies Extracted from Machine-Readable Dictionaries. 10: Sally Yeates Sedelow and Walter A. Sedelow Jr.: A Topologic Model of the English Semantic Code and its Role in Automatic Disambiguation for Discourse Analysis. Part Five: Language and Text Generation. 11: Greg Lessard, Michael Levison and Mark Olsen: Possible and Impossible Pronouns: The Role of Text Bases and Natural Language Generation in Linguistic Analysis. 12: Ulrich Schmitz: Automatic Generation of Texts without Using Cognitive Models: Television News. Part Six: Nonlinear Text. 13: John Kirk, George Munroe, and M. J. Damian O'Kane: Electronic Word Maps. 14: Expen S. Ore: Project Litera: Computer Aids in Restoring Partly Preserved Letters in Papyri. 15 Hypertext Libraries: The Automated Production of Hypertext Documents: Eve Wilson:",
                "paper_link": "https://www.semanticscholar.org/paper/70f68a5c8bbaf423add560e778aab597eabcca0d"
            },
            {
                "title": "How Student Surveys Drive Change: Using the Data Buddies Department Report from the Computing Research Association",
                "abstract": "This panel provides examples of how faculty refine, expand, and evaluate initiatives to broaden participation in computing using the Data Buddies Survey (https://cra.org/cerp/data-buddies). The Computing Research Association administers the survey annually and prepares summary reports for over 140 member institutions. Participation is free, and the survey gathers information about students' educational experiences, confidence, attitudes, and career goals. Faculty and staff from prospective and participating institutions can benefit from the panelists' discussion of aligning department initiatives for broadening participation with data about students' experiences.",
                "paper_link": "https://www.semanticscholar.org/paper/3db9cc5e4f5c9f0d9c227ecf67c2ddd8b6ff8e8c"
            },
            {
                "title": "Role of larger software artifacts in introductory computer science courses",
                "abstract": "This paper compares the effectiveness of two approaches that can be used to teach concepts in introductory courses such as CS1 and CS2 \u2014 a conventional lecture-based approach and one using larger software programs (artifacts) with accompanying guided exercises. Our assessment includes measures of students' self-confidence as well as a measurement of their knowledge of the topics used in this study: inheritance and iterators. Finally, we consider some generalizations that can be made about these treatments and how well they perform.",
                "paper_link": "https://www.semanticscholar.org/paper/8581faec5cf9348b0d6c1316020b022b2754145a"
            },
            {
                "title": "Hazard Mitigation Planning for Utilities: Forming Partnerships for Leveraging Resources and Funding Opportunities",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/8b8e5b1774eadfd96081e63a288ab008d614eec1"
            }
        ]
    },
    {
        "Professor": "John R. Hott",
        "Papers": [
            {
                "title": "Educational Landscapes during and after COVID-19",
                "abstract": "The coronavirus (COVID-19) pandemic has forced an unprecedented global shift within higher education in the ways that we communicate with and educate students. This necessary paradigm shift has compelled educators to take a critical look at their teaching styles and use of technology. Computing education traditionally focuses on experiential, in-person activities. The pandemic has mandated that educators reconsider their use of student time and has catalysed overnight innovations in the educational setting. Even in the unlikely event that we return entirely to pre-COVID-19 norms, many new practices have emerged that offer valuable lessons to be carried forward into our post-COVID-19 teaching. This working group will explore what the post-COVID-19 academic landscape might look like, and how we can use lessons learned during this educational shift to improve our subsequent practice. The exploration will strive to identify practices within computing that appear to have been improved through exposure to online tools and technologies, and that should therefore continue to be used in the online space. In the broadest sense, our motivation is to explore what the post-COVID-19 educational landscape will look like for computing education.",
                "paper_link": "https://www.semanticscholar.org/paper/c46abc36c8bfbe3eacde22faa26ee5fb38cf08a8"
            },
            {
                "title": "Teaching through a global pandemic: educational landscapes before, during and after COVID-19",
                "abstract": "The coronavirus (COVID-19) pandemic has forced an unprecedented global shift within higher education in how instructors communicate with and educate students. This necessary paradigm shift has compelled educators to take a critical look at their teaching styles and use of technology. Computing education traditionally focuses on experiential, in-person activities. The pandemic has mandated that educators reconsider their use of student time and has catalysed overnight innovations in the educational setting. Even in the unlikely event that we return entirely to pre-pandemic norms, many new practices have emerged that offer valuable lessons to be carried forward into our post-COVID-19 teaching. This working group will explore what the post-COVID-19 academic landscape might look like, and how we can use lessons learned during this educational shift to improve our subsequent practice. Following a multinational study of computing faculty, this exploratory stage will identify practices within computing that appear to have been improved through exposure to online tools and technologies, and that should therefore continue to be used in the online space. In the broadest sense, our motivation is to explore what the post-COVID-19 educational landscape will look like for computing education.",
                "paper_link": "https://www.semanticscholar.org/paper/e9fe6f5a419b11e33a34af824859b9ae0e876b7b"
            },
            {
                "title": "Gender and engagement in CS courses on piazza",
                "abstract": "Online discussion forums are being increasingly used in classrooms as a way to encourage collaborative learning and community. Piazza is one such forum that was built specifically for academic institutions, and has been widely adopted. Students have the opportunity to ask questions and seek answers from peers and instructors alike online, allowing them to find the information they need even if they do not know fellow students in the class or if they cannot make an instructor's office hours. However, recent analysis of the popular online discussion site Stack Overflow, suggests that women are more likely than men to withdraw from such a community if they do not identify other members of the same gender. Women are often a minority in computer science courses and may express difficulty interacting with or seeking help from their peers who are predominantly men. Considering the importance of providing equal access to students regardless of gender and the value of resources like Piazza in one's education, it is imperative to assess the representation and impact of gender on Piazza. We analyzed data from over 2,500 Piazza users across three computer science courses at the University of Virginia and found that women on Piazza post more questions than men, spend more time on the discussion site, and achieve higher reputation scores on average. However, they are more likely than men to both ask and answer questions anonymously and less likely to receive responses from members of the same gender.",
                "paper_link": "https://www.semanticscholar.org/paper/86446157adefb621d7f1485306ff50092187a4bc"
            },
            {
                "title": "Stop reinventing the wheel! promoting community software in computing education",
                "abstract": "Historically, computing instructors and researchers have developed a wide variety of tools to support teaching and educational research, including exam and code testing suites and data collection solutions. However, these tools often find limited adoption beyond their creators. As a result, it is common for many of the same functionalities to be re-implemented by different instructional groups within the Computing Education community. We hypothesise that this is due in part to discoverability, availability, and adaptability challenges. Further, instructors often face institutional barriers to deployment, which can include hesitance of institutions to rely on community developed solutions that often lack a centralised authority and may be community or individually maintained. To this end, our working group explored what solutions are currently available, what instructors needed, and the reasons behind the above-mentioned phenomenon. To do so, we reviewed existing literature and surveyed the community to identify the tools that have been developed by the community; the solutions that are currently available and in use by instructors; what features are needed moving forward for classroom and research use; what support for extensions is needed to support further Computing Education research; and what institutional challenges instructors and researchers are currently facing or have faced in using community software solutions. Finally, the working group identified factors that limited adoption of solutions. This work proposes ways to integrate and improve the availability, discoverability, and dissemination of existing community projects, as well as ways to manage and overcome institutional challenges.",
                "paper_link": "https://www.semanticscholar.org/paper/25398a84f237ed206201f345c030742fc017e4d7"
            },
            {
                "title": "How do students collaborate? Analyzing group choice in a collaborative learning environment",
                "abstract": "Collaborative learning has been effective and widely adopted in Computer Science education. Existing studies have controlled for group sizes by assigning members to determine the optimal collaboration environment, with some focusing on a peer-programming environment and others observing a wider range of sizes and tasks. We analyzed collaboration trends through an observational study of 189 students in a large upper-level Computer Science algorithms course, which uses a less-constrained collaborative setting. In the course, the collaboration policy encourages students to choose their own groups for each assignment, up to four other students, offering insight into how groups evolve in size and membership when students are given the freedom to self-select. Since each student is required to submit their own individual work, we collected information about the grade and self-reported collaborators of each research participant for nine assignments, including written and coding homework. Our results show that any collaboration improved individual performance on average. For programming assignments, groups of size four were optimal. Across both written and programming assignments, larger groups performed better, including chains of collaboration greater than the course policy allowed. However, sizes 4-5 performed best within the bounds of the policy. We also demonstrate that factors impacting collaboration include homework difficulty, time of grade release, students' relative performance with respect to the class, as well as the homework type.",
                "paper_link": "https://www.semanticscholar.org/paper/64d28810e7244c3ae16fb84f5f7bbde2113fd368"
            },
            {
                "title": "A course in software development",
                "abstract": "The paper discusses a course in software development, as advocated by the CC2001 report. The course revolves around a single project divided into six assignments. In addition, the course includes lab assignments covering the tool of the week. The order of coverage of topics and the order of labs is determined using just-in-time learning. Grading criteria and an assessment of the course are discussed.",
                "paper_link": "https://www.semanticscholar.org/paper/ede7664897a81364b14af67eb40529c2205a495a"
            },
            {
                "title": "Project-Based and Assignment-Based Courses: A Study of Piazza Engagement and Gender in Online Courses",
                "abstract": "Project-based (PB) learning has become increasingly popular in computer science education, particularly as studies have found that the teaching style better prepares students for future careers and improves learning outcomes through increased student engagement. Online forum usage is one measurable component of engagement. In order to study the impact of PB learning on online forum engagement, Piazza usage data from seven online computer science courses at a higher education institution were collected and examined. We analyzed the differences in online forum usage between PB and assignment-based (AB) learning, in addition to differences between men and women in each course type. Specifically, this study builds upon and replicates a previous study on Piazza that measured student engagement, anonymity usage, and peer parity. We found that students in PB courses were less actively engaged in online forums than students in AB courses; they were less likely to ask and answer questions on Piazza but were more likely to view posts and be logged on more days. Across both course types, students posted anonymously a similar amount as a proportion of the total number of questions and answers and experienced a proportionally similar amount of peer parity. Our findings mirror prior results on gender engagement on Piazza. Across both PB and AB courses, women were more engaged, asked and viewed more questions, posted anonymously more frequently, and were less likely to experience peer parity than men.",
                "paper_link": "https://www.semanticscholar.org/paper/e4f8b98a4dd4e8c1dffe12b2f39c09611f867c02"
            },
            {
                "title": "Leveraging Community Software in CS Education to Avoid Reinventing the Wheel",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Towards More Efficient Office Hours for Large Courses: Using Cosine Similarity to Efficiently Construct Student Help Groups",
                "abstract": "As undergraduate enrollment in computer science rises, instructors continue to investigate methods to improve the student experience at scale. One aspect commonly used in courses at scale is queue-driven office hours, in which students join an online queue and meet with teaching assistants on a first-come, first-serve basis (FIFO). This poster introduces a novel office hours queue feature that automatically groups students in office hours using the cosine similarity metric across their reported issues provided upon joining the queue. Using real office hour attendance data from a 480-person undergraduate course as a basis for simulation, we find that moderate decreases in student wait time during the semester overall (11% on average) are possible, with more significant decreases possible on the busiest days (20% on average). This approach is suitable for real-world testing and these gains are possible without asking students to provide any additional information than they already do when attending office hours. Therefore, this work provides motivation for implementation of such an approach in future courses.",
                "paper_link": "https://www.semanticscholar.org/paper/75147d3ac4b65f19a2aaeec792d979fa56d9dcbb"
            },
            {
                "title": "Ask me anything: Assessing academic dishonesty",
                "abstract": "We provide a method for assessing self-reported rates of cheating among students. The method is both i) privacy-preserving in the sense that one cannot use answers as evidence that any particular student cheated and ii) non-anonymous in the sense that one can record each student's answer for use in future correlative studies. Because accuracy relies on students' willful participation, we describe how to convince students that they take no risk by taking the survey. This method showed that 42% of 847 students willfully cheated in an Algorithms course. Surveying 181 CS Theory students showed no difference in cheating rates on written vs. coding assignments.",
                "paper_link": "https://www.semanticscholar.org/paper/3e054cee766847e8de4f2fe114c1b33f996dec7e"
            },
            {
                "title": "Fix the Course, Not the Student: Positive Approaches to Cultivating Academic Integrity",
                "abstract": "The best-studied techniques for reducing academic dishonesty rates rely on increasing the likelihood of consequences. These techniques offer instructors effective tools for identifying dishonest behavior as a means to \"encourage\" honesty. We wonder if we can promote integrity through proactive measures, such as designing courses' structures and assignments to reduce temptations for cheating, or by sculpting culture and forming relationships to foster a robust \"community of trust.\" Our discussion will consider students' perspectives on academic integrity, how those might differ from instructors' perspectives, and how to build firm yet compassionate systems for promoting honesty in coursework. Through conversations with students and faculty, the discussion leaders have identified several compelling explanations for student cheating that largely derive from student culture and faculty messaging. We hope to share these lessons and inspire tactics for instructors to address student temptations to cheat that do not rely on the threat of penalty. This can help foster mentoring, rather than antagonistic, relationships between students and instructors, making computing courses increasingly welcoming for a greater diversity of students.",
                "paper_link": "https://www.semanticscholar.org/paper/9e87c80f9877b80d35e27e3a5afd34f1c6058246"
            },
            {
                "title": "Evolving social structures: Networks with people as the edges",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Analyzing Student Performance with Free Late Submission Days",
                "abstract": "We investigate the effects of a flexible late policy on student performance submission behavior across two semesters of a lower-level required course (LL) and an upper-level elective (UL). The first semester late policies in both courses were strict: LL incorporated a 10% penalty per day for two days; UL rarely allowed late submissions. In each course, the late policy was relaxed in the second semester to provide two no-penalty late days for each assignment. Our results show that in the LL class with the strict late policy, grades decrease each day past the deadline. When given two no-penalty late days, students in LL tended to submit their assignments later, with 41% of submissions coming after the deadline. While students' homework scores were higher than their peers with the stricter policy (before penalties) on the first day after the deadline, their scores on the second day of submission were equivalent. With more late submissions, the overall homework scores in LL were lower with the no-penalty late days (controlling for late penalties). In contrast, students in UL submitting two days late in the second semester performed similarly to their peers in the first semester with the stricter penalty, while average scores for assignments submitted on (and around) the deadline improved dramatically. These results suggest that providing an extra day with no penalty is helpful for students, such as those making just-in-time submissions, but that additional days may lower performance overall for students in lower-level courses.",
                "paper_link": "https://www.semanticscholar.org/paper/b5e15b88f3c77e7728148f623ac53225efa737f4"
            },
            {
                "title": "Providing a Choice of Time Trackers on Online Assessments",
                "abstract": "Online assessments allow instructors to facilitate exams and quizzes in both virtual and large classes. Having a clear online timer during these assessments is vital to help students manage their time. However, these same timers can be a cause of anxiety, affecting student performance. Our goals were to determine (i) which types of visualizations are currently in use, (ii) which styles of online timer were preferred by students, and (iii) if providing students a choice of timer impacted their performance. We carried out a semester-long study employing multiple time-tracking displays across 29 online quizzes in three Computer Science courses with a total of 113 student participants. Timer visualizations included count down and elapsed time text as a text-only display or combined with a graphical representation, such as a color-changing progress bar, gray-scale progress bar, or changing phases of the moon. Overwhelmingly students chose a time tracker that counted down the time left in the quiz, preferred graphical displays to text-only, and visualizations that changed color to better indicate the passing of time. Students who were given a choice on all assessments throughout the study typically picked and kept the same timer throughout or settled on a preferred timer after only a few assessments. Providing students that choice before their quiz had no significant effect on their performance relative to students who were not given a choice. These findings indicate that it is helpful to give students the choice of online timer, providing them a more accommodating and comfortable testing environment.",
                "paper_link": "https://www.semanticscholar.org/paper/b13c6e9cfd64ac78fea94921adeb306137a4a333"
            },
            {
                "title": "Analyzing Student Experience of Time Trackers on Assessments",
                "abstract": "Visualizing time limits during online assessments is a cause of anxiety, affecting student performance. An initial survey of 34 students across two Computer Science courses found that time-tracking devices produced anxiety for 67.7% of students. While students differed on timer color preference, a majority preferred a count-down display showing time remaining with the ability to hide the timer. In a small pilot study across five exams, we employed multiple time-tracking displays. Preliminary data suggests that students presented with a count-down grayscale timer performed better on average than those presented with a green-yellow-red (GYR) version. Other displays, such as text-only digital count-down timer or elapsed time progress bars, did not elicit as large a difference in performance. These findings indicate the need for further study.",
                "paper_link": "https://www.semanticscholar.org/paper/dc8899f858ccf6fae1f40e18a9a359fc6b70aee2"
            },
            {
                "title": "Open Source Software Practices in CS2",
                "abstract": "By contributing to open source software (OSS), students can gain professional software development experience and learn about applications of computer science (CS) concepts in pragmatic contexts. However, integrating such projects in classrooms requires substantial logistical planning by instructors as well as adequate programming skills from students. To mitigate these challenges, we propose four model curricula to serve as accessible strategies of integrating practicable learning opportunities in lower-level CS classes. Depending on classroom circumstances, instructors can assign projects that involve student contributions to OSS, custom plug-ins, simulated open source communities, or practical code excerpts. As a result, students will be able to explore the utility of CS and discover an exciting future in computing.",
                "paper_link": "https://www.semanticscholar.org/paper/ef20fcd50dd51369db6c6eb2a52bc7b6b20ceeec"
            },
            {
                "title": "Identity Lenses in Analyzing Evolving Social Structures.",
                "abstract": "In the effort to capture cultural dynamics, scholars have considered social networks, that is, a graph with people as nodes and their relationships as edges. These social networks are useful; however, to capture dynamics they must be considered over time. In the literature, Time-Varying Graphs (TVGs) [1, 3, 4] have been defined. In our investigations, we have found benefit in defining TVGs with nodes as social structures and people as the edges [5,6] and then considering the dynamics of the social structures evidenced in the TVGs. Here we consider two motivating applications for our extensions to TVGs: early Mormon marital structures and an arXiv.org citation network.",
                "paper_link": "https://www.semanticscholar.org/paper/6ed0be7592c7c76a1d6d7398a9d4a9a515d7770c"
            },
            {
                "title": "Visualizing and analyzing identity classes in evolving social structures",
                "abstract": "Social networks are usually represented as graphs with each person as a node and each social relationship as an edge. As frequently observed, if one considers social structures over time, those relationships, indeed even the people, come and go so that a static graph does not capture important aspects of the overall situation. Adding a temporal component to the graph results in specifying an evolving network [1, 3, 6, 7, 9\u201312, 20]. In formulating evolving networks for several motivating applications, we have found the need to extend those specifications. In particular, we consider an entity (or relationship) that evolves, that is, has characteristics that change over time: fundamental characteristics that determine the entity\u2019s identity. Our prime motivating application for our extensions is the marital structures of the mid-1800s Mormon church in Nauvoo, Illinois. However, we note that there are other applications for which we see the need for these extensions, including co-citation graphs and corporate communication networks. Represented in the Nauvoo dataset is a rich collection of marital and family structures, including polygynous and polyandrous marriages. The historical development of these social structures is the focus of our humanities research (as discussed in [15]). We represent the marriages as the nodes, with the edges between nodes representing the individuals participating in marriages; that is, an edge representing an individual will connect the marriage of their biological parents with their own adult marriage. Each of these marital structures, as well as the network as a whole, are considered to be evolving as marriages form and partners, children, and adoptees are brought in and leave. One form of the evolving network described above can be derived by stipulating that each node represents the marriage of exactly one woman and one man, i.e., the nodes are considered to be binary marriages, together with their offspring. However, for our research it is important to move beyond the conceptualization of binary marriages to focus on the richer familial interactions presented. To enable consideration of important aspects of this research, we introduce the concept of an \u201cidentity function\u201d that takes an evolving network and creates a new evolving network by applying criteria to determine what constitutes a node, i.e., determines the node identity. For example, if instead of each node being identified by the exact woman/man pair, we extended the identity to be based on the man, and include all the women married to him, a new evolving network of polygamous marital structures emerges. The identity function that transforms the binary-marriage network into this new network we call, Vpatriarchal. A similar identity function can be designed in which the identity of each node is based on a specific woman and includes the men married to her. We call that identity function, Vmatriarchal. Given the similarity of the criteria for these two identity functions, one might imagine that the two resulting evolving networks would stand in relation to one another much as a static graph and its dual [2]; however, that is not the case. We believe aspects of the differences in the network",
                "paper_link": "https://www.semanticscholar.org/paper/e1e6e45cd219210c60e770f6ac327871cb1b4c84"
            },
            {
                "title": "Analysis and Simulations of Incentives to Seed in BitTorrent",
                "abstract": "Peer-to-peer (P2P) networking has become the preferred method of file sharing among users. BitTorrent, one such network, has emerged as the leader in peer-topeer file sharing. BitTorrent has two classes of peers: seeders, those who share a file, and leechers, those who download a file. Seeders are the backbone of the BitTorrent network. There, is, however, little or no incentive to encourage peers to seed. We have developed a simulation of a P2P network based on the BitTorrent model. By implementing a variety of different incentive mechanisms within the framework of our simulation, we have analyzed the effectiveness of these incentives to seed. In addition, we have proposed a new incentive mechanism which has also been implemented within our simulation.",
                "paper_link": "https://www.semanticscholar.org/paper/d4a3588f6287c06a95840e1c1da1c46468d68df7"
            },
            {
                "title": "KD-Tree Algorithm for Propensity Score Matching With Three or More Treatment Groups",
                "abstract": "Propensity scores (PS) have been widely used in epidemiology to control for confounding bias in non-experimental comparative studies of drugs, but the technique of matching patients by score becomes computationally impractical with studies of three or more treatment groups. Imbens\u2019 generalized propensity score (GPS) provides a method for comparing multiple treatments through regression, weighting, and other approaches. We present a multi-category matching algorithm that matches patients across multiple groups under any number of normalized factors, including the PS and GPS. The algorithm\u2019s time complexity is expected-case quadratic on the number of participants per group. Utilizing kd-tree data structures to provide efficient queries for nearby points and a search radius related to a best-guess match between participants in each treatment group, we reduce the number of participants that must be considered for each matching. We then match patients by the k factors defined, balancing the distribution of confounders in each treatment group and thereby removing bias. Our algorithm outperforms the brute force matching approach in the expected case, requiring only O(n) space and O(n2) time compared with brute force\u2019s O(nk+1) time, for k treatment groups. This difference is clearly seen in our simulation study of 1000 participants in 3 groups: our algorithm matches on propensity score in 4.5 seconds compared to brute force\u2019s 17.5 hours, using commodity hardware available in 2012. In the vast majority of cases, we can accomplish matching with three or more treatment groups without the constraint of exponential growth of the search space. Considering four groups of 5,000 patients, that is a reduction from 625 trillion matches to 100 million and orders of magnitude shorter computation time.",
                "paper_link": "https://www.semanticscholar.org/paper/bf80284709d5d2599844b24690f0363a5c9e9fd6"
            }
        ]
    },
    {
        "Professor": "Yangfeng Ji",
        "Papers": [
            {
                "title": "A Neural Network Approach to Context-Sensitive Generation of Conversational Responses",
                "abstract": "We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations. A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.",
                "paper_link": "https://www.semanticscholar.org/paper/5247a6e3a60ff0381355e66bfc313bf27512ae0c"
            },
            {
                "title": "Dynet: The dynamic neural network toolkit",
                "abstract": "We describe DyNet, a toolkit for implementing neural network models based on dynamic declaration of network structure. In the static declaration strategy that is used in toolkits like Theano, CNTK, and TensorFlow, the user first defines a computation graph (a symbolic representation of the computation), and then examples are fed into an engine that executes this computation and computes its derivatives. In DyNet's dynamic declaration strategy, computation graph construction is mostly transparent, being implicitly constructed by executing procedural code that computes the network outputs, and the user is free to use different network structures for each input. Dynamic declaration thus facilitates the implementation of more complicated network architectures, and DyNet is specifically designed to allow users to implement their models in a way that is idiomatic in their preferred programming language (C++ or Python). One challenge with dynamic declaration is that because the symbolic computation graph is defined anew for every training example, its construction must have low overhead. To achieve this, DyNet has an optimized C++ backend and lightweight graph representation. Experiments show that DyNet's speeds are faster than or comparable with static declaration toolkits, and significantly faster than Chainer, another dynamic declaration toolkit. DyNet is released open-source under the Apache 2.0 license and available at this http URL.",
                "paper_link": "https://www.semanticscholar.org/paper/480d545ac4a4ffff5b1bc291c2de613192e35d91"
            },
            {
                "title": "Representation Learning for Text-level Discourse Parsing",
                "abstract": "Text-level discourse parsing is notoriously difficult, as distinctions between discourse relations require subtle semantic judgments that are not easily captured using standard features. In this paper, we present a representation learning approach, in which we transform surface features into a latent space that facilitates RST discourse parsing. By combining the machinery of large-margin transition-based structured prediction with representation learning, our method jointly learns to parse discourse while at the same time learning a discourse-driven projection of surface features. The resulting shift-reduce discourse parser obtains substantial improvements over the previous state-of-the-art in predicting relations and nuclearity on the RST Treebank.",
                "paper_link": "https://www.semanticscholar.org/paper/9204d5b82652ee69859b6de56eb9a189a458c97c"
            },
            {
                "title": "Creative writing with a machine in the loop: Case studies on slogans and stories",
                "abstract": "As the quality of natural language generated by artificial intelligence systems improves, writing interfaces can support interventions beyond grammar-checking and spell-checking, such as suggesting content to spark new ideas. To explore the possibility of machine-in-the-loop creative writing, we performed two case studies using two system prototypes, one for short story writing and one for slogan writing. Participants in our studies were asked to write with a machine in the loop or alone (control condition). They assessed their writing and experience through surveys and an open-ended interview. We collected additional assessments of the writing from Amazon Mechanical Turk crowdworkers. Our findings indicate that participants found the process fun and helpful and could envision use cases for future systems. At the same time, machine suggestions do not necessarily lead to better written artifacts. We therefore suggest novel natural language models and design choices that may better support creative writing.",
                "paper_link": "https://www.semanticscholar.org/paper/7c9f4bece05a3c44f9c6daf65fc33f16ed571e12"
            },
            {
                "title": "Better Document-level Sentiment Analysis from RST Discourse Parsing",
                "abstract": "Discourse structure is the hidden link between surface features and document-level properties, such as sentiment polarity. We show that the discourse analyses produced by Rhetorical Structure Theory (RST) parsers can improve document-level sentiment analysis, via composition of local information up the discourse tree. First, we show that reweighting discourse units according to their position in a dependency representation of the rhetorical structure can yield substantial improvements on lexicon-based sentiment analysis. Next, we present a recursive neural network over the RST structure, which offers significant improvements over classificationbased methods.",
                "paper_link": "https://www.semanticscholar.org/paper/93547ca277f93ae8cde856f66bf54166d64b4dcf"
            },
            {
                "title": "Discriminative Improvements to Distributional Sentence Similarity",
                "abstract": "Matrix and tensor factorization have been applied to a number of semantic relatedness tasks, including paraphrase identification. The key idea is that similarity in the latent space implies semantic relatedness. We describe three ways in which labeled data can improve the accuracy of these approaches on paraphrase classification. First, we design a new discriminative term-weighting metric called TF-KLD, which outperforms TF-IDF. Next, we show that using the latent representation from matrix factorization as features in a classification algorithm substantially improves accuracy. Finally, we combine latent features with fine-grained n-gram overlap features, yielding performance that is 3% more accurate than the prior state-of-the-art.",
                "paper_link": "https://www.semanticscholar.org/paper/0e5fa90e28fab414c8ef3ac6ca937c6195c2860e"
            },
            {
                "title": "deltaBLEU: A discriminative metric for generation tasks with intrinsically diverse targets",
                "abstract": "We introduce Discriminative BLEU (\u2206BLEU), a novel metric for intrinsic evaluation of generated text in tasks that admit a diverse range of possible outputs. Reference strings are scored for quality by human raters on a scale of [\u22121, +1] to weight multi-reference BLEU. In tasks involving generation of conversational responses, \u2206BLEU correlates reasonably with human judgments and outperforms sentence-level and IBM BLEU in terms of both Spearman\u2019s \u03c1 and Kendall\u2019s \u03c4 .",
                "paper_link": "https://www.semanticscholar.org/paper/7d0684b19ba46e739e28baa1e180c008226f793a"
            },
            {
                "title": "One Vector is Not Enough: Entity-Augmented Distributed Semantics for Discourse Relations",
                "abstract": "Discourse relations bind smaller linguistic units into coherent texts. Automatically identifying discourse relations is difficult, because it requires understanding the semantics of the linked arguments. A more subtle challenge is that it is not enough to represent the meaning of each argument of a discourse relation, because the relation may depend on links between lowerlevel components, such as entity mentions. Our solution computes distributed meaning representations for each discourse argument by composition up the syntactic parse tree. We also perform a downward compositional pass to capture the meaning of coreferent entity mentions. Implicit discourse relations are then predicted from these two representations, obtaining substantial improvements on the Penn Discourse Treebank.",
                "paper_link": "https://www.semanticscholar.org/paper/635e830502319b14025594f225848fa12b80d802"
            },
            {
                "title": "A latent variable recurrent neural network for discourse relation language models",
                "abstract": "This paper presents a novel latent variable recurrent neural network architecture for jointly modeling sequences of words and (possibly latent) discourse relations between adjacent sentences. A recurrent neural network generates individual words, thus reaping the benefits of discriminatively-trained vector representations. The discourse relations are represented with a latent variable, which can be predicted or marginalized, depending on the task. The resulting model can therefore employ a training objective that includes not only discourse relation classification, but also word prediction. As a result, it outperforms state-of-the-art alternatives for two tasks: implicit discourse relation classification in the Penn Discourse Treebank, and dialog act classification in the Switchboard corpus. Furthermore, by marginalizing over latent discourse relations at test time, we obtain a discourse informed language model, which improves over a strong LSTM baseline.",
                "paper_link": "https://www.semanticscholar.org/paper/3dfd76e4b5083ee71049099c3105e8af69edb44f"
            },
            {
                "title": "Neural discourse structure for text categorization",
                "abstract": "We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization. Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task. Experiments consider variants of the approach and illustrate its strengths and weaknesses.",
                "paper_link": "https://www.semanticscholar.org/paper/fe3bd7c7b36cc563c1bd54189b963e3244b74580"
            },
            {
                "title": "Neural text generation in stories using entity representations as context",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "The gem benchmark: Natural language generation, its evaluation and metrics",
                "abstract": "We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent anglo-centric corpora with well-established, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of tasks and in which evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for the 2021 shared task at the associated GEM Workshop.",
                "paper_link": "https://www.semanticscholar.org/paper/824cd8db8a68732db04f4d8b7139eb4475e59ff2"
            },
            {
                "title": "Extracting Lexically Divergent Paraphrases from Twitter",
                "abstract": "We present MultiP (Multi-instance Learning Paraphrase Model), a new model suited to identify paraphrases within the short messages on Twitter. We jointly model paraphrase relations between word and sentence pairs and assume only sentence-level annotations during learning. Using this principled latent variable model alone, we achieve the performance competitive with a state-of-the-art method which combines a latent space model with a feature-based supervised classifier. Our model also captures lexically divergent paraphrases that differ from yet complement previous methods; combining our model with previous work significantly outperforms the state-of-the-art. In addition, we present a novel annotation methodology that has allowed us to crowdsource a paraphrase corpus from Twitter. We make this new dataset available to the research community.",
                "paper_link": "https://www.semanticscholar.org/paper/8dfa00730fafe35bb19b71a8b96be019348304c6"
            },
            {
                "title": "Dynamic entity representations in neural language models",
                "abstract": "Understanding a long document requires tracking how entities are introduced and evolve over time. We present a new type of language model, EntityNLM, that can explicitly model entities, dynamically update their representations, and contextually generate their mentions. Our model is generative and flexible; it can model an arbitrary number of entities in context while generating each entity mention at an arbitrary length. In addition, it can be used for several different tasks such as language modeling, coreference resolution, and entity prediction. Experimental results with all these tasks demonstrate that our model consistently outperforms strong baselines and prior work.",
                "paper_link": "https://www.semanticscholar.org/paper/9c81f16df774c772dbefc947fe0e467b72500844"
            },
            {
                "title": "Reevaluating adversarial examples in natural language",
                "abstract": "State-of-the-art attacks on NLP models lack a shared definition of a what constitutes a successful attack. We distill ideas from past work into a unified framework: a successful natural language adversarial example is a perturbation that fools the model and follows some linguistic constraints. We then analyze the outputs of two state-of-the-art synonym substitution attacks. We find that their perturbations often do not preserve semantics, and 38% introduce grammatical errors. Human surveys reveal that to successfully preserve semantics, we need to significantly increase the minimum cosine similarities between the embeddings of swapped words and between the sentence encodings of original and perturbed sentences.With constraints adjusted to better preserve semantics and grammaticality, the attack success rate drops by over 70 percentage points.",
                "paper_link": "https://www.semanticscholar.org/paper/2baba0ad8162a7a3e93916619fff52ad8ff47c73"
            },
            {
                "title": "Generating hierarchical explanations on text classification via feature interaction detection",
                "abstract": "Generating explanations for neural networks has become crucial for their applications in real-world with respect to reliability and trustworthiness. In natural language processing, existing methods usually provide important features which are words or phrases selected from an input text as an explanation, but ignore the interactions between them. It poses challenges for humans to interpret an explanation and connect it to model prediction. In this work, we build hierarchical explanations by detecting feature interactions. Such explanations visualize how words and phrases are combined at different levels of the hierarchy, which can help users understand the decision-making of black-box models. The proposed method is evaluated with three neural text classifiers (LSTM, CNN, and BERT) on two benchmark datasets, via both automatic and human evaluations. Experiments show the effectiveness of the proposed method in providing explanations that are both faithful to models and interpretable to humans.",
                "paper_link": "https://www.semanticscholar.org/paper/1657220981714a6c312b364dbb51d604521f894e"
            },
            {
                "title": "Hitter: Hierarchical transformers for knowledge graph embeddings",
                "abstract": "This paper examines the challenging problem of learning representations of entities and relations in a complex multi-relational knowledge graph. We propose HittER, a Hierarchical Transformer model to jointly learn Entity-relation composition and Relational contextualization based on a source entity\u2019s neighborhood. Our proposed model consists of two different Transformer blocks: the bottom block extracts features of each entity-relation pair in the local neighborhood of the source entity and the top block aggregates the relational information from outputs of the bottom block. We further design a masked entity prediction task to balance information from the relational context and the source entity itself. Experimental results show that HittER achieves new state-of-the-art results on multiple link prediction datasets. We additionally propose a simple approach to integrate HittER into BERT and demonstrate its effectiveness on two Freebase factoid question answering datasets.",
                "paper_link": "https://www.semanticscholar.org/paper/7e7499b47fe57033768f26ef98a3b644688eb2a2"
            },
            {
                "title": "Document context language models",
                "abstract": "Text documents are structured on multiple levels of detail: individual words are related by syntax, but larger units of text are related by discourse structure. Existing language models generally fail to account for discourse structure, but it is crucial if we are to have language models that reward coherence and generate coherent texts. We present and empirically evaluate a set of multi-level recurrent neural network language models, called Document-Context Language Models (DCLM), which incorporate contextual information both within and beyond the sentence. In comparison with word-level recurrent neural network language models, the DCLM models obtain slightly better predictive likelihoods, and considerably better assessments of document coherence.",
                "paper_link": "https://www.semanticscholar.org/paper/961073143d3cfe662e9e820d24c0a88f0ae94c83"
            },
            {
                "title": "Context-sensitive generation of conversational responses",
                "abstract": "We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations. A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.",
                "paper_link": "https://www.semanticscholar.org/paper/5247a6e3a60ff0381355e66bfc313bf27512ae0c"
            },
            {
                "title": "LSTM based conversation models",
                "abstract": "In this paper, we present a conversational model that incorporates both context and participant role for two-party conversations. Different architectures are explored for integrating participant role and context information into a Long Short-term Memory (LSTM) language model. The conversational model can function as a language model or a language generation model. Experiments on the Ubuntu Dialog Corpus show that our model can capture multiple turn interaction between participants. The proposed method outperforms a traditional LSTM model as measured by language model perplexity and response ranking. Generated responses show characteristic differences between the two participant roles.",
                "paper_link": "https://www.semanticscholar.org/paper/72ae4bba9aaa30dfba45f6e7e076952a76e2d751"
            }
        ]
    },
    {
        "Professor": "Adwait Jog",
        "Papers": [
            {
                "title": "OWL: Cooperative Thread Array Aware Scheduling Techniques for Improving GPGPU Performance",
                "abstract": "Emerging GPGPU architectures, along with programming models like CUDA and OpenCL, offer a cost-effective platform for many applications by providing high thread level parallelism at lower energy budgets. Unfortunately, for many general-purpose applications, available hardware resources of a GPGPU are not efficiently utilized, leading to lost opportunity in improving performance. A major cause of this is the inefficiency of current warp scheduling policies in tolerating long memory latencies.\n In this paper, we identify that the scheduling decisions made by such policies are agnostic to thread-block, or cooperative thread array (CTA), behavior, and as a result inefficient. We present a coordinated CTA-aware scheduling policy that utilizes four schemes to minimize the impact of long memory latencies. The first two schemes, CTA-aware two-level warp scheduling and locality aware warp scheduling, enhance per-core performance by effectively reducing cache contention and improving latency hiding capability. The third scheme, bank-level parallelism aware warp scheduling, improves overall GPGPU performance by enhancing DRAM bank-level parallelism. The fourth scheme employs opportunistic memory-side prefetching to further enhance performance by taking advantage of open DRAM rows. Evaluations on a 28-core GPGPU platform with highly memory-intensive applications indicate that our proposed mechanism can provide 33% average performance improvement compared to the commonly-employed round-robin warp scheduling policy.",
                "paper_link": "https://www.semanticscholar.org/paper/1eeb50d5f7937f65a910203ae61430ff8b969012"
            },
            {
                "title": "Cache Revive: Architecting Volatile STT-RAM Caches for Enhanced Performance in CMPs",
                "abstract": "High density, low leakage and non-volatility are the attractive features of Spin-Transfer-Torque-RAM (STT-RAM), which has made it a strong competitor against SRAM as a universal memory replacement in multi-core systems. However, STT-RAM suffers from high write latency and energy which has impeded its widespread adoption. To this end, we look at trading-off STT-RAM's non-volatility property (data-retention-time) to overcome these problems. We formulate the relationship between retention-time and write-latency, and find optimal retention-time for architecting an efficient cache hierarchy using STT-RAM. Our results show that, compared to SRAM-based design, our proposal can improve performance and energy consumption by 18% and 60%, respectively.",
                "paper_link": "https://www.semanticscholar.org/paper/a7ae6f24908972df89b9cbc3bb657457c35cdb37"
            },
            {
                "title": "Neither More Nor Less: Optimizing Thread-level Parallelism for GPGPUs",
                "abstract": "General-purpose graphics processing units (GPG-PUs) are at their best in accelerating computation by exploiting abundant thread-level parallelism (TLP) offered by many classes of HPC applications. To facilitate such high TLP, emerging programming models like CUDA and OpenCL allow programmers to create work abstractions in terms of smaller work units, called cooperative thread arrays (CTAs). CTAs are groups of threads and can be executed in any order, thereby providing ample opportunities for TLP. The state-of-the-art GPGPU schedulers allocate maximum possible CTAs per-core (limited by available on-chip resources) to enhance performance by exploiting TLP. However, we demonstrate in this paper that executing the maximum possible number of CTAs on a core is not always the optimal choice from the performance perspective. High number of concurrently executing threads might cause more memory requests to be issued, and create contention in the caches, network and memory, leading to long stalls at the cores. To reduce resource contention, we propose a dynamic CTA scheduling mechanism, called DYNCTA, which modulates the TLP by allocating optimal number of CTAs, based on application characteristics. To minimize resource contention, DYNCTA allocates fewer CTAs for applications suffering from high contention in the memory subsystem, compared to applications demonstrating high throughput. Simulation results on a 30-core GPGPU platform with 31 applications show that the proposed CTA scheduler provides 28% average improvement in performance compared to the existing CTA scheduler.",
                "paper_link": "https://www.semanticscholar.org/paper/e65e20502408756f728e50516c06284368bfd6cb"
            },
            {
                "title": "Orchestrated Scheduling and Prefetching for GPGPUs",
                "abstract": "In this paper, we present techniques that coordinate the thread scheduling and prefetching decisions in a General Purpose Graphics Processing Unit (GPGPU) architecture to better tolerate long memory latencies. We demonstrate that existing warp scheduling policies in GPGPU architectures are unable to effectively incorporate data prefetching. The main reason is that they schedule consecutive warps, which are likely to access nearby cache blocks and thus prefetch accurately for one another, back-to-back in consecutive cycles. This either 1) causes prefetches to be generated by a warp too close to the time their corresponding addresses are actually demanded by another warp, or 2) requires sophisticated prefetcher designs to correctly predict the addresses required by a future \"far-ahead\" warp while executing the current warp. We propose a new prefetch-aware warp scheduling policy that overcomes these problems. The key idea is to separate in time the scheduling of consecutive warps such that they are not executed back-to-back. We show that this policy not only enables a simple prefetcher to be effective in tolerating memory latencies but also improves memory bank parallelism, even when prefetching is not employed. Experimental evaluations across a diverse set of applications on a 30-core simulated GPGPU platform demonstrate that the prefetch-aware warp scheduler provides 25% and 7% average performance improvement over baselines that employ prefetching in conjunction with, respectively, the commonly-employed round-robin scheduler or the recently-proposed two-level warp scheduler. Moreover, when prefetching is not employed, the prefetch-aware warp scheduler provides higher performance than both of these baseline schedulers as it better exploits memory bank parallelism.",
                "paper_link": "https://www.semanticscholar.org/paper/0036adadc90e4826b2f7fc157752eea459070c32"
            },
            {
                "title": "Scheduling techniques for GPU architectures with processing-in-memory capabilities",
                "abstract": "Processing data in or near memory (PIM), as opposed to in conventional computational units in a processor, can greatly alleviate the performance and energy penalties of data transfers from/to main memory. Graphics Processing Unit (GPU) architectures and applications, where main memory bandwidth is a critical bottleneck, can benefit from the use of PIM. To this end, an application should be properly partitioned and scheduled to execute on either the main, powerful GPU cores that are far away from memory or the auxiliary, simple GPU cores that are close to memory (e.g., in the logic layer of 3D-stacked DRAM). This paper investigates two key code scheduling issues in such a GPU architecture that has PIM capabilities, to maximize performance and energy-efficiency: (1) how to automatically identify the code segments, or kernels, to be offloaded to the cores in memory, and (2) how to concurrently schedule multiple kernels on the main GPU cores and the auxiliary GPU cores in memory. We develop two new runtime techniques: (1) a regression-based affinity prediction model and mechanism that accurately identifies which kernels would benefit from PIM and offloads them to GPU cores in memory, and (2) a concurrent kernel management mechanism that uses the affinity prediction model, a new kernel execution time prediction model, and kernel dependency information to decide which kernels to schedule concurrently on main GPU cores and the GPU cores in memory. Our experimental evaluations across 25 GPU applications demonstrate that these two techniques can significantly improve both application performance (by 25% and 42%, respectively, on average) and energy efficiency (by 28% and 27%).",
                "paper_link": "https://www.semanticscholar.org/paper/1da50afbe8e9309ed0e8cb4b8e2638b96efaead8"
            },
            {
                "title": "Managing GPU Concurrency in Heterogeneous Architectures",
                "abstract": "Heterogeneous architectures consisting of general-purpose CPUs and throughput-optimized GPUs are projected to be the dominant computing platforms for many classes of applications. The design of such systems is more complex than that of homogeneous architectures because maximizing resource utilization while minimizing shared resource interference between CPU and GPU applications is difficult. We show that GPU applications tend to monopolize the shared hardware resources, such as memory and network, because of their high thread-level parallelism (TLP), and discuss the limitations of existing GPU-based concurrency management techniques when employed in heterogeneous systems. To solve this problem, we propose an integrated concurrency management strategy that modulates the TLP in GPUs to control the performance of both CPU and GPU applications. This mechanism considers both GPU core state and system-wide memory and network congestion information to dynamically decide on the level of GPU concurrency to maximize system performance. We propose and evaluate two schemes: one (CM-CPU) for boosting CPU performance in the presence of GPU interference, the other (CM-BAL) for improving both CPU and GPU performance in a balanced manner and thus overall system performance. Our evaluations show that the first scheme improves average CPU performance by 24%, while reducing average GPU performance by 11%. The second scheme provides 7% average performance improvement for both CPU and GPU applications. We also show that our solution allows the user to control performance trade-offs between CPUs and GPUs.",
                "paper_link": "https://www.semanticscholar.org/paper/9bd5d02b18da6fe877fdee359c0813a5b02aface"
            },
            {
                "title": "A case for core-assisted bottleneck acceleration in GPUs: enabling flexible data compression with assist warps",
                "abstract": "Modern Graphics Processing Units (CPUs) are well provisioned to support the concurrent execution of thousands of threads. Unfortunately, different bottlenecks during execution and heterogeneous application requirements create imbalances in utilization of resources in the cores. For example, when a CPU is bottle necked by the available off-chip memory bandwidth, its computational resources are often overwhelmingly idle, waiting for data from memory to arrive. This paper introduces the Core-Assisted Bottleneck Acceleration (CABA) framework that employs idle on-chip resources to alleviate different bottlenecks in CPU execution. CABA provides flexible mechanisms to automatically generate \"assist warps\" that execute on CPU cores to perform specific tasks that can improve CPU performance and efficiency. CABA enables the use of idle computational units and pipelines to alleviate the memory bandwidth bottleneck, e.g., by using assist warps to perform data compression to transfer less data from memory. Conversely, the same framework can be employed to handle cases where the CPU is bottlenecked by the available computational units, in which case the memory pipelines are idle and can be used by CABA to speed up computation, e.g., by performing memoization using assist warps. We provide a comprehensive design and evaluation of CABA to perform effective and flexible data compression in the CPU memory hierarchy to alleviate the memory bandwidth bottleneck. Our extensive evaluations show that CABA, when used to implement data compression, provides an average performance improvement of 41.7% (as high as 2.6X) across a variety of memory-bandwidth-sensitive GPGPU applications.",
                "paper_link": "https://www.semanticscholar.org/paper/1f1a1f0cd075cef63083c8ec15321021dbff2cfc"
            },
            {
                "title": "Anatomy of GPU Memory System for Multi-Application Execution",
                "abstract": "As GPUs make headway in the computing landscape spanning mobile platforms, supercomputers, cloud and virtual desktop platforms, supporting concurrent execution of multiple applications in GPUs becomes essential for unlocking their full potential. However, unlike CPUs, multi-application execution in GPUs is little explored. In this paper, we study the memory system of GPUs in a concurrently executing multi-application environment. We first present an analytical performance model for many-threaded architectures and show that the common use of misses-per-kilo-instruction (MPKI) as a proxy for performance is not accurate without considering the bandwidth usage of applications. We characterize the memory interference of applications and discuss the limitations of existing memory schedulers in mitigating this interference. We extend the analytical model to multiple applications and identify the key metrics to control various performance metrics. We conduct extensive simulations using an enhanced version of GPGPU-Sim targeted for concurrently executing multiple applications, and show that memory scheduling decisions based on MPKI and bandwidth information are more effective in enhancing throughput compared to the traditional FR-FCFS and the recently proposed RR FR-FCFS policies.",
                "paper_link": "https://www.semanticscholar.org/paper/174b4cb435c87e421c973ce59ccf5b06e09aa8af"
            },
            {
                "title": "Mask: Redesigning the GPU Memory Hierarchy to Support Multi-application Concurrency",
                "abstract": "Graphics Processing Units (GPUs) exploit large amounts of threadlevel parallelism to provide high instruction throughput and to efficiently hide long-latency stalls. The resulting high throughput, along with continued programmability improvements, have made GPUs an essential computational resource in many domains. Applications from different domains can have vastly different compute and memory demands on the GPU. In a large-scale computing environment, to efficiently accommodate such wide-ranging demands without leaving GPU resources underutilized, multiple applications can share a single GPU, akin to how multiple applications execute concurrently on a CPU. Multi-application concurrency requires several support mechanisms in both hardware and software. One such key mechanism is virtual memory, which manages and protects the address space of each application. However, modern GPUs lack the extensive support for multi-application concurrency available in CPUs, and as a result suffer from high performance overheads when shared by multiple applications, as we demonstrate. We perform a detailed analysis of which multi-application concurrency support limitations hurt GPU performance the most. We find that the poor performance is largely a result of the virtual memory mechanisms employed in modern GPUs. In particular, poor address translation performance is a key obstacle to efficient GPU sharing. State-of-the-art address translation mechanisms, which were designed for single-application execution, experience significant inter-application interference when multiple applications spatially share the GPU. This contention leads to frequent misses in the shared translation lookaside buffer (TLB), where a single miss can induce long-latency stalls for hundreds of threads. As a result, the GPU often cannot schedule enough threads to successfully hide the stalls, which diminishes system throughput and becomes a first-order performance concern. Based on our analysis, we propose MASK, a new GPU framework that provides low-overhead virtual memory support for the concurrent execution of multiple applications. MASK consists of three novel address-translation-aware cache and memory management mechanisms that work together to largely reduce the overhead of address translation: (1) a token-based technique to reduce TLB contention, (2) a bypassing mechanism to improve the effectiveness of cached address translations, and (3) an application-aware memory scheduling scheme to reduce the interference between address translation and data requests. Our evaluations show that MASK restores much of the throughput lost to TLB contention. Relative to a state-of-the-art GPU TLB, MASK improves system throughput by 57.8%, improves IPC throughput by 43.4%, and reduces applicationlevel unfairness by 22.4%. MASK's system throughput is within 23.2% of an ideal GPU system with no address translation overhead.",
                "paper_link": "https://www.semanticscholar.org/paper/636a4d1dcd7f1cfdd41db52b6c5de6c527b183e7"
            },
            {
                "title": "Application-aware Memory System for Fair and Efficient Execution of Concurrent GPGPU Applications",
                "abstract": "The available computing resources in modern GPUs are growing with each new generation. However, as many general purpose applications with limited thread-scalability are tuned to take advantage of GPUs, available compute resources might not be optimally utilized. To address this, modern GPUs will need to execute multiple kernels simultaneously. As current generations of GPUs (e.g., NVIDIA Kepler, AMD Radeon) already enable concurrent execution of kernels from the same application, in this paper we address the next logical step: executing multiple concurrent applications in GPUs. We show that while this paradigm has a potential to improve the overall system performance, negative interactions among concurrently executing applications in the memory system can severely hamper the performance and fairness among applications. We show that the current application agnostic GPU memory system design can (1) lead to sub-optimal GPU performance; and (2) create significant imbalance in performance slowdowns across kernels. Thus, we argue that GPU memory system should be augmented with application awareness. As one example to the applicability of this concept, we augment the memory system hardware with application awareness such that requests from different applications can be scheduled in a round robin (RR) fashion while still preserving the benefits of the current first-ready FCFS (FR-FCFS) memory scheduling policy. Evaluations with different multi-application workloads demonstrate that the proposed memory scheduling policy, first-ready round-robin FCFS (FR-RR-FCFS), improves fairness and delivers better system performance compared to the existing FR-FCFS memory scheduling scheme.",
                "paper_link": "https://www.semanticscholar.org/paper/a2304dca58992acd50b1306827bcab00efbef579"
            },
            {
                "title": "Zorua: A holistic approach to resource virtualization in GPUs",
                "abstract": "This paper introduces a new resource virtualization framework, Zorua, that decouples the programmer-specified resource usage of a GPU application from the actual allocation in the on-chip hardware resources. Zorua enables this decoupling by virtualizing each resource transparently to the programmer. The virtualization provided by Zorua builds on two key concepts - dynamic allocation of the on-chip resources and their oversubscription using a swap space in memory. Zorua provides a holistic GPU resource virtualization strategy, designed to (i) adaptively control the extent of oversubscription, and (ii) coordinate the dynamic management of multiple on-chip resources (i.e., registers, scratchpad memory, and thread slots), to maximize the effectiveness of virtualization. Zorua employs a hardware-software code-sign, comprising the compiler, a runtime system and hardware-based virtualization support. The runtime system leverages information from the compiler regarding resource requirements of each program phase to (i) dynamically allocate/deallocate the different resources in the physically available on-chip resources or their swap space, and (ii) manage the tradeoffbetween higher thread-level parallelism due to virtualization versus the latency and capacity overheads of swap space usage. We demonstrate that by providing the illusion of more resources than physically available via controlled and coordinated virtualization, Zorua offers several important benefits: (i) Programming Ease. Zorua eases the burden on the programmer to provide code that is tuned to efficiently utilize the physically available on-chip resources. (ii) Portability. Zorua alleviates the necessity of re-tuning an application's resource usage when porting the application across GPU generations. (iii) Performance. By dynamically allocating resources and carefully oversubscribing them when necessary, Zorua improves or retains the performance of applications that are already highly tuned to best utilize the hardware resources. The holistic virtualization provided by Zorua can also enable other uses, including fine-grained resource sharing among multiple kernels and low-latency preemption of GPU programs.",
                "paper_link": "https://www.semanticscholar.org/paper/ac19760ed804ea46e769034fd98a2c8ce211464d"
            },
            {
                "title": "Exploiting core criticality for enhanced GPU performance",
                "abstract": "Modern memory access schedulers employed in GPUs typically optimize for memory throughput. They implicitly assume that all requests from different cores are equally important. However, we show that during the execution of a subset of CUDA applications, different cores can have different amounts of tolerance to latency. In particular, cores with a larger fraction of warps waiting for data to come back from DRAM are less likely to tolerate the latency of an outstanding memory request. Requests from such cores are more critical than requests from others. Based on this observation, this paper introduces a new memory scheduler, called (C)ritica(L)ity (A)ware (M)emory (S)cheduler (CLAMS), which takes into account the latency-tolerance of the cores that generate memory requests. The key idea is to use the fraction of critical requests in the memory request buffer to switch between scheduling policies optimized for criticality and locality. If this fraction is below a threshold, CLAMS prioritizes critical requests to ensure cores that cannot tolerate latency are serviced faster. Otherwise, CLAMS optimizes for locality, anticipating that there are too many critical requests and prioritizing one over another would not significantly benefit performance. We first present a core-criticality estimation mechanism for determining critical cores and requests, and then discuss issues related to finding a balance between criticality and locality in the memory scheduler. We progressively devise three variants of CLAMS, and show that the Dynamic CLAMS provides significantly higher performance, across a variety of workloads, than the commonly-employed GPU memory schedulers optimized solely for locality. The results indicate that a GPU memory system that considers both core criticality and DRAM access locality can provide significant improvement in performance.",
                "paper_link": "https://www.semanticscholar.org/paper/e78088474a45517b25c15ef26af7a79d6d23ffa1"
            },
            {
                "title": "Controlled Kernel Launch for Dynamic Parallelism in GPUs",
                "abstract": "Dynamic parallelism (DP) is a promising feature for GPUs, which allows on-demand spawning of kernels on the GPU without any CPU intervention. However, this feature has two major drawbacks. First, the launching of GPU kernels can incur significant performance penalties. Second, dynamically-generated kernels are not always able to efficiently utilize the GPU cores due to hardware-limits. To address these two concerns cohesively, we propose SPAWN, a runtime framework that controls the dynamically-generated kernels, thereby directly reducing the associated launch overheads and queuing latency. Moreover, it allows a better mix of dynamically-generated and original (parent) kernels for the scheduler to effectively hide the remaining overheads and improve the utilization of the GPU resources. Our results show that, across 13 benchmarks, SPAWN achieves 69% and 57% speedup over the flat (non-DP) implementation and baseline DP, respectively.",
                "paper_link": "https://www.semanticscholar.org/paper/80ee7ed9758de53db1e1d6334edbe6178fcaa49c"
            },
            {
                "title": "Fault Site Pruning for Practical Reliability Analysis of GPGPU Applications",
                "abstract": "Graphics Processing Units (GPUs) have rapidly evolved to enable energy-efficient data-parallel computing for a broad range of scientific areas. While GPUs achieve exascale performance at a stringent power budget, they are also susceptible to soft errors, often caused by high-energy particle strikes, that can significantly affect the application output quality. Understanding the resilience of general purpose GPU applications is the purpose of this study. To this end, it is imperative to explore the range of application output by injecting faults at all the potential fault sites. This problem is especially challenging because unlike CPU applications, which are mostly single-threaded, GPGPU applications can contain hundreds to thousands of threads, resulting in a tremendously large fault site space \u2013 in the order of billions even for some simple applications. In this paper, we present a systematic way to progressively prune the fault site space aiming to dramatically reduce the number of fault injections such that assessment for GPGPU application error resilience can be practical. The key insight behind our proposed methodology stems from the fact that GPGPU applications spawn a lot of threads, however, many of them execute the same set of instructions. Therefore, several fault sites are redundant and can be pruned by a careful analysis of faults across threads and instructions. We identify important features across a set of 10 applications (16 kernels) from Rodinia and Polybench suites and conclude that threads can be first classified based on the number of the dynamic instructions they execute. We achieve significant fault site reduction by analyzing only a small subset of threads that are representative of the dynamic instruction behavior (and therefore error resilience behavior) of the GPGPU applications. Further pruning is achieved by identifying and analyzing: a) the dynamic instruction commonalities (and differences) across code blocks within this representative set of threads, b) a subset of loop iterations within the representative threads, and c) a subset of destination register bit positions. The above steps result in a tremendous reduction of fault sites by up to seven orders of magnitude. Yet, this reduced fault site space accurately captures the error resilience profile of GPGPU applications.",
                "paper_link": "https://www.semanticscholar.org/paper/b428927a54d4855a946c3562fc6d45158d89c480"
            },
            {
                "title": "Opportunistic Computing in GPU Architectures",
                "abstract": "Data transfer overhead between computing cores and memory hierarchy has been a persistent issue for von Neumann architectures and the problem has only become more challenging with the emergence of manycore systems. A conceptually powerful approach to mitigate this overhead is to bring the computation closer to data, known as Near Data Computing (NDC). Recently, NDC has been investigated in different flavors for CPU-based multicores, while the GPU domain has received little attention. In this paper, we present a novel NDC solution for GPU architectures with the objective of minimizing on-chip data transfer between the computing cores and Last-Level Cache (LLC). To achieve this, we first identify frequently occurring Load-Compute- Store instruction chains in GPU applications. These chains, when offloaded to a compute unit closer to where the data resides, can significantly reduce data movement. We develop two offloading techniques, called LLC-Compute and Omni- Compute. The first technique, LLC-Compute, augments the LLCs with computational hardware for handling the computation offloaded to them. The second technique (Omni- Compute) employs simple bookkeeping hardware to enable GPU cores to compute instructions offloaded by other GPU cores. Our experimental evaluations on nine GPGPU workloads indicate that the LLC-Compute technique provides, on an average, 19% performance improvement (IPC), 11% per- formance/watt improvement, and 29% reduction in on-chip data movement compared to the baseline GPU design. The Omni-Compute design boosts these benefits to 31%, 16% and 44%, respectively.",
                "paper_link": "https://www.semanticscholar.org/paper/3913072f0bfaa3fb23dec8f5bbafb62308693763"
            },
            {
                "title": "Efficient and Fair Multi-programming in GPUs via Effective Bandwidth Management",
                "abstract": "Managing the thread-level parallelism (TLP) of GPGPU applications by limiting it to a certain degree is known to be effective in improving the overall performance. However, we find that such prior techniques can lead to sub-optimal system throughput and fairness when two or more applications are co-scheduled on the same GPU. It is because they attempt to maximize the performance of individual applications in isolation, ultimately allowing each application to take a disproportionate amount of shared resources. This leads to high contention in shared cache and memory. To address this problem, we propose new application-aware TLP management techniques for a multi-application execution environment such that all co-scheduled applications can make good and judicious use of all the shared resources. For measuring such use, we propose an application-level utility metric, called effective bandwidth, which accounts for two runtime metrics: attained DRAM bandwidth and cache miss rates. We find that maximizing the total effective bandwidth and doing so in a balanced fashion across all co-located applications can significantly improve the system throughput and fairness. Instead of exhaustively searching across all the different combinations of TLP configurations that achieve these goals, we find that a significant amount of overhead can be reduced by taking advantage of the trends, which we call patterns, in the way application's effective bandwidth changes with different TLP combinations. Our proposed pattern-based TLP management mechanisms improve the system throughput and fairness by 20% and 2x, respectively, over a baseline where each application executes with a TLP configuration that provides the best performance when it executes alone.",
                "paper_link": "https://www.semanticscholar.org/paper/f01a7f3fe226cf596c502cecc916d26edadc6cc2"
            },
            {
                "title": "\u03bcC-States: Fine-grained GPU datapath power management",
                "abstract": "To improve the performance of Graphics Processing Units (GPUs) beyond simply increasing core count, architects are recently adopting a scale-up approach: the peak throughput and individual capabilities of the GPU cores are increasing rapidly. This big-core trend in GPUs leads to various challenges, including higher static power consumption and lower and imbalanced utilization of the datapath components of a big core. As we show in this paper, two key problems ensue: (1) the lower and imbalanced datapath utilization can waste power as an application does not always utilize all portions of the big core datapath, and (2) the use of big cores can lead to application performance degradation in some cases due to the higher memory system contention caused by the more memory requests generated by each big core. This paper introduces a new analysis of datapath component utilization in big-core GPUs based on queuing theory principles. Building on this analysis, we introduce a fine-grained dynamic power- and clock-gating mechanism for the entire datapath, called \u03bcC-States, which aims to minimize power consumption by turning off or tuning-down datapath components that are not bottlenecks for the performance of the running application. Our experimental evaluation demonstrates that \u03bcC-States significantly reduces both static and dynamic power consumption in a big-core GPU, while also significantly improving the performance of applications affected by high memory system contention. We also show that our analysis of datapath component utilization can guide scheduling and design decisions in a GPU architecture that contains heterogeneous cores.",
                "paper_link": "https://www.semanticscholar.org/paper/a3eb0826dd5d88669d506c0cbfb0f3dc90937fed"
            },
            {
                "title": "RCoal: Mitigating GPU Timing Attack via Subwarp-based Randomized Coalescing Techniques",
                "abstract": "Graphics processing units (GPUs) are becoming default accelerators in many domains such as high-performance computing (HPC), deep learning, and virtual/augmented reality. Recently, GPUs have also shown significant speedups for a variety of security-sensitive applications such as encryptions. These speedups have largely benefited from the high memory bandwidth and compute throughput of GPUs. One of the key features to optimize the memory bandwidth consumption in GPUs is intra-warp memory access coalescing, which merges memory requests originating from different threads of a single warp into as few cache lines as possible. However, this coalescing feature is also shown to make the GPUs prone to the correlation timing attacks as it exposes the relationship between the execution time and the number of coalesced accesses. Consequently, an attacker is able to correctly reveal an AES private key via repeatedly gathering encrypted data and execution time on a GPU. In this work, we propose a series of defense mechanisms to alleviate such timing attacks by carefully trading off performance for improved security. Specifically, we propose to randomize the coalescing logic such that the attacker finds it hard to guess the correct number of coalesced accesses generated. To this end, we propose to randomize: a) the granularity (called as subwarp) at which warp threads are grouped together for coalescing, and b) the threads selected by each subwarp for coalescing. Such randomization techniques result in three mechanisms: fixed-sized subwarp (FSS), random-sized subwarp (RSS), and random-threaded subwarp (RTS). We find that the combination of these security mechanisms offers 24- to 961-times improvement in the security against the correlation timing attacks with 5 to 28% performance degradation.",
                "paper_link": "https://www.semanticscholar.org/paper/c3eaf15f09bba95a3ed2b271e9eec12dcf30cd1a"
            },
            {
                "title": "Practical resilience analysis of gpgpu applications in the presence of single-and multi-bit faults",
                "abstract": "Graphics Processing Units (GPUs) have rapidly evolved to enable energy-efficient data-parallel computing for a broad range of scientific areas. While GPUs achieve exascale performance at a stringent power budget, they are also susceptible to soft errors, often caused by high-energy particle strikes, that can significantly affect the application output quality. Understanding the resilience of general purpose GPU (GPGPU) applications is especially challenging because unlike CPU applications, which are mostly single-threaded, GPGPU applications can contain hundreds to thousands of threads, resulting in a tremendously large fault site space in the order of billions, even for some simple applications and even when considering the occurrence of just a single-bit fault. We present a systematic way to progressively prune the fault site space aiming to dramatically reduce the number of fault injections such that assessment for GPGPU application error resilience becomes practical. The key insight behind our proposed methodology stems from the fact that while GPGPU applications spawn a lot of threads, many of them execute the same set of instructions. Therefore, several fault sites are redundant and can be pruned by careful analysis. We identify important features across a set of 10 applications (16 kernels) from Rodinia and Polybench suites and conclude that threads can be primarily classified based on the number of the dynamic instructions they execute. We therefore achieve significant fault site reduction by analyzing only a small subset of threads that are representative of the dynamic instruction behavior (and therefore error resilience behavior) of the GPGPU applications. Further pruning is achieved by identifying the dynamic instruction commonalities (and differences) across code blocks within this representative set of threads, a subset of loop iterations within the representative threads, and a subset of destination register bit positions. The above steps result in a tremendous reduction of fault sites by up to seven orders of magnitude. Yet, this reduced fault site space accurately captures the error resilience profile of GPGPU applications. We show the effectiveness of the proposed progressive pruning technique for a single-bit model and illustrate its application to even more challenging cases with three distinct multi-bit fault models.",
                "paper_link": "https://www.semanticscholar.org/paper/4ed5a41df85fc8a0e3ee84ff72fa41eadcbca532"
            },
            {
                "title": "Why GPUs are Slow at Executing NFAs and How to Make them Faster",
                "abstract": "Non-deterministic Finite Automata (NFA) are space-efficient finite state machines that have significant applications in domains such as pattern matching and data analytics. In this paper, we investigate why the Graphics Processing Unit (GPU)---a massively parallel computational device with the highest memory bandwidth available on general-purpose processors---cannot efficiently execute NFAs. First, we identify excessive data movement in the GPU memory hierarchy and describe how to privatize reads effectively using GPU's on-chip memory hierarchy to reduce this excessive data movement. We also show that in several cases, indirect table lookups in NFAs can be eliminated by converting memory reads into computation, to further reduce the number of memory reads. Although our optimization techniques significantly alleviate these memory-related bottlenecks, a side effect of these techniques is the static assignment of work to cores. This leads to poor compute utilization, where GPU cores are wasted on idle NFA states. Therefore, we propose a new dynamic scheme that effectively balances compute utilization with reduced memory usage. Our combined optimizations provide a significant improvement over the previous state-of-the-art GPU implementations of NFAs. Moreover, they enable current GPUs to outperform the domain-specific accelerator for NFAs (i.e., Automata Processor) across several applications while performing within an order of magnitude for the rest of the applications.",
                "paper_link": "https://www.semanticscholar.org/paper/1c70b147aa2a138085793c66033eb03e015b8782"
            }
        ]
    },
    {
        "Professor": "Henry Kautz",
        "Papers": [
            {
                "title": "Planning as Satisfiability.",
                "abstract": "SATPLAN04 is a updated version of the planning as satisfiability approach originally proposed in (Kautz & Selman 1992; 1996) using hand-generated translations, and implemented for PDDL input in the blackbox system (Kautz & Selman 1999). Like blackbox, SATPLAN04 accepts the STRIPS subset of PDDL and finds solutions with minimal parallel length: that is, many (non-interferring) actions may occur in parallel at each time step, and the total number of time steps in guaranteed to be as small as possible. Also like blackbox, SATPLAN works by:",
                "paper_link": "https://www.semanticscholar.org/paper/5218c8b2f990b1b9bccb9306fa122369573b96af"
            },
            {
                "title": "Referral Web: combining social networks and collaborative filtering",
                "abstract": "Part of the success of social networks can be attributed to the \u201csix degrees of separation\u2019\u2019 phenomena that means the distance between any two individuals in terms of direct personal relationships is relatively small. An equally important factor is there are limits to the amount and kinds of information a person is able or willing to make available to the public at large. For example, an expert in a particular field is almost certainly unable to write down all he knows about the topic, and is likely to be unwilling to make letters of recommendation he or she has written for various people publicly available. Thus, searching for a piece of information in this situation becomes a matter of searching the social network for an expert on the topic together with a chain of personal referrals from the searcher to the expert. The referral chain serves two key functions: It provides a reason for the expert to agree to respond to the requester by making their relationship explicit (for example, they have a mutual collaborator), and it provides a criteria for the searcher to use in evaluating the trustworthiness of the expert. Nonetheless, manually searching for a referral chain can be a frustrating and time-consuming task. One is faced with the trade-off of contacting a large number of individuals at each step, and thus straining both the time and goodwill of the possible respondents, or of contacting a smaller, more focused set, and being more likely to fail to locate an appropriate expert. In response to these problems we are building ReferralWeb, an interactive system for reconstructing, visualizing, and searching social networks on the World-Wide Web. Simulation experiments we ran before we began construction of ReferralWeb showed that automatically generated referrals can be highly",
                "paper_link": "https://www.semanticscholar.org/paper/2469a507d0b9c877e7b75f2bfe0b87975513bcfa"
            },
            {
                "title": "Pushing the envelope: Planning, propositional logic, and stochastic search",
                "abstract": "Planning is a notoriously hard combinatorial search problem. In many interesting domains, current planning algorithms fail to scale up gracefully. By combining a general, stochastic search algorithm and appropriate problem encodings based on propositional logic, we are able to solve hard planning problems many times faster than the best current planning systems. Although stochastic methods have been shown to be very effective on a wide range of scheduling problems, this is the first demonstration of its power on truly challenging classical planning instances. This work also provides a new perspective on representational issues in planning.",
                "paper_link": "https://www.semanticscholar.org/paper/141c77b1d82bcae04a293c972cea02502e181dba"
            },
            {
                "title": "Noise strategies for improving local search",
                "abstract": "It has recently been shown that local search is surprisingly good at finding satisfying assignments for certain computationally hard classes of CNF formulas. The performance of basic local search methods can be further enhanced by introducing mechanisms for escaping from local minima in the search space. We will compare three such mechanisms: simulated annealing, random noise, and a strategy called \"mixed random walk\". We show that mixed random walk is the superior strategy. We also present results demonstrating the effectiveness of local search with walk for solving circuit synthesis and circuit diagnosis problems. Finally, we demonstrate that mixed random walk improves upon the best known methods for solving MAX-SAT problems.",
                "paper_link": "https://www.semanticscholar.org/paper/677583cfc715ec44e85a37e8c4055301e0f8f009"
            },
            {
                "title": "Inferring activities from interactions with objects",
                "abstract": "A key aspect of pervasive computing is using computers and sensor networks to effectively and unobtrusively infer users' behavior in their environment. This includes inferring which activity users are performing, how they're performing it, and its current stage. Recognizing and recording activities of daily living is a significant problem in elder care. A new paradigm for ADL inferencing leverages radio-frequency-identification technology, data mining, and a probabilistic inference engine to recognize ADLs, based on the objects people use. We propose an approach that addresses these challenges and shows promise in automating some types of ADL monitoring. Our key observation is that the sequence of objects a person uses while performing an ADL robustly characterizes both the ADL's identity and the quality of its execution. So, we have developed Proactive Activity Toolkit (PROACT).",
                "paper_link": "https://www.semanticscholar.org/paper/a106cd5537886f2920aaa1d214cf46865e424f7f"
            },
            {
                "title": "Learning and inferring transportation routines",
                "abstract": "This paper introduces a hierarchical Markov model that can learn and infer a user\u2019s daily movements through an urban community. The model uses multiple levels of abstraction in order to bridge the gap between raw GPS sensor measurements and high level information such as a user\u2019s destination and mode of transportation. To achieve efficient inference, we apply Rao-Blackwellized particle filters at multiple levels of the model hierarchy. Locations such as bus stops and parking lots, where the user frequently changes mode of transportation, are learned from GPS data logs without manual labeling of training data. We experimentally demonstrate how to accurately detect novel behavior or user errors (e.g. taking a wrong bus) by explicitly modeling activities in the context of the user\u2019s historical data. Finally, we discuss an application called \u201cOpportunity Knocks\u201d that employs our techniques to help cognitively-impaired people use public transportation safely.",
                "paper_link": "https://www.semanticscholar.org/paper/7bd48a717fd3435bd7024d3153e715393c763fee"
            },
            {
                "title": "Constraint propagation algorithms for temporal reasoning.",
                "abstract": "This paper considers computational aspects of several temporal representation languages. It investigates an interval-based representation, and a point-based one. Computing the consequences of temporal assertions is shown to be computationally intractable in the interval-based representation, but not in the point-based one. However, a fragment of the interval language can be expressed using the point language and benefits from the tractability of the latter.",
                "paper_link": "https://www.semanticscholar.org/paper/e0339a61865527b64dba0c18f4f9a11af24c8a97"
            },
            {
                "title": "Local search strategies for satisfiability testing.",
                "abstract": "It has recently been shown that local search is surprisingly good at nding satisfying assignments for certain classes of CNF formulas 24]. In this paper we demonstrate that the power of local search for satissability testing can be further enhanced by employinga new strategy, called \\mixed random walk\", for escaping from local minima. We present experimental results showing how this strategy allows us to handle formulas that are substantially larger than those that can be solved with basic local search. We also present a detailed comparison of our random walk strategy with simulated annealing. Our results show that mixed random walk is the superior strategy on several classes of computationally diicult problem instances. Finally, we present results demonstrating the eeectiveness of local search with walk for solving circuit synthesis and diagnosis problems.",
                "paper_link": "https://www.semanticscholar.org/paper/754b96e00671c74de0add9df1ef60dcf21160483"
            },
            {
                "title": "Boosting combinatorial search through randomization",
                "abstract": "Unpredictability in the running time of complete search procedures can often be explained by the phenomenon of \"heavy-tailed cost distributions\", meaning that at any time during the experiment there is a non-negligible probability of hitting a problem that requires exponentially more time to solve than any that has been encountered before (Gomes et al. 1998a). We present a general method for introducing controlled randomization into complete search algorithms. The \"boosted\" search methods provably eliminate heavy-tails to the right of the median. Furthermore, they can take advantage of heavy-tails to the left of the median (that is, a nonnegligible chance of very short runs) to dramatically shorten the solution time. We demonstrate speedups of several orders of magnitude for state-of-the-art complete search procedures running on hard, real-world problems.",
                "paper_link": "https://www.semanticscholar.org/paper/0bf171c284a2556bc493c2aba58dd4ed192706aa"
            },
            {
                "title": "Generalized plan recognition.",
                "abstract": "This paper outlines a new theory of plan recognition that is significantly more powerful than previous approaches. Concurrent actions, shared steps between actions, and disjunctive information are all handled. The theory allows one to draw conclusions based on the class of possible plans being performed, rather than having to prematurely commit to a single interpretation. The theory employs circumscription to transform a first-order theory of action into an action taxonomy, which can be used to logically deduce the complex action(s) an agent is performing.",
                "paper_link": "https://www.semanticscholar.org/paper/adc713f0787f9fdef701d63615acd4aad61165a6"
            },
            {
                "title": "Message filtering techniques",
                "abstract": "A social network is a set of people or organizations or other social entities connected by set of social relationships such as friendship, co-working or information exchange. Online Social Networks (OSN) usually not support to the user for message filtering. To solve this issue, which allows OSN users to have a direct control on the messages posted on their walls. The users can control the unwanted messages posted on their own private space .To avoid unwanted messages displayed and they can also block their friend from friends list using filtering rule, content based filtering and short text classification.",
                "paper_link": "https://www.semanticscholar.org/paper/726f92c657876d08c23a4994bd366830302fbf16"
            },
            {
                "title": "Inferring high-level behavior from low-level sensors",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/ae8f9768ffcda5bb055e78b94dd5cc281c0f7bb5"
            },
            {
                "title": "Towards a theory of natural language interfaces to databases",
                "abstract": "The need for Natural Language Interfaces to databases (NLIs) has become increasingly acute as more and more people access information through their web browsers, PDAs, and cell phones. Yet NLIs are only usable if they map natural language questions to SQL queries correctly. As Schneiderman and Norman have argued, people are unwilling to trade reliable and predictable user interfaces for intelligent but unreliable ones. In this paper, we introduce a theoretical framework for reliable NLIs, which is the foundation for the fully implemented Precise NLI. We prove that, for a broad class of semantically tractable natural language questions, Precise is guaranteed to map each question to the corresponding SQL query. We report on experiments testing Precise on several hundred questions drawn from user studies over three benchmark databases. We find that over 80% of the questions are semantically tractable questions, which Precise answers correctly. Precise automatically recognizes the 20% of questions that it cannot handle, and requests a paraphrase. Finally, we show that Precise compares favorably with Mooney's learning NLI and with Microsoft's English Query product",
                "paper_link": "https://www.semanticscholar.org/paper/b50f78c9534182a09c060580811274928702b38d"
            },
            {
                "title": "A formal theory of plan recognition and its implementation",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/d0a9805201dd4f4ba25c5eb45c7f9be8242eaa54"
            },
            {
                "title": "Unifying SAT-based and graph-based planning",
                "abstract": "The Blackbox planning system unifies the planning as satisfiability framework (Kautz and Selman 1992, 1996) with the plan graph approach to STRIPS planning (Blum and Furst 1995). We show that STRIPS problems can be directly translated into SAT and efficiently solved using new randomized systematic solvers. For certain computationally challenging benchmark problems this unified approach outperforms both SATPLAN and Graphplan alone. We also demonstrate that polynomialtime SAT simplification algorithms applied to the encoded problem instances are a powerful complement to the \"mutex\" propagation algorithm that works directly on the plan graph.",
                "paper_link": "https://www.semanticscholar.org/paper/a74226cf56a8cc50747d08e50fb2e9c783f7cf51"
            },
            {
                "title": "Activity recognition using the velocity histories of tracked keypoints",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Constraint propagation algorithms for temporal reasoning: A revised report",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/dcff7aa8ca9327ed1297cff206202bd3565eb8c1"
            },
            {
                "title": "Extracting places and activities from gps traces using hierarchical conditional random fields",
                "abstract": "Learning patterns of human behavior from sensor data is extremely important for high-level activity inference. This paper describes how to extract a person\u2019s activities and significant places from traces of GPS data. The system uses hierarchically structured conditional random fields to generate a consistent model of a person\u2019s activities and places. In contrast to existing techniques, this approach takes the high-level context into account in order to detect the significant places of a person. Experiments show significant improvements over existing techniques. Furthermore, they indicate that the proposed system is able to robustly estimate a person\u2019s activities using a model that is trained from data collected by other persons.",
                "paper_link": "https://www.semanticscholar.org/paper/0b3c22e0aba108d47f11e8a5bc35f228f2849d95"
            },
            {
                "title": "Fine-grained activity recognition by aggregating abstract object usage",
                "abstract": "In this paper we present results related to achieving finegrained activity recognition for context-aware computing applications. We examine the advantages and challenges of reasoning with globally unique object instances detected by an RFID glove. We present a sequence of increasingly powerful probabilistic graphical models for activity recognition. We show the advantages of adding additional complexity and conclude with a model that can reason tractably about aggregated object instances and gracefully generalizes from object instances to their classes by using abstraction smoothing. We apply these models to data collected from a morning household routine.",
                "paper_link": "https://www.semanticscholar.org/paper/dbc810e36c29a0162c55a42b9128870ace2339ce"
            },
            {
                "title": "Heavy-tailed phenomena in satisfiability and constraint satisfaction problems",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/6eb44d3238bcab0b0b7ad634288a49787b84abde"
            }
        ]
    },
    {
        "Professor": "Samira Khan",
        "Papers": [
            {
                "title": "Sampling dead block prediction for last-level caches",
                "abstract": "Last-level caches (LLCs) are large structures with significant power requirements. They can be quite inefficient. On average, a cache block in a 2MB LRU-managed LLC is dead 86% of the time, i.e., it will not be referenced again before it is evicted. This paper introduces sampling dead block prediction, a technique that samples program counters (PCs) to determine when a cache block is likely to be dead. Rather than learning from accesses and evictions from every set in the cache, a sampling predictor keeps track of a small number of sets using partial tags. Sampling allows the predictor to use far less state than previous predictors to make predictions with superior accuracy. Dead block prediction can be used to drive a dead block replacement and bypass optimization. A sampling predictor can reduce the number of LLC misses over LRU by 11.7% for memory-intensive single-thread benchmarks and 23% for multi-core workloads. The reduction in misses yields a geometric mean speedup of 5.9% for single-thread benchmarks and a geometric mean normalized weighted speedup of 12.5% for multi-core workloads. Due to the reduced state and number of accesses, the sampling predictor consumes only 3.1% of the of the dynamic power and 1.2% of the leakage power of a baseline 2MB LLC, comparing favorably with more costly techniques. The sampling predictor can even be used to significantly improve a cache with a default random replacement policy.",
                "paper_link": "https://www.semanticscholar.org/paper/beb69e174c75a19299c446a427425edfd55209f2"
            },
            {
                "title": "Adaptive-Latency DRAM: Optimizing DRAM Timing for the Common-Case",
                "abstract": "In current systems, memory accesses to a DRAM chip must obey a set of minimum latency restrictions specified in the DRAM standard. Such timing parameters exist to guarantee reliable operation. When deciding the timing parameters, DRAM manufacturers incorporate a very large margin as a provision against two worst-case scenarios. First, due to process variation, some outlier chips are much slower than others and cannot be operated as fast. Second, chips become slower at higher temperatures, and all chips need to operate reliably at the highest supported (i.e., worst-case) DRAM temperature (85\u00b0 C). In this paper, we show that typical DRAM chips operating at typical temperatures (e.g., 55\u00b0 C) are capable of providing a much smaller access latency, but are nevertheless forced to operate at the largest latency of the worst-case. Our goal in this paper is to exploit the extra margin that is built into the DRAM timing parameters to improve performance. Using an FPGA-based testing platform, we first characterize the extra margin for 115 DRAM modules from three major manufacturers. Our results demonstrate that it is possible to reduce four of the most critical timing parameters by a minimum/maximum of 17.3%/54.8% at 55\u00b0C without sacrificing correctness. Based on this characterization, we propose Adaptive-Latency DRAM (AL-DRAM), a mechanism that adoptively reduces the timing parameters for DRAM modules based on the current operating condition. AL-DRAM does not require any changes to the DRAM chip or its interface. We evaluate AL-DRAM on a real system that allows us to reconfigure the timing parameters at runtime. We show that AL-DRAM improves the performance of memory-intensive workloads by an average of 14% without introducing any errors. We discuss and show why AL-DRAM does not compromise reliability. We conclude that dynamically optimizing the DRAM timing parameters can reliably improve system performance.",
                "paper_link": "https://www.semanticscholar.org/paper/012d556d67acedc6898930b4c93f54b87aabf5ee"
            },
            {
                "title": "Accelerating Pointer Chasing in 3D-Stacked Memory: Challenges, Mechanisms, Evaluation",
                "abstract": "Pointer chasing is a fundamental operation, used by many important data-intensive applications (e.g., databases, key-value stores, graph processing workloads) to traverse linked data structures. This operation is both memory bound and latency sensitive, as it (1) exhibits irregular access patterns that cause frequent cache and TLB misses, and (2) requires the data from every memory access to be sent back to the CPU to determine the next pointer to access. Our goal is to accelerate pointer chasing by performing it inside main memory, thereby avoiding inefficient and high-latency data transfers between main memory and the CPU. To this end, we propose the In-Memory PoInter Chasing Accelerator (IMPICA), which leverages the logic layer within 3D-stacked memory for linked data structure traversal. This paper identifies the key design challenges of designing a pointer chasing accelerator in memory, describes new mechanisms employed within IMPICA to solve these challenges, and evaluates the performance and energy benefits of our accelerator. IMPICA addresses the key challenges of (1) how to achieve high parallelism in the presence of serial accesses in pointer chasing, and (2) how to effectively perform virtual-to-physical address translation on the memory side without requiring expensive accesses to the CPU's memory management unit. We show that the solutions to these challenges, address-access decoupling and a region-based page table, respectively, are simple and low-cost. We believe these solutions are also applicable to many other in-memory accelerators, which are likely to also face the two challenges. Our evaluations on a quad-core system show that IMPICA improves the performance of pointer chasing operations in three commonly-used linked data structures (linked lists, hash tables, and B-trees) by 92%, 29%, and 18%, respectively. This leads to a significant performance improvement in applications that utilize linked data structures - on a real database application, DBx1000, IMPICA improves transaction throughput and response time by 16% and 13%, respectively. IMPICA also significantly reduces overall system energy consumption (by 41%, 23%, and 10% for the three commonly-used data structures, and by 6% for DBx1000).",
                "paper_link": "https://www.semanticscholar.org/paper/5e41307a2f2850f164ad0175f372799ce61e0bf9"
            },
            {
                "title": "AVATAR: A Variable-Retention-Time (VRT) Aware Refresh for DRAM Systems",
                "abstract": "Multirate refresh techniques exploit the non-uniformity in retention times of DRAM cells to reduce the DRAM refresh overheads. Such techniques rely on accurate profiling of retention times of cells, and perform faster refresh only for a few rows which have cells with low retention times. Unfortunately, retention times of some cells can change at runtime due to Variable Retention Time (VRT), which makes it impractical to reliably deploy multirate refresh. Based on experimental data from 24 DRAM chips, we develop architecture-level models for analyzing the impact of VRT. We show that simply relying on ECC DIMMs to correct VRT failures is unusable as it causes a data error once every few months. We propose AVATAR, a VRT-aware multirate refresh scheme that adaptively changes the refresh rate for different rows at runtime based on current VRT failures. AVATAR provides a time to failure in the regime of several tens of years while reducing refresh operations by 62%-72%.",
                "paper_link": "https://www.semanticscholar.org/paper/35b92289fe9c19e5baf462ffe91bdd2d25768b00"
            },
            {
                "title": "Understanding latency variation in modern DRAM chips: Experimental characterization, analysis, and optimization",
                "abstract": "Long DRAM latency is a critical performance bottleneck in current systems. DRAM access latency is defined by three fundamental operations that take place within the DRAM cell array: (i) activation of a memory row, which opens the row to perform accesses; (ii) precharge, which prepares the cell array for the next memory access; and (iii) restoration of the row, which restores the values of cells in the row that were destroyed due to activation. There is significant latency variation for each of these operations across the cells of a single DRAM chip due to irregularity in the manufacturing process. As a result, some cells are inherently faster to access, while others are inherently slower. Unfortunately, existing systems do not exploit this variation. The goal of this work is to (i) experimentally characterize and understand the latency variation across cells within a DRAM chip for these three fundamental DRAM operations, and (ii) develop new mechanisms that exploit our understanding of the latency variation to reliably improve performance. To this end, we comprehensively characterize 240 DRAM chips from three major vendors, and make several new observations about latency variation within DRAM. We find that (i) there is large latency variation across the cells for each of the three operations; (ii) variation characteristics exhibit significant spatial locality: slower cells are clustered in certain regions of a DRAM chip; and (iii) the three fundamental operations exhibit different reliability characteristics when the latency of each operation is reduced. Based on our observations, we propose Flexible-LatencY DRAM (FLY-DRAM), a mechanism that exploits latency variation across DRAM cells within a DRAM chip to improve system performance. The key idea of FLY-DRAM is to exploit the spatial locality of slower cells within DRAM, and access the faster DRAM regions with reduced latencies for the fundamental operations. Our evaluations show that FLY-DRAM improves the performance of a wide range of applications by 13.3%, 17.6%, and 19.5%, on average, for each of the three different vendors' real DRAM chips, in a simulated 8-core system. We conclude that the experimental characterization and analysis of latency variation within modern DRAM, provided by this work, can lead to new techniques that improve DRAM and system performance.",
                "paper_link": "https://www.semanticscholar.org/paper/fd9f985e61e20a5226bef3f5aeb6b0d1f9f5bcb6"
            },
            {
                "title": "The application slowdown model: Quantifying and controlling the impact of inter-application interference at shared caches and main memory",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "ThyNVM: Enabling Software-Transparent Crash Consistency in Persistent Memory Systems",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "The efficacy of error mitigation techniques for DRAM retention failures: a comparative experimental study",
                "abstract": "As DRAM cells continue to shrink, they become more susceptible to retention failures. DRAM cells that permanently exhibit short retention times are fairly easy to identify and repair through the use of memory tests and row and column redundancy. However, the retention time of many cells may vary over time due to a property called Variable Retention Time (VRT). Since these cells intermittently transition between failing and non-failing states, they are particularly difficult to identify through memory tests alone. In addition, the high temperature packaging process may aggravate this problem as the susceptibility of cells to VRT increases after the assembly of DRAM chips. A promising alternative to manufacture-time testing is to detect and mitigate retention failures after the system has become operational. Such a system would require mechanisms to detect and mitigate retention failures in the field, but would be responsive to retention failures introduced after system assembly and could dramatically reduce the cost of testing, enabling much longer tests than are practical with manufacturer testing equipment.\n In this paper, we analyze the efficacy of three common error mitigation techniques (memory tests, guardbands, and error correcting codes (ECC)) in real DRAM chips exhibiting both intermittent and permanent retention failures. Our analysis allows us to quantify the efficacy of recent system-level error mitigation mechanisms that build upon these techniques. We revisit prior works in the context of the experimental data we present, showing that our measured results significantly impact these works' conclusions. We find that mitigation techniques that rely on run-time testing alone [38, 27, 50, 26] are unable to ensure reliable operation even after many months of testing. Techniques that incorporate ECC[4, 52], however, can ensure reliable DRAM operation after only a few hours of testing. For example, VS-ECC[4], which couples testing with variable strength codes to allocate the strongest codes to the most error-prone memory regions, can ensure reliable operation for 10 years after only 19 minutes of testing. We conclude that the viability of these mitigation techniques depend on efficient online profiling of DRAM performed without disrupting system operation.",
                "paper_link": "https://www.semanticscholar.org/paper/15e63d368aa803c73b8f5d1315a51ebd7ceea3c3"
            },
            {
                "title": "Simultaneous Multi-Layer Access: Improving 3D-Stacked Memory Bandwidth at Low Cost",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Design-induced latency variation in modern dram chips: Characterization, analysis, and latency reduction mechanisms",
                "abstract": "Variation has been shown to exist across the cells within a modern DRAM chip. Prior work has studied and exploited several forms of variation, such as manufacturing-process- or temperature-induced variation. We empirically demonstrate a new form of variation that exists within a real DRAM chip, induced by the design and placement of different components in the DRAM chip: different regions in DRAM, based on their relative distances from the peripheral structures, require different minimum access latencies for reliable operation. In particular, we show that in most real DRAM chips, cells closer to the peripheral structures can be accessed much faster than cells that are farther. We call this phenomenon design-induced variation in DRAM. Our goals are to i) understand design-induced variation that exists in real, state-of-the-art DRAM chips, ii) exploit it to develop low-cost mechanisms that can dynamically find and use the lowest latency at which to operate a DRAM chip reliably, and, thus, iii) improve overall system performance while ensuring reliable system operation. To this end, we first experimentally demonstrate and analyze designed-induced variation in modern DRAM devices by testing and characterizing 96 DIMMs (768 DRAM chips). Our characterization identifies DRAM regions that are vulnerable to errors, if operated at lower latency, and finds consistency in their locations across a given DRAM chip generation, due to design-induced variation. Based on our extensive experimental analysis, we develop two mechanisms that reliably reduce DRAM latency. First, DIVA Profiling uses runtime profiling to dynamically identify the lowest DRAM latency that does not introduce failures. DIVA Profiling exploits design-induced variation and periodically profiles only the vulnerable regions to determine the lowest DRAM latency at low cost. It is the first mechanism to dynamically determine the lowest latency that can be used to operate DRAM reliably. DIVA Profiling reduces the latency of read/write requests by 35.1%/57.8%, respectively, at 55\u00b0C. Our second mechanism, DIVA Shuffling, shuffles data such that values stored in vulnerable regions are mapped to multiple error-correcting code (ECC) codewords. As a result, DIVA Shuffling can correct 26% more multi-bit errors than conventional ECC. Combined together, our two mechanisms reduce read/write latency by 40.0%/60.5%, which translates to an overall system performance improvement of 14.7%/13.7%/13.8% (in 2-/4-/8-core systems) across a variety of workloads, while ensuring reliable operation.",
                "paper_link": "https://www.semanticscholar.org/paper/77f62bda59b101b598846a71256b4941b4d4b163"
            },
            {
                "title": "Design-Induced Latency Variation in Modern DRAM Chips",
                "abstract": "Variation has been shown to exist across the cells within a modern DRAM chip. Prior work has studied and exploited several forms of variation, such as manufacturing-process- or temperature-induced variation. We empirically demonstrate a new form of variation that exists within a real DRAM chip, induced by the design and placement of different components in the DRAM chip: different regions in DRAM, based on their relative distances from the peripheral structures, require different minimum access latencies for reliable operation. In particular, we show that in most real DRAM chips, cells closer to the peripheral structures can be accessed much faster than cells that are farther. We call this phenomenon design-induced variation in DRAM. Our goals are to i) understand design-induced variation that exists in real, state-of-the-art DRAM chips, ii) exploit it to develop low-cost mechanisms that can dynamically find and use the lowest latency at which to operate a DRAM chip reliably, and, thus, iii) improve overall system performance while ensuring reliable system operation. To this end, we first experimentally demonstrate and analyze designed-induced variation in modern DRAM devices by testing and characterizing 96 DIMMs (768 DRAM chips). Our characterization identifies DRAM regions that are vulnerable to errors, if operated at lower latency, and finds consistency in their locations across a given DRAM chip generation, due to design-induced variation. Based on our extensive experimental analysis, we develop two mechanisms that reliably reduce DRAM latency. First, DIVA Profiling uses runtime profiling to dynamically identify the lowest DRAM latency that does not introduce failures. DIVA Profiling exploits design-induced variation and periodically profiles only the vulnerable regions to determine the lowest DRAM latency at low cost. It is the first mechanism to dynamically determine the lowest latency that can be used to operate DRAM reliably. DIVA Profiling reduces the latency of read/write requests by 35.1%/57.8%, respectively, at 55\u00b0C. Our second mechanism, DIVA Shuffling, shuffles data such that values stored in vulnerable regions are mapped to multiple error-correcting code (ECC) codewords. As a result, DIVA Shuffling can correct 26% more multi-bit errors than conventional ECC. Combined together, our two mechanisms reduce read/write latency by 40.0%/60.5%, which translates to an overall system performance improvement of 14.7%/13.7%/13.8% (in 2-/4-/8-core systems) across a variety of workloads, while ensuring reliable operation.",
                "paper_link": "https://www.semanticscholar.org/paper/77f62bda59b101b598846a71256b4941b4d4b163"
            },
            {
                "title": "PARBOR: An efficient system-level technique to detect data-dependent failures in DRAM",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "SoftMC: A flexible and practical open-source infrastructure for enabling experimental DRAM studies",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Detecting and mitigating data-dependent DRAM failures by exploiting current memory content",
                "abstract": "DRAM cells in close proximity can fail depending on the data content in neighboring cells. These failures are called data-dependent failures. Detecting and mitigating these failures online, while the system is running in the field, enables various optimizations that improve reliability, latency, and energy efficiency of the system. For example, a system can improve performance and energy efficiency by using a lower refresh rate for most cells and mitigate the failing cells using higher refresh rates or error correcting codes. All these system optimizations depend on accurately detecting every possible data-dependent failure that could occur with any content in DRAM. Unfortunately, detecting all data-dependent failures requires the knowledge of DRAM internals specific to each DRAM chip. As internal DRAM architecture is not exposed to the system, detecting data-dependent failures at the system-level is a major challenge.In this paper, we decouple the detection and mitigation of data-dependent failures from physical DRAM organization such that it is possible to detect failures without knowledge of DRAM internals. To this end, we propose MEMCON, a memory content-based detection and mitigation mechanism for data-dependent failures in DRAM. MEMCON does not detect every possible data-dependent failure. Instead, it detects and mitigates failures that occur only with the current content in memory while the programs are running in the system. Such a mechanism needs to detect failures whenever there is a write access that changes the content of memory. As detection of failure with a runtime testing has a high overhead, MEMCON selectively initiates a test on a write, only when the time between two consecutive writes to that page (i.e., write interval) is long enough to provide significant benefit by lowering the refresh rate during that interval. MEMCON builds upon a simple, practical mechanism that predicts the long write intervals based on our observation that the write intervals in real workloads follow a Pareto distribution: the longer a page remains idle after a write, the longer it is expected to remain idle. Our evaluation shows that compared to a system that uses an aggressive refresh rate, MEMCON reduces refresh operations by 65-74%, leading to a 10%/17%/40% (min) to 12%/22%/50% (max) performance improvement for a single-core and 10%/23%/52% (min) to 17%/29%/65% (max) performance improvement for a 4-core system using 8/16/32 Gb DRAM chips.CCS CONCEPTS\u2022 Computer systems organization $\\rightarrow$ Processors and memory architectures; \u2022 Hardware $\\rightarrow$ Dynamic memory;",
                "paper_link": "https://www.semanticscholar.org/paper/4d23b5d9225f86efb56b0241c3a00349a6f51136"
            },
            {
                "title": "A Case for Efficient Hardware/Software Cooperative Management of Storage and Memory",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Using dead blocks as a virtual victim cache",
                "abstract": "Caches mitigate the long memory latency that limits the performance of modern processors. However, caches can be quite inefficient. On average, a cache block in a 2MB L2 cache is dead 59% of the time, i.e., it will not be referenced again before it is evicted. Increasing cache efficiency can improve performance by reducing miss rate, or alternately, improve power and energy by allowing a smaller cache with the same miss rate.",
                "paper_link": "https://www.semanticscholar.org/paper/070955a98213cda742709edfd93d32c718cfc887"
            },
            {
                "title": "Crash Consistency in Encrypted Non-volatile Main Memory Systems",
                "abstract": "Non-Volatile Main Memory (NVMM) systems provide high performance by directly manipulating persistent data in-memory, but require crash consistency support to recover data in a consistent state in case of a power failure or system crash. In this work, we focus on the interplay between the crash consistency mechanisms and memory encryption. Memory encryption is necessary for these systems to protect data against the attackers with physical access to the persistent main memory. As decrypting data at every memory read access can significantly degrade the performance, prior works propose to use a memory encryption technique, counter-mode encryption, that reduces the decryption overhead by performing a memory read access in parallel with the decryption process using a counter associated with each cache line. Therefore, a pair of data and counter value is needed to correctly decrypt data after a system crash. We demonstrate that counter-mode encryption does not readily extend to crash consistent NVMM systems as the system will fail to recover data in a consistent state if the encrypted data and associated counter are not written back to memory atomically, a requirement we refer to as counter-atomicity. We show that na\u00a8\u0131vely enforcing counter-atomicity for all NVMM writes can serialize memory accesses and results in a significant performance degradation. In order to improve the performance, we make an observation that not all writes to NVMM need to be counter-atomic. The crash consistency mechanisms rely on versioning to keep one consistent copy of data intact while manipulating another version directly in-memory. As the recovery process only relies on the unmodified consistent version, it is not necessary to strictly enforce counter-atomicity for the writes that do not affect data recovery. Based on this insight, we propose selective counter-atomicity that allows reordering of writes to data and associated counters when the writes to persistent memory do not alter the recoverable consistent state. We propose efficient software and hardware support to enforce selective counter-atomicity. Our evaluation demonstrates that in a 1/2/4/8- core system, selective counter-atomicity improves performance by 6/11/22/40% compared to a system that enforces counter-atomicity for all NVMM writes. The performance of our selective counter-atomicity design comes within 5% of an ideal NVMM system that provides crash consistency of encrypted data at no cost.",
                "paper_link": "https://www.semanticscholar.org/paper/83ea81daf6cdad70081b386cfddefd9780b88ba8"
            },
            {
                "title": "PMTest: A fast and flexible testing framework for persistent memory programs",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Improving Cache Performance by Exploiting Read-Write Disparity",
                "abstract": "Cache read misses stall the processor if there are no independent instructions to execute. In contrast, most cache write misses are off the critical path of execution, since writes can be buffered in the cache or the store buffer. With few exceptions, cache lines that serve loads are more critical for performance than cache lines that serve only stores. Unfortunately, traditional cache management mechanisms do not take into account this disparity between read-write criticality. The key contribution of this paper is the new idea of distinguishing between lines that are reused by reads versus those that are reused only by writes to focus cache management policies on the more critical read lines. We propose a Read-Write Partitioning (RWP) policy that minimizes read misses by dynamically partitioning the cache into clean and dirty partitions, where partitions grow in size if they are more likely to receive future read requests. We show that exploiting the differences in read-write criticality provides better performance over prior cache management mechanisms. For a single-core system, RWP provides 5% average speedup across the entire SPEC CPU2006 suite, and 14% average speedup for cachesensitive benchmarks, over the baseline LRU replacement policy. We also show that RWP can perform within 3% of a new yet complex instruction-address-based technique, Read Reference Predictor (RRP), that bypasses cache lines which are unlikely to receive any read requests, while requiring only 5.4% of RRP\u2019s state overhead. On a 4-core system, our RWP mechanism improves system throughput by 6% over the baseline and outperforms three other state-of-the-art mechanisms we evaluate.",
                "paper_link": "https://www.semanticscholar.org/paper/f705b005051c0a944573eebd45ccd6a8017c44ca"
            },
            {
                "title": "Last-level cache deduplication",
                "abstract": "Caches are essential to the performance of modern micro- processors. Much recent work on last-level caches has focused on exploiting reference locality to improve efficiency. However, value redundancy is another source of potential improvement. We find that many blocks in the working set of typical benchmark programs have the same values. We propose cache deduplication that effectively increases last- level cache capacity. Rather than exploit specific value redundancy with compression, as in previous work, our scheme detects duplicate data blocks and stores only one copy of the data in a way that can be accessed through multiple physical addresses. We find that typical benchmarks exhibit significant value redundancy, far beyond the zero-content blocks one would expect in any program. Our deduplicated cache effectively increases capacity by an average of 112% com- pared to an 8MB last-level cache while reducing the physical area by 12.2%, yielding an average performance improvement of 15.2%.",
                "paper_link": "https://www.semanticscholar.org/paper/24f56717022075157ba4ad01e84f65ab6ebce505"
            }
        ]
    },
    {
        "Professor": "Hyojoon Kim",
        "Papers": [
            {
                "title": "Improving network management with software defined networking",
                "abstract": "Network management is challenging. To operate, maintain, and secure a communication network, network operators must grapple with low-level vendor-specific configuration to implement complex high-level network policies. Despite many previous proposals to make networks easier to manage, many solutions to network management problems amount to stop-gap solutions because of the difficulty of changing the underlying infrastructure. The rigidity of the underlying infrastructure presents few possibilities for innovation or improvement, since network devices have generally been closed, proprietary, and vertically integrated. A new paradigm in networking, software defined networking (SDN), advocates separating the data plane and the control plane, making network switches in the data plane simple packet forwarding devices and leaving a logically centralized software program to control the behavior of the entire network. SDN introduces new possibilities for network management and configuration methods. In this article, we identify problems with the current state-of-the-art network configuration and management mechanisms and introduce mechanisms to improve various aspects of network management. We focus on three problems in network management: enabling frequent changes to network conditions and state, providing support for network configuration in a highlevel language, and providing better visibility and control over tasks for performing network diagnosis and troubleshooting. The technologies we describe enable network operators to implement a wide range of network policies in a high-level policy language and easily determine sources of performance problems. In addition to the systems themselves, we describe various prototype deployments in campus and home networks that demonstrate how SDN can improve common network management tasks.",
                "paper_link": "https://www.semanticscholar.org/paper/3fc00b96d6385f6b0081952d03993583bad4d15e"
            },
            {
                "title": "Procera: A language for high-level reactive network control",
                "abstract": "Our previous experience building systems for implementing network policies in home and enterprise networks has revealed that the intuitive notion of network policy in these domains is inherently dynamic and stateful. Current configuration languages, both in traditional network architectures and in OpenFlow systems, are not expressive enough to capture these policies. As a result, most prototype OpenFlow systems lack a configurable interface and instead require operators to program in the system implementation language, often C++. We describe Procera, a control architecture for software-defined networking (SDN) that includes a declarative policy language based on the notion of functional reactive programming; we extend this formalism with both signals relevant for expressing high-level network policies in a variety of network settings, including home and enterprise networks, and a collection of constructs expressing temporal queries over event streams that occur frequently in network policies. Although sophisticated users can take advantage of Procera's full expressiveness by expressing network policies directly in Procera, simpler configuration interfaces (e.g., graphical user interfaces) can also easily be built on top of this formalism.",
                "paper_link": "https://www.semanticscholar.org/paper/f96581b4134ca9aae70015ff506c918862fe9ada"
            },
            {
                "title": "Kinetic: Verifiable Dynamic Network Control",
                "abstract": "Network conditions are dynamic; unfortunately, current approaches to configuring networks. Network operators need tools to express how a network's data-plane behavior should respond to a wide range of events and changing conditions, ranging from unexpected failures to shifting traffic patterns to planned maintenance. Yet, to update the network configuration today, operators typically rely on a combination of manual intervention and ad hoc scripts. In this paper, we present Kinetic, a domain specific language and network control system that enables operators to control their networks dynamically in a concise, intuitive way. Kinetic also automatically verifies the correctness of these control programs with respect to user-specified temporal properties. Our user study of Kinetic with several hundred network operators demonstrates that Kinetic is intuitive and usable, and our performance evaluation shows that realistic Kinetic programs scale well with the number of policies and the size of the network.",
                "paper_link": "https://www.semanticscholar.org/paper/d2efaf41bf1cab8c2961607bb0a685d1f34ee67a"
            },
            {
                "title": "Coronet: Fault tolerance for software defined networks",
                "abstract": "Software Defined Networking, or SDN, based networks are being deployed not only in testbed networks, but also in production networks. Although fault-tolerance is one of the most desirable properties in production networks, there are not much study in providing fault-tolerance to SDN-based networks. The goal of this work is to develop a fault tolerant SDN architecture that can rapidly recover from faults and scale to large network sizes. This paper presents CORONET, a SDN fault-tolerant system that recovers from multiple link failures in the data plane. We describe a prototype implementation based on NOX that demonstrates fault recovery for emulated topologies using Mininet. We also discuss possible extensions to handle control plane and controller faults.",
                "paper_link": "https://www.semanticscholar.org/paper/48fcf55c8f2c40b5c7821b3ee2752ac41073046c"
            },
            {
                "title": "The evolution of network configuration: A tale of two campuses",
                "abstract": "Studying network configuration evolution can improve our understanding of the evolving complexity of networks and can be helpful in making network configuration less error-prone. Unfortunately, the nature of changes that operators make to network configuration is poorly understood. Towards improving our understanding, we examine and analyze five years of router, switch, and firewall configurations from two large campus networks using the logs from version control systems used to store the configurations. We study how network configuration is distributed across different network operations tasks and how the configuration for each task evolves over time, for different types of devices and for different locations in the network. To understand the trends of how configuration evolves over time, we study the extent to which configuration for various tasks are added, modified, or deleted. We also study whether certain devices experience configuration changes more frequently than others, as well as whether configuration changes tend to focus on specific portions of the configuration (or on specific tasks). We also investigate when network operators make configuration changes of various types. Our results concerning configuration changes can help the designers of configuration languages understand which aspects of configuration might be more automated or tested more rigorously and may ultimately help improve configuration languages.",
                "paper_link": "https://www.semanticscholar.org/paper/6c60edaa919c7a238345f866d7e20bee2897fb6c"
            },
            {
                "title": "Peeking behind the NAT: an empirical study of home networks",
                "abstract": "We present the first empirical study of home network availability, infrastructure, and usage, using data collected from home networks around the world. In each home, we deploy a router with custom firmware to collect information about the availability of home broadband network connectivity, the home network infrastructure (including the wireless connectivity in each home network and the number of devices connected to the network), and how people in each home network use the network. Downtime is more frequent and longer in developing countries---sometimes due to the network, and in other cases because they simply turn their home router off. We also find that some portions of the wireless spectrum are extremely crowded, that diurnal patterns are more pronounced during the week, and that most traffic in home networks is exchanged over a few connections to a small number of domains. Our study is both a preliminary view into many home networks and an illustration of how measurements from a home router can yield significant information about home networks.",
                "paper_link": "https://www.semanticscholar.org/paper/1141cab27e6acd664c100690ba6b3b5126e8ed3e"
            },
            {
                "title": "Measuring TCP round-trip time in the data plane",
                "abstract": "We present a data-plane algorithm that passively and continuously monitors the Round-Trip Time of TCP traffic, by matching data packets with their associated acknowledgments and calculating a time difference. Compared with traditional measurement systems based on active probing or measuring only SYN/ACK packets, our algorithm passively produces many samples for long-running connections. This enables network operators to observe abnormal RTT increases, which signal possible security or performance issues in the network, in real-time. To satisfy the stringent memory size and access constraints of programmable switches, our algorithm uses a multi-stage hash table data structure to maintain records for in-flight packets; the records not receiving their acknowledgments are lazily expired and overwritten. We implement our algorithm on a Barefoot Tofino programmable switch. Evaluation using a real-world traffic trace from a 10 Gbps campus network link demonstrates that our solution can accurately capture 99% of available RTT samples, using only 4 MB of data-plane memory.",
                "paper_link": "https://www.semanticscholar.org/paper/a6370e879a54a9d11921b70af937c369d2c7fac6"
            },
            {
                "title": "Continuous in-network round-trip time monitoring",
                "abstract": "Round-trip time (RTT) is a central metric that influences end-user QoE and can expose traffic-interception attacks. Many popular RTT monitoring techniques either send active probes (that do not capture application-level RTTs) or passively monitor only the TCP handshake (which can be inaccurate, especially for long-lived flows). High-speed programmable switches present a unique opportunity to monitor the RTTs continuously and react in real time to improve performance and security. In this paper, we present Dart, an inline, real-time, and continuous RTT measurement system that can enable automated detection of network events and adapt (e.g., routing, scheduling, marking, or dropping traffic) inside the network. However, designing Dart is fraught with challenges, due to the idiosyncrasies of the TCP protocol and the resource constraints in high-speed switches. Dart overcomes these challenges by strategically limiting the tracking of packets to only those that can generate useful RTT samples, and by identifying the synergy between per-flow state and per-packet state for efficient memory use. We present a P4 prototype of Dart for the Tofino switch, as well our experiments on a campus testbed and simulations using anonymized campus traces. Dart, running in real time and with limited data-plane memory, is able to collect 99% of the RTT samples of an offline, software baseline---a variant of the popular tcptrace tool that has access to unlimited memory.",
                "paper_link": "https://www.semanticscholar.org/paper/945cdcf0ec894fee32da7f4241d46e779d79a42d"
            },
            {
                "title": "ucap: An internet data management tool for the home",
                "abstract": "Internet Service Providers (ISPs) have introduced \"data caps\", or quotas on the amount of data that a customer can download during a billing cycle. Under this model, Internet users who reach a data cap can be subject to degraded performance, extra fees, or even temporary interruption of Internet service. For this reason, users need better visibility into and control over their Internet usage to help them understand what uses up data and control how these quotas are reached. In this paper, we present the design and implementation of a tool, called uCap, to help home users manage Internet data. We conducted a field trial of uCap in 21 home networks in three countries and performed an in-depth qualitative study of ten of these homes. We present the results of the evaluation and implications for the design of future Internet data management tools.",
                "paper_link": "https://www.semanticscholar.org/paper/3382ad3d235f8e901cb9c6dc50bc894432a0a3bd"
            },
            {
                "title": "Communicating with caps: Managing usage caps in home networks",
                "abstract": "As Internet service providers increasingly implement and impose \"usage caps\", consumers need better ways to help them understand and control how devices in the home use up the available network resources or available capacity. Towards this goal, we will demonstrate a system that allows users to monitor and manage their usage caps. The system uses the BISMark firmware running on network gateways to collect usage statistics and report them to a logically centralized controller, which displays usage information. The controller allows users to specify policies about how different people, devices, and applications should consume the usage cap; it implements and enforces these policies via a secure OpenFlow control channel to each gateway device. The demonstration will show various use cases, such as limiting the usage of a particular application, visualizing usage statistics, and allowing users within a single household to \"trade\" caps with one another.",
                "paper_link": "https://www.semanticscholar.org/paper/66615ceb5cb96d8edf5b46415c19c2a5e25d626a"
            },
            {
                "title": "Enabling passive measurement of zoom performance in production networks",
                "abstract": "Video-conferencing applications impose high loads and stringent performance requirements on the network. To better understand and manage these applications, we need effective ways to measure performance in the wild. For example, these measurements would help network operators in capacity planning, troubleshooting, and setting QoS policies. Unfortunately, large-scale measurements of production networks cannot rely on end-host cooperation, and an in-depth analysis of packet traces requires knowledge of the header formats. Zoom is one of the most sophisticated and popular applications, but it uses a proprietary network protocol. In this paper, we demystify how Zoom works at the packet level, and design techniques for analyzing Zoom performance from packet traces. We conduct systematic controlled experiments to discover the relevant unencrypted fields in Zoom packets, as well as how to group streams into meetings and how to identify peer-to-peer meetings. We show how to use the header fields to compute metrics like media bit rates, frame sizes and rates, and latency and jitter, and demonstrate the value of these fine-grained metrics on a 12-hour trace of Zoom traffic on our campus network.",
                "paper_link": "https://www.semanticscholar.org/paper/f229bfb778997aaf2347fe27aab8bca1fba6394e"
            },
            {
                "title": "ONTAS: Flexible and scalable online network traffic anonymization system",
                "abstract": "Access to packet traces is required not only to detect and diagnose various network issues related to performance and security, but also to train intelligent learning models enabling networks that can run themselves. However, packets in a network carry a lot of information which can be used to personally identify users and their online behavior. This requires network operators to anonymize packet traces before sharing them with other researchers and analysts. Existing tools anonymize packet traces in an offline manner, which incurs significant computational, storage, and memory overhead---limiting their ability to scale as the volume of the collected packet trace increases. In this paper, we present the design and implementation of an Online Network Traffic Anonymization System, ONTAS, which can flexibly anonymize packet traces in the data plane itself using modern PISA-based programmable switches.",
                "paper_link": "https://www.semanticscholar.org/paper/54cef7b3481461692eff20329568789cb7b7e96b"
            },
            {
                "title": "Analyzing traffic by domain name in the data plane",
                "abstract": "Associating network traffic with human-readable domain names, instead of low-level identifiers like IP addresses, is helpful for measuring traffic by domain name, rate-limiting packets by domain, and identifying IoT devices. However, existing monitoring techniques require examining traffic at an external compute node, introducing overhead and privacy risks. In this paper, we introduce Meta4, a framework for monitoring traffic by domain name in the data plane by extracting the client IP, server IP, and domain name from DNS response messages and associating the domain name with data traffic from the subsequent client-server session. A data-plane implementation has the benefits of running efficiently at line-rate, enabling the switch to take direct action on the packets (e.g., to rate-limit, block, or mark traffic based on the associated domain), and protecting the privacy of user information. We implemented Meta4 on an Intel Tofino switch and evaluated our prototype against packet traces from an operational network.",
                "paper_link": "https://www.semanticscholar.org/paper/a1d6466c1e024dc70b8765acc43b63ba7f66b860"
            },
            {
                "title": "Experience-driven research on programmable networks",
                "abstract": "Many promising networking research ideas in programmable networks never see the light of day. Yet, deploying research prototypes in production networks can help validate research ideas, improve them with faster feedback, uncover new research questions, and also ease the subsequent transition to practice. In this paper, we show how researchers can run and validate their research ideas in their own backyards---on their production campus networks---and we have seen that such a demonstrator can expedite the deployment of a research idea in practice to solve real network operation problems. We present P4Campus, a proof-of-concept that encompasses tools, an infrastructure design, strategies, and best practices---both technical and non-technical---that can help researchers run experiments against their programmable network idea in their own network. We use network tapping devices, packet brokers, and commodity programmable switches to enable running experiments to evaluate research ideas on a production campus network. We present several compelling data-plane applications as use cases that run on our campus and solve production network problems. By sharing our experiences and open-sourcing our P4 apps [28], we hope to encourage similar efforts on other campuses.",
                "paper_link": "https://www.semanticscholar.org/paper/5d1239bb6cb81fcc95e435884d910acefc0d832a"
            },
            {
                "title": "Passive OS fingerprinting on commodity switches",
                "abstract": "Operating System (OS) fingerprinting allows network administrators to identify which operating systems are running on the hosts communicating over their network. This information is useful for detecting OS-specific vulnerabilities and for administering OS-related security policies that block, rate-limit, or redirect traffic. Passive fingerprinting can identify hosts\u2019 OS types without active probes that introduce additional network load. However, existing software-based passive fingerprinting tools cannot keep up with the traffic in high-speed networks. This paper presents P40f, a tool that runs on programmable switch hardware to perform OS fingerprinting and apply security policies at line rate. Unlike p0f, P40f can fingerprint devices\u2019 OS types and react to it (e.g., drop, rate-limit) in real time directly in the switch, without requiring any control-plane messages. P40f is a P4 implementation of an existing software tool, p0f. We present our prototype implemented with the P4 language, which compiles and runs on the Intel Tofino switch. We present experiments against packet traces from a real campus network, and make our code publicly available.",
                "paper_link": "https://www.semanticscholar.org/paper/df011ef0b69e58a0db0d02714d561e9c0132e57b"
            },
            {
                "title": "Traffic refinery: Cost-aware data representation for machine learning on network traffic",
                "abstract": "Network management often relies on machine learning to make predictions about performance and security from network traffic. Often, the representation of the traffic is as important as the choice of the model. The features that the model relies on, and the representation of those features, ultimately determine model accuracy, as well as where and whether the model can be deployed in practice. Thus, the design and evaluation of these models ultimately requires understanding not only model accuracy but also the systems costs associated with deploying the model in an operational network. Towards this goal, this paper develops a new framework and system that enables a joint evaluation of both the conventional notions of machine learning performance (model accuracy) and the systems-level costs of different representations of network traffic. We highlight these two dimensions for two practical network management tasks, video streaming quality inference and malware detection, to demonstrate the importance of exploring different representations to find the appropriate operating point. We demonstrate the benefit of exploring a range of representations of network traffic and present Traffic Refinery, a proof-of-concept implementation that both monitors network traffic at 10~Gbps and transforms traffic in real time to produce a variety of feature representations for machine learning. Traffic Refinery both highlights this design space and makes it possible to explore different representations for learning, balancing systems costs related to feature extraction and model training against model accuracy.",
                "paper_link": "https://www.semanticscholar.org/paper/f62e144f3b06a60ef80375d445ea3558260b527a"
            },
            {
                "title": "Programmable in-network obfuscation of traffic",
                "abstract": "Recent advances in programmable switch hardware offer a fresh opportunity to protect user privacy. This paper presents PINOT, a lightweight in-network anonymity solution that runs at line rate within the memory and processing constraints of hardware switches. PINOT encrypts a client's IPv4 address with an efficient encryption scheme to hide the address from downstream ASes and the destination server. PINOT is readily deployable, requiring no end-user software or cooperation from networks other than the trusted network where it runs. We implement a PINOT prototype on the Barefoot Tofino switch, deploy PINOT in a campus network, and present results on protecting user identity against public DNS, NTP, and WireGuard VPN services.",
                "paper_link": "https://www.semanticscholar.org/paper/08cd6b776649cc885ebc79041b354fea4e45a1de"
            },
            {
                "title": "Programmable in-network obfuscation of DNS traffic",
                "abstract": "In conventional DNS, or Do53, requests and responses are sent in cleartext. Thus, DNS recursive resolvers or any on-path adversaries can access privacy-sensitive information. To address this issue, several encryption-based approaches (e.g., DNS-over-HTTPS) and proxy-based approaches (e.g., Obliv-ious DNS) were proposed. However, encryption-based approaches put too much trust in recursive resolvers. Proxy-based approaches can help hide the client\u2019s identity, but sets a higher deployment barrier while also introducing noticeable performance overhead. We propose PINOT, a packet-header obfuscation system that runs entirely in the data plane of a programmable network switch, which provides a lightweight, low-deployment-barrier anonymization service for clients sending and receiving DNS packets. PINOT does not require any modification to the DNS protocol or additional client software installation or proxy setup. Yet, it can also be combined with existing approaches to provide stronger privacy guarantees. We implement a PINOT prototype on a commodity switch, deploy it in a campus network, and present results on protecting user identity against public DNS services.",
                "paper_link": "https://www.semanticscholar.org/paper/44eac06bc2bfa59f9a15d2d56d315c55807146a9"
            },
            {
                "title": "Simpler network configuration with state-based network policies",
                "abstract": "Operators make hundreds of changes to a network\u2019s router and switch configurations every day\u2014a painstaking, errorprone process. If the network configuration could instead encode different forwarding behavior for different network states a priori, a network controller could automatically alter forwarding behavior when conditions change. To enable this capability, we introduce state-based network policies, which describe how a network\u2019s forwarding behavior should change in response to arbitrary network events. A state-based network policy comprises many tasks, each of which encodes the forwarding behavior for a single network management operation (e.g., intrusion detection) or part of the network (e.g., a sub-organization), and how that behavior should change when network conditions change. Composing these policies produces a networkwide control program that adapts to different operating conditions. We implement state-based network policies in a system called PyResonance and demonstrate with real-world examples and use cases that PyResonance is expressive enough to specify a wide range of network policies and simple enough for many operators to use. Our evaluation based on event traces from the Georgia Tech campus network shows that PyResonance can achieve good performance in operational settings.",
                "paper_link": "https://www.semanticscholar.org/paper/aa962b7a0f129748303c24ad1389c3c15f65d495"
            },
            {
                "title": "Predictive validity of radiographic signs of complete discoid lateral meniscus in children using machine learning techniques",
                "abstract": "The diagnostic utility of radiographic signs of complete discoid lateral meniscus remains controversial. This study aimed to investigate the diagnostic accuracy and determine which sign is most reliably detects the presence of a complete discoid lateral meniscus in children. A total of 141 knees (age 7\u201016) with complete discoid lateral meniscus and 141 age\u2010 and sex\u2010matched knees with normal meniscus were included. The following radiographic signs were evaluated: lateral joint (LJ) space, fibular head (FH) height, lateral tibial spine (LTS) height, lateral tibial plateau (LTP) obliquity, lateral femoral condyle (LFC) squaring, LTP cupping, LFC notching, and prominence ratio of the femoral condyle. Prediction models were constructed using logistic regressions, decision trees, and random forest analyses. Receiver operating characteristic curves and area under the curve (AUC) were estimated to compare the diagnostic accuracy of the radiographic signs and model fit. The random forest model yielded the best diagnostic accuracy (AUC: 0.909), with 86.5% sensitivity and 82.2% specificity. LJ space height, FH height, and prominence ratio showed statistically large AUC compared with LTS height and LTP obliquity (P\u2009<\u2009.05 in all). The cut\u2010off values for diagnosing discoid meniscus to be <12.55\u2009mm for FH height, <0.804 for prominence ratio, and >6.6\u2009mm for LJ space height when using the random forest model. On the basis of the results of this study, in clinical practice, LJ space height, FH height and prominence ratio could be easily used as supplementary tools for complete discoid lateral meniscus in children.",
                "paper_link": "https://www.semanticscholar.org/paper/95a18634340cc0d9fff3bf76b2a2081b5a5b98da"
            }
        ]
    },
    {
        "Professor": "Yen-Ling Kuo",
        "Papers": [
            {
                "title": "Community-based game design: experiments on social games for commonsense data collection",
                "abstract": "Games with A Purpose have successfully harvested information from web users. However, designing games that encourage sustainable and quality data contribution remains a great challenge. Given that many online communities have enjoyed active participation from a loyal following, this research explores how human computation games may benefit from rich interactions inherent in a community. We experimented by implementing two games for commonsense data collection on the leading social community platforms: the Rapport Game on Facebook and the Virtual Pet Game on PTT. In this paper, we present the choices of interaction mode and goal-oriented user model for building a community-based game. The data quality, collection efficiency, player retention, concept diversity, and game stability of both games are analyzed quantitatively from data collected since August/November 2008. Our findings should provide useful suggestions for designing community-based games in the future.",
                "paper_link": "https://www.semanticscholar.org/paper/75ed0506fa56a1848cc5054713d79686fa77379b"
            },
            {
                "title": "Encoding formulas as deep networks: Reinforcement learning for zero-shot execution of LTL formulas",
                "abstract": "We demonstrate a reinforcement learning agent which uses a compositional recurrent neural network that takes as input an LTL formula and determines satisfying actions. The input LTL formulas have never been seen before, yet the network performs zero-shot generalization to satisfy them. This is a novel form of multi-task learning for RL agents where agents learn from one diverse set of tasks and generalize to a new set of diverse tasks. The formulation of the network enables this capacity to generalize. We demonstrate this ability in two domains. In a symbolic domain, the agent finds a sequence of letters that is accepted. In a Minecraft-like environment, the agent finds a sequence of actions that conform to the formula. While prior work could learn to execute one formula reliably given examples of that formula, we demonstrate how to encode all formulas reliably. This could form the basis of new multitask agents that discover sub-tasks and execute them without any additional training, as well as the agents which follow more complex linguistic commands. The structures required for this generalization are specific to LTL formulas, which opens up an interesting theoretical question: what structures are required in neural networks for zero-shot generalization to different logics?",
                "paper_link": "https://www.semanticscholar.org/paper/025d142e712c5ddf97157e62765ff518da6ef9b2"
            },
            {
                "title": "Learning a natural-language to LTL executable semantic parser for grounded robotics",
                "abstract": "Children acquire their native language with apparent ease by observing how language is used in context and attempting to use it themselves. They do so without laborious annotations, negative examples, or even direct corrections. We take a step toward robots that can do the same by training a grounded semantic parser, which discovers latent linguistic representations that can be used for the execution of natural-language commands. In particular, we focus on the difficult domain of commands with a temporal aspect, whose semantics we capture with Linear Temporal Logic, LTL. Our parser is trained with pairs of sentences and executions as well as an executor. At training time, the parser hypothesizes a meaning representation for the input as a formula in LTL. Three competing pressures allow the parser to discover meaning from language. First, any hypothesized meaning for a sentence must be permissive enough to reflect all the annotated execution trajectories. Second, the executor -- a pretrained end-to-end LTL planner -- must find that the observed trajectories are likely executions of the meaning. Finally, a generator, which reconstructs the original input, encourages the model to find representations that conserve knowledge about the command. Together these ensure that the meaning is neither too general nor too specific. Our model generalizes well, being able to parse and execute both machine-generated and human-generated commands, with near-equal accuracy, despite the fact that the human-generated sentences are much more varied and complex with an open lexicon. The approach presented here is not specific to LTL; it can be applied to any domain where sentence meanings can be hypothesized and an executor can verify these meanings, thus opening the door to many applications for robotic agents.",
                "paper_link": "https://www.semanticscholar.org/paper/44223a1e410cae844952806a754cc9fe50deb930"
            },
            {
                "title": "Deep compositional robotic planners that follow natural language commands",
                "abstract": "We demonstrate how a sampling-based robotic planner can be augmented to learn to understand a sequence of natural language commands in a continuous configuration space to move and manipulate objects. Our approach combines a deep network structured according to the parse of a complex command that includes objects, verbs, spatial relations, and attributes, with a sampling-based planner, RRT. A recurrent hierarchical deep network controls how the planner explores the environment, determines when a planned path is likely to achieve a goal, and estimates the confidence of each move to trade off exploitation and exploration between the network and the planner. Planners are designed to have near-optimal behavior when information about the task is missing, while networks learn to exploit observations which are available from the environment, making the two naturally complementary. Combining the two enables generalization to new maps, new kinds of obstacles, and more complex sentences that do not occur in the training set. Little data is required to train the model despite it jointly acquiring a CNN that extracts features from the environment as it learns the meanings of words. The model provides a level of interpretability through the use of attention maps allowing users to see its reasoning steps despite being an end-to-end model. This end-to-end model allows robots to learn to follow natural language commands in challenging continuous environments.",
                "paper_link": "https://www.semanticscholar.org/paper/8b46dfd64d6a24c24e231dec6f3c868d8c540384"
            },
            {
                "title": "Deep sequential models for sampling-based planning",
                "abstract": "We demonstrate how a sequence model and a sampling-based planner can influence each other to produce efficient plans and how such a model can automatically learn to take advantage of observations of the environment. Sampling-based planners such as RRT generally know nothing of their environments even if they have traversed similar spaces many times. A sequence model, such as an HMM or LSTM, guides the search for good paths. The resulting model, called DeRRT*, observes the state of the planner and the local environment to bias the next move and next planner state. The neural-network-based models avoid manual feature engineering by co-training a convolutional network which processes map features and observations from sensors. We incorporate this sequence model in a manner that combines its likelihood with the existing bias for searching large unexplored Voronoi regions. This leads to more efficient trajectories with fewer rejected samples even in difficult domains such as when escaping bug traps. This model can also be used for dimensionality reduction in multi-agent environments with dynamic obstacles. Instead of planning in a high-dimensional space that includes the configurations of the other agents, we plan in a low-dimensional subspace relying on the sequence model to bias samples using the observed behavior of the other agents. The techniques presented here are general, include both graphical models and deep learning approaches, and can be adapted to a range of planners.",
                "paper_link": "https://www.semanticscholar.org/paper/51aa54e14983f4e17a542f459fc8f9e062958aa0"
            },
            {
                "title": "Social interactions as recursive mdps",
                "abstract": ",",
                "paper_link": "https://www.semanticscholar.org/paper/80d73dee7c10df046476c9200a4a56c8b7a9174e"
            },
            {
                "title": "Reconstructing action-conditioned human-object interactions using commonsense knowledge priors",
                "abstract": "We present a method for inferring diverse 3D models of human-object interactions from images. Reasoning about how humans interact with objects in complex scenes from a single 2D image is a challenging task given ambiguities arising from the loss of information through projection. In addition, modeling 3D interactions requires the generalization ability towards diverse object categories and interaction types. We propose an action-conditioned modeling of interactions that allows us to infer diverse 3D arrangements of humans and objects without supervision on contact regions or 3D scene geometry. Our method extracts high-level commonsense knowledge from large language models (such as GPT-3), and applies them to perform 3D reasoning of human-object interactions. Our key insight is priors extracted from large language models can help in reasoning about human-object contacts from textural prompts only. We quantitatively evaluate the inferred 3D models on a large human-object interaction dataset and show how our method leads to better 3D reconstructions. We further qualitatively evaluate the effectiveness of our method on real images and demonstrate its generalizability towards interaction types and object categories.",
                "paper_link": "https://www.semanticscholar.org/paper/bb26db1a4af5b3199d4b9a4767fa12c23507b40f"
            },
            {
                "title": "Compositional networks enable systematic generalization for grounded language understanding",
                "abstract": "Humans are remarkably flexible when understanding new sentences that include combinations of concepts they have never encountered before. Recent work has shown that while deep networks can mimic some human language abilities when presented with novel sentences, systematic variation uncovers the limitations in the language-understanding abilities of neural networks. We demonstrate that these limitations can be overcome by addressing the generalization challenges in a recently-released dataset, gSCAN, which explicitly measures how well a robotic agent is able to interpret novel ideas grounded in vision, e.g., novel pairings of adjectives and nouns. The key principle we employ is compositionality: that the compositional structure of networks should reflect the compositional structure of the problem domain they address, while allowing all other parameters and properties to be learned end-to-end with weak supervision. We build a general-purpose mechanism that enables robots to generalize their language understanding to compositional domains. Crucially, our base network has the same state-of-the-art performance as prior work, 97% execution accuracy, while at the same time generalizing its knowledge when prior work does not; for example, achieving 95% accuracy on novel adjective-noun compositions where previous work has 55% average accuracy. Robust language understanding without dramatic failures and without corner causes is critical to building safe and fair robots; we demonstrate the significant role that compositionality can play in achieving that goal.",
                "paper_link": "https://www.semanticscholar.org/paper/3268a9371aad1bc8ba1458338ab184ef000a9525"
            },
            {
                "title": "Trajectory prediction with linguistic representations",
                "abstract": "Language allows humans to build mental models that interpret what is happening around them resulting in more accurate long-term predictions. We present a novel trajectory prediction model that uses linguistic intermediate representations to forecast trajectories, and is trained using trajectory samples with partially-annotated captions. The model learns the meaning of each of the words without direct per-word supervision. At inference time, it generates a linguistic description of trajectories which captures maneuvers and interactions over an extended time interval. This generated description is used to refine predictions of the trajectories of multiple agents. We train and validate our model on the Argoverse dataset, and demonstrate improved accuracy results in trajectory prediction. In addition, our model is more interpretable: it presents part of its reasoning in plain language as captions, which can aid model development and can aid in building confidence in the model before deploying it.",
                "paper_link": "https://www.semanticscholar.org/paper/43dcfbd1090ff8905dc08db48723941c0f6a9898"
            },
            {
                "title": "Mmtom-qa: Multimodal theory of mind question answering",
                "abstract": "Theory of Mind (ToM), the ability to understand people's mental states, is an essential ingredient for developing machines with human-level social intelligence. Recent machine learning models, particularly large language models, seem to show some aspects of ToM understanding. However, existing ToM benchmarks use unimodal datasets - either video or text. Human ToM, on the other hand, is more than video or text understanding. People can flexibly reason about another person's mind based on conceptual representations (e.g., goals, beliefs, plans) extracted from any available data. To address this, we introduce a multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA comprehensively evaluates machine ToM both on multimodal data and on different kinds of unimodal data about a person's activity in a household environment. To engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian Inverse Planning Accelerated by Language Models). BIP-ALM extracts unified representations from multimodal data and utilizes language models for scalable Bayesian inverse planning. We conducted a systematic comparison of human performance, BIP-ALM, and state-of-the-art models, including GPT-4. The experiments demonstrate that large language models and large multimodal models still lack robust ToM capacity. BIP-ALM, on the other hand, shows promising results, by leveraging the power of both model-based mental inference and language models.",
                "paper_link": "https://www.semanticscholar.org/paper/18155f3818e12f33d926f180b92a5c1f68e22f93"
            },
            {
                "title": "Contextual restaurant recommendation utilizing implicit feedback",
                "abstract": "Selecting a good, appropriate restaurant for an event is a common problem for most people. In addition to the main features of restaurants (e.g. food style, price, and taste), a good recommendation system should also consider diners' context information. Although there are many context-aware restaurant recommenders, most of them only focus on location information. This research aims to incorporate a greater variety of useful contexts into the recommendation process. Instead of explicit user restaurant ratings, our system relies on diners' restaurant booking logs to recommend restaurants. Each booking record contains the dining context: event type, dining time, number of diners, etc. In this paper, we propose using the canonical decomposition Bayesian personalized ranking (CD-BPR) algorithm to model the context information in a restaurant booking record. Experiments were conducted using three years of booking logs from EZTable, the largest online restaurant booking service in Taiwan. Experiment results show that adding context information into BPR significantly outperforms the baseline BPR method.",
                "paper_link": "https://www.semanticscholar.org/paper/cbff030a3453472efceb060e1582436c7cc14106"
            },
            {
                "title": "Goal-oriented knowledge collection",
                "abstract": "Games with A Purpose (GWAP) has been demonstrated to be efficient in collecting large amount of knowledge from online users, e.g. Verbosity and Virtual Pet game. However, its effectiveness in knowledge base (KB) construction has not been explored in previous research. This paper examines the knowledge collected in the Virtual Pet game and presents an approach to collect more knowledge driven by the existing relations in KB. In this paper, goal-oriented knowledge collection successfully draws 10572 answers for the \u201cfood\u201d domain. The answers are verified by online voting to show that 92.07% of them are good sentences and 95.89% of them are new sentences. This result is a significant improvement over the original Virtual Pet game, with 80.58% good sentences and 67.56% weekly new information.",
                "paper_link": "https://www.semanticscholar.org/paper/c7a12d4bead0fa080b42fb68ce8178bbee85e7a3"
            },
            {
                "title": "Summarize the past to predict the future: Natural language descriptions of context boost multimodal object interaction",
                "abstract": "We study object interaction anticipation in egocentric videos. This task requires an understanding of the spatio-temporal context formed by past actions on objects, coined action context. We propose TransFusion, a multimodal transformer-based architecture for short-term object interaction anticipation. Our method exploits the representational power of language by summarizing the action con-text textually, after leveraging pre-trained vision-language foundation models to extract the action context from past video frames. The summarized action context and the last observed video frame are processed by the multimodal fusion module to forecast the next object interaction. Experiments on the Ego4D next active object interaction dataset show the effectiveness of our multimodal fusion model and highlight the benefits of using the power of foundation models and language-based context summaries in a task where vision may appear to suffice. Our novel approach outperforms all state-of-the-art methods on both versions of the Ego4D dataset. A project video and code are available at https://eth-ait.github.io/transfusion-proj/.",
                "paper_link": "https://www.semanticscholar.org/paper/7df705d1f31508e570483f82aeeb83cf146d2098"
            },
            {
                "title": "Incorporating rich social interactions into mdps",
                "abstract": "Much of what we do as humans is engage socially with other agents, a skill that robots must also eventually possess. We demonstrate that a rich theory of social interactions originating from microsociology can be formalized by extending a nested MDP where agents reason about arbitrary functions of each other's rewards. This extended Social MDP allows us to encode the five basic interactions that underlie microsociology: cooperation, conflict, coercion, competition, and exchange. The result is a robotic agent capable of executing social interactions in new environments with no interaction-specific training; like humans it can engage socially in novel ways even without a single example of that social interaction. Moreover, the estimations of these Social MDPs align closely with the judge-ments of humans when considering which social interaction is taking place in an environment. This method both sheds light on the nature of social interactions, by providing concrete mathematical definitions, and brings rich social interactions into a mathematical framework that has proven to be natural for robotics.",
                "paper_link": "https://www.semanticscholar.org/paper/3e3159d6d4cca058c5b9b19d9e1f2f946ec181e5"
            },
            {
                "title": "Contextual commonsense knowledge acquisition from social content by crowd-sourcing explanations",
                "abstract": "Contextual knowledge is essential in answering questions given speci\ufb01c observations. While recent approaches to building commonsense knowledge bases via text mining and/or crowdsourcing are successful, contextual knowledge is largely missing. To address this gap, this paper presents SocialExplain , a novel approach to acquiring contextual commonsense knowledge from explanations of social content. The acquisition process is broken into two cognitively simple tasks: to identify contextual clues from the given social content, and to explain the content with the clues. An ex-periment was conducted to show that multiple pieces of contextual commonsense knowledge can be identi-\ufb01ed from a small number of tweets. Online users ver-i\ufb01ed that 92.45% of the acquired sentences are good, and 95.92% are new sentences compared with existing crowd-sourced commonsense knowledge bases.",
                "paper_link": "https://www.semanticscholar.org/paper/4d53bfa875906f5bd696bd8353c5da003d82f640"
            },
            {
                "title": "Bridging common sense knowledge bases with analogy by graph similarity",
                "abstract": "Present-day programs are brittle as computers are notoriously lacking in common sense. While significant progress has been made in building large common sense knowledge bases, they are intrinsically incomplete and inconsistent. This paper presents a novel approach to bridging the gaps between multiple knowledge bases, making it possible to answer queries based on knowledge collected from multiple sources without a common ontology. New assertions are found by computing graph similarity with principle component analysis to draw analogies across multiple knowledge bases. Experiments are designed to find new assertions for a Chinese commonsense knowledge base using the OMCS ConceptNet and similarly for WordNet. The assertions are voted by online users to verify that 75.77% / 77.59% for Chinese ConceptNet / WordNet respectively are good, despite the low overlap in coverage among the knowledge bases.",
                "paper_link": "https://www.semanticscholar.org/paper/505098b9d874fbb01715e1f5fcf3a5a717e1f15c"
            },
            {
                "title": "Resource-bounded crowd-sourcing of commonsense knowledge",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/d05121c96dec542f598521d6d5425c4e27ca1bcf"
            },
            {
                "title": "Deep sequential models for sampling-based planning. In 2018 IEEE",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Compositional rl agents that follow language commands in temporal logic",
                "abstract": "We demonstrate how a reinforcement learning agent can use compositional recurrent neural networks to learn to carry out commands specified in linear temporal logic (LTL). Our approach takes as input an LTL formula, structures a deep network according to the parse of the formula, and determines satisfying actions. This compositional structure of the network enables zero-shot generalization to significantly more complex unseen formulas. We demonstrate this ability in multiple problem domains with both discrete and continuous state-action spaces. In a symbolic domain, the agent finds a sequence of letters that satisfy a specification. In a Minecraft-like environment, the agent finds a sequence of actions that conform to a formula. In the Fetch environment, the robot finds a sequence of arm configurations that move blocks on a table to fulfill the commands. While most prior work can learn to execute one formula reliably, we develop a novel form of multi-task learning for RL agents that allows them to learn from a diverse set of tasks and generalize to a new set of diverse tasks without any additional training. The compositional structures presented here are not specific to LTL, thus opening the path to RL agents that perform zero-shot generalization in other compositional domains.",
                "paper_link": "https://www.semanticscholar.org/paper/ebc22c739d7d8ec93d29c3fcf3b7c9f16d5563ba"
            },
            {
                "title": "IntuModels: Enabling Interactive Modeling for the Novice through Idea Generation and Selection",
                "abstract": "We present IntuModels, a machine-assisted interactive modeling workflow to enable the novice to create 3D models. The workflow uses a phase-driven approach, including idea generation and selection, to stimulate creativity and assist users who are not familiar with generating design ideas and 3D modeling techniques. By transforming parametric models using continual input data controlled by users, IntuModels motivates users to intuitively generate a huge amount of 3D model options with a good diversity. For selecting from the created models, we design a balanced overview showing the models using clustering and staged tools to help the user view and understand the models correctly. We tested IntuModels with a sample of novices who were asked to create a wire-based jewelry model. Presented in thematic networks and quantitative charts, the results showed that the novice considered IntuModels to be intuitive to use and useful for creating models that exceeded their expectations for post-production.",
                "paper_link": "https://www.semanticscholar.org/paper/70f17de362c52cbd20558b4880ce06a35f8ad16d"
            }
        ]
    },
    {
        "Professor": "Yonghwi Kwon",
        "Papers": [
            {
                "title": "Eavesdropping on fine-grained user activities within smartphone apps over encrypted network traffic",
                "abstract": "Smartphone apps have changed the way we interact with online services, but highly specialized apps come at a cost to privacy. In this paper we will demonstrate that a passive eavesdropper is capable of identifying finegrained user activities within the wireless network traffic generated by apps. Despite the widespread use of fully encrypted communication, our technique, called NetScope, is based on the intuition that the highly specific implementation of each app leaves a fingerprint on its traffic behavior (e.g., transfer rates, packet exchanges, and data movement). By learning the subtle traffic behavioral differences between activities (e.g., \"browsing\" versus \"chatting\" in a dating app), NetScope is able to perform robust inference of users' activities, for both Android and iOS devices, based solely on inspecting IP headers. Our evaluation with 35 widely popular app activities (ranging from social networking and dating to personal health and presidential campaigns) shows that NetScope yields high detection accuracy (78.04% precision and 76.04% recall on average).",
                "paper_link": "https://www.semanticscholar.org/paper/f935a3865b0c5e6cb94d1462a4931442873f2cc1"
            },
            {
                "title": "MCI: Modeling-based causality inference in audit logging for attack investigation",
                "abstract": "In this paper, we develop a model based causality inference technique for audit logging that does not require any application instrumentation or kernel modification. It leverages a recent dynamic analysis, dual execution (LDX), that can infer precise causality between system calls but unfortunately requires doubling the resource consumption such as CPU time and memory consumption. For each application, we use LDX to acquire precise causal models for a set of primitive operations. Each model is a sequence of system calls that have inter-dependences, some of them caused by memory operations and hence implicit at the system call level. These models are described by a language that supports various complexity such as regular, context-free, and even context-sensitive. In production run, a novel parser is deployed to parse audit logs (without any enhancement) to model instances and hence derive causality. Our evaluation on a set of real-world programs shows that the technique is highly effective. The generated models can recover causality with 0% false-positives (FP) and false-negatives (FN) for most programs and only 8.3% FP and 5.2% FN in the worst cases. The models also feature excellent composibility, meaning that the models derived from primitive operations can be composed together to describe causality for large and complex real world missions. Applying our technique to attack investigation shows that the system-wide attack causal graphs are highly precise and concise, having better quality than the state-of-the-art.",
                "paper_link": "https://www.semanticscholar.org/paper/b53c3f6c1440aa00bc140fa4b1ac3c8a20de1785"
            },
            {
                "title": "Kernel-supported cost-effective audit logging for causality tracking",
                "abstract": "The Linux Audit system is widely used as a causality tracking system in real-world deployments for problem diagnosis and forensic analysis. However, it has poor performance. We perform a comprehensive analysis on the Linux Audit system and find that it suffers from high runtime and storage overheads due to the large volume of redundant events. To address these shortcomings, we propose an in-kernel cache-based online log-reduction system to enable high-performance audit logging. It features a multi-layer caching scheme distributed in various kernel data structures, and uses the caches to detect and suppress redundant events. Our technique is designed to reduce the runtime overhead caused by transferring, processing, and writing logs, as well as the space overhead caused by storing them on disk. Compared to existing log reduction techniques that first generate the huge raw logs before reduction, our technique avoids generating redundant events at the first place. Our experimental results of the prototype KCAL (Kernel-supported Cost-effective Audit Logging) on one-month real-world workloads show that KCAL can reduce the runtime overhead from 40+% to 15-%, and reduce space consumption by 90% on average. KCAL achieves such a large reduction with 4% CPU consumption on average, whereas a state-of-the-art user space log-reduction technique has to occupy a processor with 95+% CPU consumption all the time.",
                "paper_link": "https://www.semanticscholar.org/paper/fb837b52ec26238f975db63d211ec979984f71f5"
            },
            {
                "title": "J-force: Forced execution on javascript",
                "abstract": "Web-based malware equipped with stealthy cloaking and obfuscation techniques is becoming more sophisticated nowadays. In this paper, we propose J-FORCE, a crash-free forced JavaScript execution engine to systematically explore possible execution paths and reveal malicious behaviors in such malware. In particular, J-FORCE records branch outcomes and mutates them for further explorations. J-FORCE inspects function parameter values that may reveal malicious intentions and expose suspicious DOM injections. We addressed a number of technical challenges encountered. For instance, we keep track of missing objects and DOM elements, and create them on demand. To verify the efficacy of our techniques, we apply J-FORCE to detect Exploit Kit (EK) attacks and malicious Chrome extensions. We observe that J-FORCE is more effective compared to the existing tools.",
                "paper_link": "https://www.semanticscholar.org/paper/f79b6150d51bbc79f459d25b6f0e7f697d46513a"
            },
            {
                "title": "LDX: Causality inference by lightweight dual execution",
                "abstract": "Causality inference, such as dynamic taint anslysis, has many applications (e.g., information leak detection). It determines whether an event e is causally dependent on a preceding event c during execution. We develop a new causality inference engine LDX. Given an execution, it spawns a slave execution, in which it mutates c and observes whether any change is induced at e. To preclude non-determinism, LDX couples the executions by sharing syscall outcomes. To handle path differences induced by the perturbation, we develop a novel on-the-fly execution alignment scheme that maintains a counter to reflect the progress of execution. The scheme relies on program analysis and compiler transformation. LDX can effectively detect information leak and security attacks with an average overhead of 6.08% while running the master and the slave concurrently on separate CPUs, much lower than existing systems that require instruction level monitoring. Furthermore, it has much better accuracy in causality inference.",
                "paper_link": "https://www.semanticscholar.org/paper/94247306a2c67d0ec3e631fc9ff848bc8218b5f0"
            },
            {
                "title": "Probabilistic disassembly",
                "abstract": "Disassembling stripped binaries is a prominent challenge for binary analysis, due to the interleaving of code segments and data, and the difficulties of resolving control transfer targets of indirect calls and jumps. As a result, most existing disassemblers have both false positives (FP) and false negatives (FN). We observe that uncertainty is inevitable in disassembly due to the information loss during compilation and code generation. Therefore, we propose to model such uncertainty using probabilities and propose a novel disassembly technique, which computes a probability for each address in the code space, indicating its likelihood of being a true positive instruction. The probability is computed from a set of features that are reachable to an address, including control flow and data flow features. Our experiments with more than two thousands binaries show that our technique does not have any FN and has only 3.7% FP. In comparison, a state-of-the-art superset disassembly technique has 85% FP. A rewriter built on our disassembly can generate binaries that are only half of the size of those by superset disassembly and run 3% faster. While many widely-used disassemblers such as IDA and BAP suffer from missing function entries, our experiment also shows that even without any function entry information, our disassembler can still achieve 0 FN and 6.8% FP.",
                "paper_link": "https://www.semanticscholar.org/paper/e8154c71ae8bdb1b7bed4fbe8fa36a0704a904e6"
            },
            {
                "title": "RevARM: A platform-agnostic ARM binary rewriter for security applications",
                "abstract": "ARM is the leading processor architecture in the emerging mobile and embedded market. Unfortunately, there has been a myriad of security issues on both mobile and embedded systems. While many countermeasures of such security issues have been proposed in recent years, a majority of applications still cannot be patched or protected due to run-time and space overhead constraints and the unavailability of source code. More importantly, the rapidly evolving mobile and embedded market makes any platform-specific solution ineffective. In this paper, we propose RevARM, a binary rewriting technique capable of instrumenting ARM-based binaries without limitation on the target platform. Unlike many previous binary instrumentation tools that are designed to instrument binaries based on x86, RevARM must resolve a number of new, ARM-specific binary rewriting challenges. Moreover, RevARM is able to handle stripped binaries, requires no symbolic/semantic information, and supports Mach-O binaries, overcoming the limitations of existing approaches. Finally, we demonstrate the capabilities of RevARM in solving real-world security challenges. Our evaluation results across a variety of platforms, including popular mobile and embedded systems, show that RevARM is highly effective in instrumenting ARM binaries with an average of 3.2% run-time and 1.3% space overhead.",
                "paper_link": "https://www.semanticscholar.org/paper/e305fdbbb78f62ae8cb5304f906022c3dfc2bcad"
            },
            {
                "title": "OSPREY: Recovery of Variable and Data Structure via Probabilistic Analysis for Stripped Binary",
                "abstract": "Recovering variables and data structure information from stripped binary is a prominent challenge in binary program analysis. While various state-of-the-art techniques are effective in specific settings, such effectiveness may not generalize. This is mainly because the problem is inherently uncertain due to the information loss in compilation. Most existing techniques are deterministic and lack a systematic way of handling such uncertainty. We propose a novel probabilistic technique for variable and structure recovery. Random variables are introduced to denote the likelihood of an abstract memory location having various types and structural properties such as being a field of some data structure. These random variables are connected through probabilistic constraints derived through program analysis. Solving these constraints produces the posterior probabilities of the random variables, which essentially denote the recovery results. Our experiments show that our technique substantially outperforms a number of state-of-the-art systems, including IDA, Ghidra, Angr, and Howard. Our case studies demonstrate the recovered information improves binary code hardening and binary decompilation.",
                "paper_link": "https://www.semanticscholar.org/paper/cab7ab16605bd116b9020f848e4b99836490d08a"
            },
            {
                "title": "WebRanz: web page randomization for better advertisement delivery and web-bot prevention",
                "abstract": "Nowadays, a rapidly increasing number of web users are using Ad-blockers to block online advertisements. Ad-blockers are browser-based software that can block most Ads on the websites, speeding up web browsers and saving bandwidth. Despite these benefits to end users, Ad-blockers could be catastrophic for the economic structure underlying the web, especially considering the rise of Ad-blocking as well as the number of technologies and services that rely exclusively on Ads to compensate their cost. In this paper, we introduce WebRanz that utilizes a randomization mechanism to circumvent Ad-blocking. Using WebRanz, content publishers can constantly mutate the internal HTML elements and element attributes of their web pages, without affecting their visual appearances and functionalities. Randomization invalidates the pre-defined patterns that Ad-blockers use to filter out Ads. Though the design of WebRanz is motivated by evading Ad-blockers, WebRanz also benefits the defense against bot scripts. We evaluate the effectiveness of WebRanz and its overhead using 221 randomly sampled top Alexa web pages and 8 representative bot scripts.",
                "paper_link": "https://www.semanticscholar.org/paper/5a5c5f5edc427ab356f6bad0eec2b9eccd1a14b5"
            },
            {
                "title": "TLS 1.3 in Practice: How TLS 1.3 Contributes to the Internet",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Trace: Enterprise-wide provenance tracking for real-time apt detection",
                "abstract": "We present TRACE, a comprehensive provenance tracking system for scalable, real-time, enterprise-wide APT detection. TRACE uses static analysis to identify program unit structures and inter-unit dependences, such that the provenance of an output event includes the input events within the same unit. Provenance collected from individual hosts are integrated to facilitate construction of a distributed enterprise-wide causal graph. We describe the evolution of TRACE over a four-year period, during which our improvements to the system focused on performance, scalability, and fidelity. In this time span, the system call coverage increased (from 47 to 66) while the time and space overhead reduced by over one and two orders of magnitude, respectively. We also provide results from five adversarial engagements where an independent team of system evaluators conducted APT attacks and assessed system performance. The input from our system was used by three other teams to implement real-time APT detection logic. Retrospective analysis revealed that TRACE provided sufficient evidence to detect over 80% of the attack stages across all evaluations. By the last engagement, temporal and spatial overhead had been reduced significantly to 18% and 10%, respectively.",
                "paper_link": "https://www.semanticscholar.org/paper/5849ddfd76c855c7194dd822484550505ae2a0ab"
            },
            {
                "title": "Apex: automatic programming assignment error explanation",
                "abstract": "This paper presents Apex, a system that can automatically generate explanations for programming assignment bugs, regarding where the bugs are and how the root causes led to the runtime failures. It works by comparing the passing execution of a correct implementation (provided by the instructor) and the failing execution of the buggy implementation (submitted by the student). The technique overcomes a number of technical challenges caused by syntactic and semantic differences of the two implementations. It collects the symbolic traces of the executions and matches assignment statements in the two execution traces by reasoning about symbolic equivalence. It then matches predicates by aligning the control dependences of the matched assignment statements, avoiding direct matching of path conditions which are usually quite different. Our evaluation shows that Apex is every effective for 205 buggy real world student submissions of 4 programming assignments, and a set of 15 programming assignment type of buggy programs collected from stackoverflow.com, precisely pinpointing the root causes and capturing the causality for 94.5% of them. The evaluation on a standard benchmark set with over 700 student bugs shows similar results. A user study in the classroom shows that Apex has substantially improved student productivity.",
                "paper_link": "https://www.semanticscholar.org/paper/dbaa39e95ef3fa66eb7c20ef52017d4774b34917"
            },
            {
                "title": "An Empirical Study of Bugs in WebAssembly Compilers",
                "abstract": "WebAssembly is the newest programming language for the Web. It defines a portable bytecode format for use as a compilation target for programs developed in high-level languages such as C, C++, and Rust. As a result, WebAssembly binaries are generally created by WebAssembly compilers rather than being written manually. To port native code to the Web, WebAssembly compilers need to address the differences between the source and target languages and dissimilarities in their execution environments. A deep understanding of the bugs in WebAssembly compilers can help compiler developers determine where to focus development and testing efforts. In this paper, we conduct two empirical studies to understand the characteristics of the bugs found in WebAssembly compilers. First, we perform a qualitative analysis of bugs in Emscripten, the most widely-used WebAssembly compiler. We investigate 146 bug reports in Emscripten related to the unique challenges WebAssembly compilers encounter compared with traditional compilers. Second, we provide a quantitative analysis of 1,054 bugs in three open-source WebAssembly compilers, AssemblyScript, Emscripten, and Rustc/Wasm-Bindgen. We analyze these bugs along three dimensions: lifecycle, impact, and sizes of bug-inducing inputs and bug fixes. These studies deepen our understanding of WebAssembly compiler bugs. We hope that the findings of our study will shed light on opportunities to design practical tools for testing and debugging WebAssembly compilers.",
                "paper_link": "https://www.semanticscholar.org/paper/86445e516829ecdc1da08c57ba92f7cf2f62149a"
            },
            {
                "title": "Security Analysis on Practices of Certificate Authorities in the HTTPS Phishing Ecosystem",
                "abstract": "Phishing attacks are causing substantial damage albeit extensive effort in academia and industry. Recently, a large volume of phishing attacks transit toward adopting HTTPS, leveraging TLS certificates issued from Certificate Authorities (CAs), to make the attacks more effective. In this paper, we present a comprehensive study on the security practices of CAs in the HTTPS phishing ecosystem. We focus on the CAs, critical actors under-studied in previous literature, to better understand the importance of the security practices of CAs and thwart the proliferating HTTPS phishing. In particular, we first present the current landscape and effectiveness of HTTPS phishing attacks comparing to traditional HTTP ones. Then, we conduct an empirical experiment on the CAs' security practices in terms of the issuance and revocation of the certificates. Our findings highlight serious conflicts between the expected security practices of CAs and reality, raising significant security concerns. We further validate our findings using a longitudinal dataset of abusive certificates used for real phishing attacks in the wild. We confirm that the security concerns of CAs prevail in the wild and these concerns can be one of the main contributors to the recent surge of HTTPS phishing attacks.",
                "paper_link": "https://www.semanticscholar.org/paper/bb5c4f21bbcdf941bd4d09919561326d44eb451f"
            },
            {
                "title": "BDA: practical dependence analysis for binary executables by unbiased whole-program path sampling and per-path abstract interpretation",
                "abstract": "Binary program dependence analysis determines dependence between instructions and hence is important for many applications that have to deal with executables without any symbol information. A key challenge is to identify if multiple memory read/write instructions access the same memory location. The state-of-the-art solution is the value set analysis (VSA) that uses abstract interpretation to determine the set of addresses that are possibly accessed by memory instructions. However, VSA is conservative and hence leads to a large number of bogus dependences and then substantial false positives in downstream analyses such as malware behavior analysis. Furthermore, existing public VSA implementations have difficulty scaling to complex binaries. In this paper, we propose a new binary dependence analysis called BDA enabled by a randomized abstract interpretation technique. It features a novel whole program path sampling algorithm that is not biased by path length, and a per-path abstract interpretation avoiding precision loss caused by merging paths in traditional analyses. It also provides probabilistic guarantees. Our evaluation on SPECINT2000 programs shows that it can handle complex binaries such as gcc whereas VSA implementations from the-state-of-art platforms have difficulty producing results for many SPEC binaries. In addition, the dependences reported by BDA are 75 and 6 times smaller than Alto, a scalable binary dependence analysis tool, and VSA, respectively, with only 0.19% of true dependences observed during dynamic execution missed (by BDA). Applying BDA to call graph generation and malware analysis shows that BDA substantially supersedes the commercial tool IDA in recovering indirect call targets and outperforms a state-of-the-art malware analysis tool Cuckoo by disclosing 3 times more hidden payloads.",
                "paper_link": "https://www.semanticscholar.org/paper/ac5d1f25679d38ab29036c8d157b192650e2e0b6"
            },
            {
                "title": "Pmp: Cost-effective forced execution with probabilistic memory pre-planning",
                "abstract": "Malware is a prominent security threat and exposing malware behavior is a critical challenge. Recent malware often has payload that is only released when certain conditions are satisfied. It is hence difficult to fully disclose the payload by simply executing the malware. In addition, malware samples may be equipped with cloaking techniques such as VM detectors that stop execution once detecting that the malware is being monitored. Forced execution is a highly effective method to penetrate malware self-protection and expose hidden behavior, by forcefully setting certain branch outcomes. However, an existing state-of-the-art forced execution technique X-Force is very heavyweight, requiring tracing individual instructions, reasoning about pointer alias relations on-the-fly, and repairing invalid pointers by on-demand memory allocation. We develop a light-weight and practical forced execution technique. Without losing analysis precision, it avoids tracking individual instructions and on-demand allocation. Under our scheme, a forced execution is very similar to a native one. It features a novel memory pre-planning phase that pre-allocates a large memory buffer, and then initializes the buffer, and variables in the subject binary, with carefully crafted values in a random fashion before the real execution. The pre-planning is designed in such a way that dereferencing an invalid pointer has a very large chance to fall into the pre-allocated region and hence does not cause any exception, and semantically unrelated invalid pointer dereferences highly likely access disjoint (pre-allocated) memory regions, avoiding state corruptions with probabilistic guarantees. Our experiments show that our technique is 84 times faster than X-Force, has 6.5X and 10% fewer false positives and negatives for program dependence detection, respectively, and can expose 98% more malicious behaviors in 400 recent malware samples.",
                "paper_link": "https://www.semanticscholar.org/paper/bc56c45cd7e540c8ae41c81e05a36ceaade1c9ac"
            },
            {
                "title": "Drivefuzz: Discovering autonomous driving bugs through driving quality-guided fuzzing",
                "abstract": "Autonomous driving has become real; semi-autonomous driving vehicles in an affordable price range are already on the streets, and major automotive vendors are actively developing full self-driving systems to deploy them in this decade. Before rolling the products out to the end-users, it is critical to test and ensure the safety of the autonomous driving systems, consisting of multiple layers intertwined in a complicated way. However, while safety-critical bugs may exist in any layer and even across layers, relatively little attention has been given to testing the entire driving system across all the layers. Prior work mainly focuses on white-box testing of individual layers and preventing attacks on each layer. In this paper, we aim at holistic testing of autonomous driving systems that have a whole stack of layers integrated in their entirety. Instead of looking into the individual layers, we focus on the vehicle states that the system continuously changes in the driving environment. This allows us to design DriveFuzz, a new systematic fuzzing framework that can uncover potential vulnerabilities regardless of their locations. DriveFuzz automatically generates and mutates driving scenarios based on diverse factors leveraging a high-fidelity driving simulator. We build novel driving test oracles based on the real-world traffic rules to detect safety-critical misbehaviors, and guide the fuzzer towards such misbehaviors through driving quality metrics referring to the physical states of the vehicle. DriveFuzz has discovered 30 new bugs in various layers of two autonomous driving systems (Autoware and CARLA Behavior Agent) and three additional bugs in the CARLA simulator. We further analyze the impact of these bugs and how an adversary may exploit them as security vulnerabilities to cause critical accidents in the real world.",
                "paper_link": "https://www.semanticscholar.org/paper/a2e273894b86b83969de3b483ec34a6030df4059"
            },
            {
                "title": "Dual execution for on the fly fine grained execution comparison",
                "abstract": "Execution comparison has many applications in debugging, malware analysis, software feature identification, and intrusion detection. Existing comparison techniques have various limitations. Some can only compare at the system event level and require executions to take the same input. Some require storing instruction traces that are very space-consuming and have difficulty dealing with non-determinism. In this paper, we propose a novel dual execution technique that allows on-the-fly comparison at the instruction level. Only differences between the executions are recorded. It allows executions to proceed in a coupled mode such that they share the same input sequence with the same timing, reducing nondeterminism. It also allows them to proceed in a decoupled mode such that the user can interact with each one differently. Decoupled executions can be recoupled to share the same future inputs and facilitate further comparison. We have implemented a prototype and applied it to identifying functional components for reuse, comparative debugging with new GDB primitives, and understanding real world regression failures. Our results show that dual execution is a critical enabling technique for execution comparison.",
                "paper_link": "https://www.semanticscholar.org/paper/fc40bb734808a5a479bfdb806f098dd4ed37109a"
            },
            {
                "title": "Swarmbug: debugging configuration bugs in swarm robotics",
                "abstract": "Swarm robotics collectively solve problems that are challenging for individual robots, from environmental monitoring to entertainment. The algorithms enabling swarms allow individual robots of the swarm to plan, share, and coordinate their trajectories and tasks to achieve a common goal. Such algorithms rely on a large number of configurable parameters that can be tailored to target particular scenarios. This large configuration space, the complexity of the algorithms, and the dependencies with the robots\u2019 setup and performance make debugging and fixing swarms configuration bugs extremely challenging. This paper proposes Swarmbug, a swarm debugging system that automatically diagnoses and fixes buggy behaviors caused by misconfiguration. The essence of Swarmbug is the novel concept called the degree of causal contribution (Dcc), which abstracts impacts of environment configurations (e.g., obstacles) to the drones in a swarm via behavior causal analysis. Swarmbug automatically generates, validates, and ranks fixes for configuration bugs. We evaluate Swarmbug on four diverse swarm algorithms. Swarmbug successfully fixes four configuration bugs in the evaluated algorithms, showing that it is generic and effective. We also conduct a real-world experiment with physical drones to show the Swarmbug\u2019s fix is effective in the real-world.",
                "paper_link": "https://www.semanticscholar.org/paper/532a5449fccc294c5fc61cada08cc7a894366220"
            },
            {
                "title": "TARDIS: Rolling back the clock on cms-targeting cyber attacks",
                "abstract": "Over 55% of the world\u2019s websites run on Content Management Systems (CMS). Unfortunately, this huge user population has made CMS-based websites a high-profile target for hackers. Worse still, the vast majority of the website hosting industry has shifted to a \"backup and restore\" model of security, which relies on error-prone AV scanners to prompt users to roll back to a pre-infection nightly snapshot. This research had the opportunity to study these nightly backups for over 300,000 unique production websites. In doing so, we measured the attack landscape of CMS-based websites and assessed the effectiveness of the backup and restore protection scheme. To our surprise, we found that the evolution of tens of thousands of attacks exhibited clear long-lived multi-stage attack patterns. We now propose TARDIS, an automated provenance inference technique, which enables the investigation and remediation of CMS-targeting attacks based on only the nightly backups already being collected by website hosting companies. With the help of our industry collaborator, we applied TARDIS to the nightly backups of those 300K websites and found 20,591 attacks which lasted from 6 to 1,694 days, some of which were still yet to be detected.",
                "paper_link": "https://www.semanticscholar.org/paper/a48a640d9df4de2db3034bf7fd29c6e53adca327"
            }
        ]
    },
    {
        "Professor": "Kyusang Lee",
        "Papers": [
            {
                "title": "Remote epitaxy through graphene enables two-dimensional material-based layer transfer",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/e3370f9d7ddae3b59118da5210f23451355d5635"
            },
            {
                "title": "Dynamic kirigami structures for integrated solar tracking",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/9c54a6aba0294d15c24033032a98022759ab92ba"
            },
            {
                "title": "Heterogeneous integration of single-crystalline complex-oxide membranes",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/60a6ef2f212a7c083676c156ad23347d368a8068"
            },
            {
                "title": "Controlled crack propagation for atomic precision handling of wafer-scale two-dimensional materials",
                "abstract": "Cleaving with a metal handle Using adhesive tape to pull off monolayers of two-dimensional (2D) materials is now a well-established approach. However, the flakes tend to be micrometer scale, and the creation of multilayer stacks for device application can be challenging and time consuming. Shim et al. show that monolayers of a variety of 2D materials, including molybdenum disulfide and hexagonal boron nitride, can be cleaved from multilayers grown as 5-centimeter-diameter wafers. The multilayer is capped with a nickel layer, which can be used to pull off the entire grown stack. The bottom of the stack is again capped with nickel, and a second round of cleaving leaves the monolayer on the bottom nickel layer. The monolayers could be transferred to other surfaces, which allowed the authors to make field-effect transistors with high charge-carrier mobilities. Science, this issue p. 665 Nickel overlayers transfer stress and enable cleavage of two-dimensional materials as monolayers at the wafer scale. Although flakes of two-dimensional (2D) heterostructures at the micrometer scale can be formed with adhesive-tape exfoliation methods, isolation of 2D flakes into monolayers is extremely time consuming because it is a trial-and-error process. Controlling the number of 2D layers through direct growth also presents difficulty because of the high nucleation barrier on 2D materials. We demonstrate a layer-resolved 2D material splitting technique that permits high-throughput production of multiple monolayers of wafer-scale (5-centimeter diameter) 2D materials by splitting single stacks of thick 2D materials grown on a single wafer. Wafer-scale uniformity of hexagonal boron nitride, tungsten disulfide, tungsten diselenide, molybdenum disulfide, and molybdenum diselenide monolayers was verified by photoluminescence response and by substantial retention of electronic conductivity. We fabricated wafer-scale van der Waals heterostructures, including field-effect transistors, with single-atom thickness resolution.",
                "paper_link": "https://www.semanticscholar.org/paper/868e355d54e9508888fb9620602f1db1f0f92623"
            },
            {
                "title": "Non-destructive wafer recycling for epitaxial lift-off thin-film device using a superlattice epitaxial layer",
                "abstract": "We demonstrate the integration of GaAs thin-film solar cells with low-cost plastic mini-compound parabolic concentrators (CPC) by combing a non-destructive epitaxial lift-off (ND-ELO) technique and a vacuum-assisted thermoforming process. To simplify the thin film III-V active layer transfer and to eliminate the use of adhesives of the epi to the secondary plastic substrate, we employ cold-weld bonding. The integration of thin film GaAs photovoltaic cells with a low-cost plastic mini CPC that can track solar radiation increases energy harvesting by a factor of 2. The Combination of cost effective concentrators with previously demonstrated non-destructive wafer recycling utilizing epitaxial protection layers provides the potential for a dramatic cost reduction in the production of III-V semiconductor photovoltaic cells.",
                "paper_link": "https://www.semanticscholar.org/paper/9377bef7439bb715114b69cafe095bd40ff9a3e1"
            },
            {
                "title": "Polarity governs atomic interaction through two-dimensional materials",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Epitaxial growth and layer-transfer techniques for heterogeneous integration of materials for electronic and photonic devices",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Chip-less wireless electronic skins by remote epitaxial freestanding compound semiconductors",
                "abstract": "Recent advances in flexible and stretchable electronics have led to a surge of electronic skin (e-skin)\u2013based health monitoring platforms. Conventional wireless e-skins rely on rigid integrated circuit chips that compromise the overall flexibility and consume considerable power. Chip-less wireless e-skins based on inductor-capacitor resonators are limited to mechanical sensors with low sensitivities. We report a chip-less wireless e-skin based on surface acoustic wave sensors made of freestanding ultrathin single-crystalline piezoelectric gallium nitride membranes. Surface acoustic wave\u2013based e-skin offers highly sensitive, low-power, and long-term sensing of strain, ultraviolet light, and ion concentrations in sweat. We demonstrate weeklong monitoring of pulse. These results present routes to inexpensive and versatile low-power, high-sensitivity platforms for wireless health monitoring devices. Description Chip-less electronic skin Flexible electronic materials, or e-skins, can be limited by the need to include rigid components. A range of techniques have emerged to bypass this problem, including approaches for wireless communication and charging based on silicon, carbon nanotubes, or conducting polymers. Kim et al. show that epitaxially grown, single-crystalline gallium nitride films on flexible substrates can be used for chip-less, flexible e-skins. The main advantage is that the material is flexible and breathable, thus providing better comfort. The devices convert electrical energy into surface acoustic waves using a piezoelectric resonator. The resonator is sensitive to changes in strain, mass changes due to the absorption or loss of ions, and ultraviolet light, all of which can be used for different sensing measurements. \u2014MSL Single-crystalline gallium nitride nanomembranes enable high-sensitivity surface acoustic wave sensors for wireless electronic skin.",
                "paper_link": "https://www.semanticscholar.org/paper/d382c692f02904d531bdc2d5b645ede5eb759107"
            },
            {
                "title": "Vertical full-colour micro-LEDs via 2D materials-based layer transfer",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Graphene-assisted spontaneous relaxation towards dislocation-free heteroepitaxy",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/5dabcd45dd1a70014b7938ad986f343afcb44032"
            },
            {
                "title": "Reuse of GaAs substrates for epitaxial lift-off by employing protection layers",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Recent progress in Van der Waals (vdW) heterojunction-based electronic and optoelectronic devices",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Transforming the cost of solar-to-electrical energy conversion: Integrating thin-film GaAs solar cells with non-tracking mini-concentrators",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/aa91831f7596db73fce6585a19a4c1657eea85e7"
            },
            {
                "title": "Non\u2010destructive wafer recycling for low\u2010cost thin\u2010film flexible optoelectronics",
                "abstract": "Compound semiconductors are the basis for many of the highest performance optical and electronic devices in use today. Their widespread commercial application has, however, been limited due to the high cost of substrates. Device costs can be significantly reduced if the substrate is reused in a simple, totally non\u2010destructive and rapid process. Here, a method that allows the indefinite reuse and recycling of wafers is demonstrated, employing a combination of epitaxial \u201cprotection layers\u201d, plasma cleaning techniques that return the wafers to their original, pristine, and epi\u2010ready condition following epitaxial layer removal, and adhesive\u2010free bonding to a secondary plastic substrate. The generality of this process is demonstrated by fabricating high performance GaAs\u2010based photovoltaic cells, light emitting diodes, and metal\u2010semiconductor field effect transistors that are transferred without loss of performance onto flexible and lightweight plastic substrates, and then the parent wafer is recycled for subsequent growth of additional device layers. This process potentially leads to a transformational change in device cost, arising from the inevitable consumption of the wafer that accompanies conventional epitaxial liftoff followed by chemo\u2010mechanical polishing.",
                "paper_link": "https://www.semanticscholar.org/paper/235dad99b6463955b7eb6a5c8dfb1eb2ab6c6dfb"
            },
            {
                "title": "Visible and infrared dual-band imaging via Ge/MoS2van der Waals heterostructure",
                "abstract": "Ge/MoS2 van der Waals heterostructure enables bias-dependent selective detection of visible and near infrared.",
                "paper_link": "https://www.semanticscholar.org/paper/a46bfb3e3b70fa84935b1ec4ce644634b5d93a81"
            },
            {
                "title": "Remote epitaxy",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/3ad4cb086855f31a876b6db5bca8d9c87d7c771e"
            },
            {
                "title": "Small\u2010Molecule Planar\u2010Mixed Heterojunction Photovoltaic Cells with Fullerene\u2010Based Electron Filtering Buffers",
                "abstract": "X. Xiao, Dr. J. D. Zimmerman, K. Lee, Prof. S. R. Forrest Department of Electrical Engineering and Computer Science University of Michigan Ann Arbor , MI , 48109 , USA E-mail: stevefor@umich.edu K. J. Bergemann, Prof. S. R. Forrest Department of Physics University of Michigan Ann Arbor , MI , 48109 , USA Prof. S. R. Forrest Department of Materials Science and Engineering University of Michigan Ann Arbor , MI , 48109 , USA",
                "paper_link": "https://www.semanticscholar.org/paper/1f8c8637561a456b74148e320515b03146195b5a"
            },
            {
                "title": "Air\u2010Operable, High\u2010Mobility Organic Transistors with Semifluorinated Side Chains and Unsubstituted Naphthalenetetracarboxylic Diimide Cores: High Mobility and Environmental and\u00a0\u2026",
                "abstract": "N,N\u2032\u2010bis(3\u2010(perfluoroctyl)propyl)\u20101,4,5,8\u2010naphthalenetetracarboxylic acid diimide (8\u20133\u2010NTCDI) was newly synthesized, as were related fluorooctylalkyl\u2010NTCDIs and alkyl\u2010NTCDIs. The 8\u20133\u2010NTCDI\u2010based organic thin\u2010film transistor (OTFT) on an octadecyltrimethoxysilane (OTS)\u2010treated Si/SiO2 substrate shows apparent electron mobility approaching 0.7 cm2 V\u20101s\u20101 in air. The fluorooctylethyl\u2010NTCDI (8\u20132\u2010NTCDI) and fluorooctylbutyl\u2010NTCDI (8\u20134\u2010NTCDI) had significantly inferior properties even though their chemical structures are only slightly different, and nonfluorinated decyl and undecyl NTCDIs did not operate predictably in air. From atomic force microscopy, the 8\u20133\u2010NTCDI active layer deposited with the substrate at 120 \u00b0C forms a polycrystalline film with grain sizes >4\u03bcm. Mobilities were stable in air for one week. After 100 days in air, the average mobility of three OTFTs decreased from 0.62 to 0.12 cm2 V\u20101s\u20101, but stabilized thereafter. The threshold voltage (VT) increased by 15 V in air, but only by 3 V under nitrogen, after one week. On/off ratios were stable in air throughout. We also investigated transistor stability to gate bias stress. The transistor on hexamethlydisilazane (HMDS) is more stable than that on OTS with mobility comparable to amorphous Si TFTs. VT shifts caused by ON (30 V) and OFF (\u201320 V) gate bias stress for the HMDS samples for 1 hour were 1.79 V and 1.27 V under N2, respectively, and relaxation times of 106 and 107 s were obtained using the stretched exponential model. These performances are promising for use in transparent display backplanes.",
                "paper_link": "https://www.semanticscholar.org/paper/ab3af82be39b3bd0a9b7685ecb6121e1fb42596e"
            },
            {
                "title": "Ultrahigh Deep-Ultraviolet Responsivity of a \u03b2-Ga2O3/MgO Heterostructure-Based Phototransistor",
                "abstract": "Deep-ultraviolet (DUV) photodetectors based on wide-band-gap semiconductors have attracted significant interest across a wide range of applications in the industrial, biological, environmental, and...",
                "paper_link": "https://www.semanticscholar.org/paper/b9df30f9f86c9b613198961f8ccf468871994b94"
            },
            {
                "title": "Thin-film architectures with high spectral selectivity for thermophotovoltaic cells",
                "abstract": "Thermophotovoltaic (TPV) systems are a promising technology for distributed conversion of high-temperature heat to electricity. To achieve high conversion efficiency, the transport of sub-bandgap radiation between the thermal emitter and PV cell should be suppressed. This can be achieved by recycling sub-bandgap radiation back to the emitter using a spectrally selective cell. However, conventional TPV cells exhibit limited sub-bandgap reflectance. Here we demonstrate thin-film In0.53Ga0.47As-based structures with high spectral selectivity, including record-high average sub-bandgap reflectance (96%). Selectivity is enabled by short optical paths through a high-quality material fabricated using epitaxial lift-off, high-reflectance back surfaces, and optimized interference. In addition, we use a parallel-plate TPV model to evaluate the impact of specific structural features on performance and to optimize the cell architecture. We show that a dielectric spacer between InGaAs and the Au back surface is an impo...",
                "paper_link": "https://www.semanticscholar.org/paper/1cb4969c7e92ea3cbfc55595f5a87f4961cb390a"
            }
        ]
    },
    {
        "Professor": "Jundong Li",
        "Papers": [
            {
                "title": "Feature Selection: A Data Perspective",
                "abstract": "The rapid advancement of Large Language Models (LLMs) has significantly influenced various domains, leveraging their exceptional few-shot and zero-shot learning capabilities. In this work, we aim to explore and understand the LLMs-based feature selection methods from a data-centric perspective. We begin by categorizing existing feature selection methods with LLMs into two groups: data-driven feature selection which requires numerical values of samples to do statistical inference and text-based feature selection which utilizes prior knowledge of LLMs to do semantical associations using descriptive context. We conduct experiments in both classification and regression tasks with LLMs in various sizes (e.g., GPT-4, ChatGPT and LLaMA-2). Our findings emphasize the effectiveness and robustness of text-based feature selection methods and showcase their potentials using a real-world medical application. We also discuss the challenges and future opportunities in employing LLMs for feature selection, offering insights for further research and development in this emerging field.",
                "paper_link": "https://www.semanticscholar.org/paper/5cb8f11c36acb61ebe4aa8d6ab1c442d5ad7c0f3"
            },
            {
                "title": "Label Informed Attributed Network Embedding",
                "abstract": "Attributed network embedding aims to seek low-dimensional vector representations for nodes in a network, such that original network topological structure and node attribute proximity can be preserved in the vectors. These learned representations have been demonstrated to be helpful in many learning tasks such as network clustering and link prediction. While existing algorithms follow an unsupervised manner, nodes in many real-world attributed networks are often associated with abundant label information, which is potentially valuable in seeking more effective joint vector representations. In this paper, we investigate how labels can be modeled and incorporated to improve attributed network embedding. This is a challenging task since label information could be noisy and incomplete. In addition, labels are completely distinct with the geometrical structure and node attributes. The bewildering combination of heterogeneous information makes the joint vector representation learning more difficult. To address these issues, we propose a novel Label informed Attributed Network Embedding (LANE) framework. It can smoothly incorporate label information into the attributed network embedding while preserving their correlations. Experiments on real-world datasets demonstrate that the proposed framework achieves significantly better performance compared with the state-of-the-art embedding algorithms.",
                "paper_link": "https://www.semanticscholar.org/paper/44044556dae0e21cab058c18f704b15d33bd17c5"
            },
            {
                "title": "A Survey of Learning Causality with Data: Problems and Methods",
                "abstract": "The era of big data provides researchers with convenient access to copious data. However, we often have little knowledge of such data. The increasing prevalence of massive data is challenging the traditional methods of learning causality because they were developed for the cases with limited amount of data and strong prior causal knowledge. This survey aims to close the gap between big data and learning causality with a comprehensive and structured review of both traditional and frontier methods followed by a discussion about some open problems of learning causality. We begin with preliminaries of learning causality. Then we categorize and revisit methods of learning causality for the typical problems and data types. After that, we discuss the connections between learning causality and machine learning. At the end, some open problems are presented to show the great potential of learning causality with data",
                "paper_link": "https://www.semanticscholar.org/paper/1e3ee4a75451ef74febb720a7bdda561f16b964a"
            },
            {
                "title": "Deep Anomaly Detection on Attributed Networks",
                "abstract": "Attributed networks are ubiquitous and form a critical component of modern information infrastructure, where additional node attributes complement the raw network structure in knowledge discovery. Recently, detecting anomalous nodes on attributed networks has attracted an increasing amount of research attention, with broad applications in various high-impact domains, such as cybersecurity, \ufb01-nance, and healthcare. Most of the existing attempts, however, tackle the problem with shallow learning mechanisms by ego-network or community analysis, or through subspace selection. Undoubtedly, these models cannot fully address the computational challenges on attributed networks. For example, they often su\ufb00er from the network sparsity and data nonlinearity issues, and fail to capture the complex interactions between di\ufb00erent information modalities, thus negatively impact the performance of anomaly detection. To tackle the aforementioned problems, in this paper, we study the anomaly detection problem on attributed networks by developing a novel deep model. In particular, our proposed deep model: (1) explicitly models the topological structure and nodal attributes seamlessly for node embedding learning with the prevalent graph convolutional network (GCN); and (2) is customized to address the anomaly detection problem by virtue of deep autoencoder that leverages the learned embeddings to reconstruct the original data. The synergy between GCN and autoencoder enables us to spot anomalies by measuring the reconstruction errors of nodes from both the structure and the attribute perspectives. Extensive experiments on real-world attributed network datasets demonstrate the e\ufb03cacy of our proposed algorithm.",
                "paper_link": "https://www.semanticscholar.org/paper/bcfdbb6b8911272139170ef4b24e31d9145093e7"
            },
            {
                "title": "Self-Supervised Multi-Channel Hypergraph Convolutional Network for Social Recommendation",
                "abstract": "Social relations are often used to improve recommendation quality when user-item interaction data is sparse in recommender systems. Most existing social recommendation models exploit pairwise relations to mine potential user preferences. However, real-life interactions among users are very complex and user relations can be high-order. Hypergraph provides a natural way to model high-order relations, while its potentials for improving social recommendation are under-explored. In this paper, we fill this gap and propose a multi-channel hypergraph convolutional network to enhance social recommendation by leveraging high-order user relations. Technically, each channel in the network encodes a hypergraph that depicts a common high-order user relation pattern via hypergraph convolution. By aggregating the embeddings learned through multiple channels, we obtain comprehensive user representations to generate recommendation results. However, the aggregation operation might also obscure the inherent characteristics of different types of high-order connectivity information. To compensate for the aggregating loss, we innovatively integrate self-supervised learning into the training of the hypergraph convolutional network to regain the connectivity information with hierarchical mutual information maximization. Extensive experiments on multiple real-world datasets demonstrate the superiority of the proposed model over the current SOTA methods, and the ablation study verifies the effectiveness and rationale of the multi-channel setting and the self-supervised task. The implementation of our model is available via https://github.com/Coder-Yu/RecQ.",
                "paper_link": "https://www.semanticscholar.org/paper/68c9d777b66c24967a7c808210093684d89eba10"
            },
            {
                "title": "Attributed Network Embedding for Learning in a Dynamic Environment",
                "abstract": "Network embedding leverages the node proximity manifested to learn a low-dimensional node vector representation for each node in the network. The learned embeddings could advance various learning tasks such as node classification, network clustering, and link prediction. Most, if not all, of the existing works, are overwhelmingly performed in the context of plain and static networks. Nonetheless, in reality, network structure often evolves over time with addition/deletion of links and nodes. Also, a vast majority of real-world networks are associated with a rich set of node attributes, and their attribute values are also naturally changing, with the emerging of new content patterns and the fading of old content patterns. These changing characteristics motivate us to seek an effective embedding representation to capture network and attribute evolving patterns, which is of fundamental importance for learning in a dynamic environment. To our best knowledge, we are the first to tackle this problem with the following two challenges: (1) the inherently correlated network and node attributes could be noisy and incomplete, it necessitates a robust consensus representation to capture their individual properties and correlations; (2) the embedding learning needs to be performed in an online fashion to adapt to the changes accordingly. In this paper, we tackle this problem by proposing a novel dynamic attributed network embedding framework - DANE. In particular, DANE first provides an offline method for a consensus embedding and then leverages matrix perturbation theory to maintain the freshness of the end embedding results in an online manner. We perform extensive experiments on both synthetic and real attributed networks to corroborate the effectiveness and efficiency of the proposed framework.",
                "paper_link": "https://www.semanticscholar.org/paper/736e8deabcae7e2f9eb6c41a1bfae1b5270a8dbd"
            },
            {
                "title": "Accelerated Attributed Network Embedding",
                "abstract": "Network embedding is to learn low-dimensional vector representations for nodes in a network. It has shown to be e\ufb00ective in a variety of tasks such as node classi\ufb01cation and link prediction. While embedding algorithms on pure networks have been intensively studied, in many real-world applications, nodes are often accompanied with a rich set of attributes or features, aka attributed networks. It has been observed that network topological structure and node attributes are often strongly correlated with each other. Thus modeling and incorporating node attribute proximity into network embedding could be potentially helpful, though non-trivial, in learning better vector representations. Meanwhile, real-world networks often contain a large number of nodes and features, which put demands on the scalability of embedding algorithms. To bridge the gap, in this paper, we propose an accelerated attributed network embedding algorithm AANE, which enables the joint learning process to be done in a distributed manner by decomposing the complex modeling and optimization into many sub-problems. Experimental results on several real-world datasets demonstrate the e\ufb00ectiveness and e\ufb03ciency of the proposed algorithm.",
                "paper_link": "https://www.semanticscholar.org/paper/0efb659b15737c76a2fc50010a694123f6c45f64"
            },
            {
                "title": "Challenges of Feature Selection for Big Data Analytics",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Self-Supervised Learning for Recommender Systems: A Survey",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Be More with Less: Hypergraph Attention Networks for Inductive Text Classification",
                "abstract": "Text classification is a critical research topic with broad applications in natural language processing. Recently, graph neural networks (GNNs) have received increasing attention in the research community and demonstrated their promising results on this canonical task. Despite the success, their performance could be largely jeopardized in practice since they are: (1) unable to capture high-order interaction between words; (2) inefficient to handle large datasets and new documents. To address those issues, in this paper, we propose a principled model -- hypergraph attention networks (HyperGAT), which can obtain more expressive power with less computational consumption for text representation learning. Extensive experiments on various benchmark datasets demonstrate the efficacy of the proposed approach on the text classification task.",
                "paper_link": "https://www.semanticscholar.org/paper/bba5e0ad118f323ff6c72536e2b5e1fb1b9c1ff9"
            },
            {
                "title": "Radar: Residual Analysis for Anomaly Detection in Attributed Networks",
                "abstract": "Attributed networks are pervasive in different domains, ranging from social networks, gene regulatory networks to financial transaction networks. This kind of rich network representation presents challenges for anomaly detection due to the heterogeneity of two data representations. A vast majority of existing algorithms assume certain properties of anomalies are given a prior. Since various types of anomalies in real-world attributed networks coexist, the assumption that priori knowledge regarding anomalies is available does not hold. In this paper, we investigate the problem of anomaly detection in attributed networks generally from a residual analysis perspective, which has been shown to be effective in traditional anomaly detection problems. However, it is a non-trivial task in attributed networks as interactions among instances complicate the residual modeling process. Methodologically, we propose a learning framework to characterize the residuals of attribute information and its coherence with network information for anomaly detection. By learning and analyzing the residuals, we detect anomalies whose behaviors are singularly different from the majority. Experiments on real datasets show the effectiveness and generality of the proposed framework.",
                "paper_link": "https://www.semanticscholar.org/paper/a5160db2d31d47545fb68a4a17580969e1e02f80"
            },
            {
                "title": "Multi-Label Informed Feature Selection",
                "abstract": "Multi-label learning has been extensively studied in the area of bioinformatics, information retrieval, multimedia annotation, etc. In multi-label learning, each instance is associated with multiple interdependent class labels, the label information can be noisy and incomplete. In addition, multilabeled data often has noisy, irrelevant and redundant features of high dimensionality. As an effective data preprocessing step, feature selection has shown its effectiveness to prepare high-dimensional data for numerous data mining and machine learning tasks. Most of existing multi-label feature selection algorithms either boil down to solving multiple single-labeled feature selection problems or directly make use of imperfect labels. Therefore, they may not be able to find discriminative features that are shared by multiple labels. In this paper, we propose a novel multi-label informed feature selection framework MIFS, which exploits label correlations to select discriminative features across multiple labels. Specifically, to reduce the negative effects of imperfect label information in finding label correlations, we decompose the multi-label information into a low-dimensional space and then employ the reduced space to steer the feature selection process. Empirical studies on real-world datasets demonstrate the effectiveness and efficiency of the proposed framework.",
                "paper_link": "https://www.semanticscholar.org/paper/d077b33bca908d373e1eb5ff75c3656c1e9321b0"
            },
            {
                "title": "Line Graph Neural Networks for Link Prediction",
                "abstract": "We consider the graph link prediction task, which is a classic graph analytical problem with many real-world applications. With the advances of deep learning, current link prediction methods commonly compute features from subgraphs centered at two neighboring nodes and use the features to predict the label of the link between these two nodes. In this formalism, a link prediction problem is converted to a graph classification task. In order to extract fixed-size features for classification, graph pooling layers are necessary in the deep learning model, thereby incurring information loss. To overcome this key limitation, we propose to seek a radically different and novel path by making use of the line graphs in graph theory. In particular, each node in a line graph corresponds to a unique edge in the original graph. Therefore, link prediction problems in the original graph can be equivalently solved as a node classification problem in its corresponding line graph, instead of a graph classification task. Experimental results on fourteen datasets from different applications demonstrate that our proposed method consistently outperforms the state-of-the-art methods, while it has fewer parameters and high training efficiency.",
                "paper_link": "https://www.semanticscholar.org/paper/ad720ec891fd143090de390a2e800eb8e9e6eb4d"
            },
            {
                "title": "ANOMALOUS: A Joint Modeling Approach for Anomaly Detection on Attributed Networks",
                "abstract": "The key point of anomaly detection on attributed networks lies in the seamless integration of network structure information and attribute information. A vast majority of existing works are mainly based on the Homophily assumption that implies the nodal attribute similarity of connected nodes. Nonetheless, this assumption is untenable in practice as the existence of noisy and structurally irrelevant attributes may adversely affect the anomaly detection performance. Despite the fact that recent attempts perform subspace selection to address this issue, these algorithms treat subspace selection and anomaly detection as two separate steps which often leads to suboptimal solutions. In this paper, we investigate how to fuse attribute and network structure information more synergistically to avoid the adverse effects brought by noisy and structurally irrelevant attributes. Methodologically, we propose a novel joint framework to conduct attribute selection and anomaly detection as a whole based on CUR decomposition and residual analysis. By filtering out noisy and irrelevant node attributes, we perform anomaly detection with the remaining representative attributes. Experimental results on both synthetic and real-world datasets corroborate the effectiveness of the proposed framework.",
                "paper_link": "https://www.semanticscholar.org/paper/864babdcb6d4c04ff0008eb4a8a848f2f55404db"
            },
            {
                "title": "Enhancing Social Recommendation with Adversarial Graph Convolutional Networks",
                "abstract": "Social recommender systems are expected to improve recommendation quality by incorporating social information when there is little user-item interaction data. However, recent reports from industry show that social recommender systems consistently fail in practice. According to the negative findings, the failure is attributed to: (1) A majority of users only have a very limited number of neighbors in social networks and can hardly benefit from social relations; (2) Social relations are noisy but they are indiscriminately used; (3) Social relations are assumed to be universally applicable to multiple scenarios while they are actually multi-faceted and show heterogeneous strengths in different scenarios. Most existing social recommendation models only consider the homophily in social networks and neglect these drawbacks. In this paper we propose a deep adversarial framework based on graph convolutional networks (GCN) to address these problems. Concretely, for (1) and (2), a GCN-based autoencoder is developed to augment the relation data by encoding high-order and complex connectivity patterns, and meanwhile is optimized subject to the constraint of reconstructing the social profile to guarantee the validity of the identified neighborhood. After obtaining enough purified social relations for each user, a GCN-based attentive social recommendation module is designed to address (3) by capturing the heterogeneous strengths of social relations. Finally, we adopt adversarial training to unify all the components by playing a Minimax game and ensure a coordinated effort to enhance recommendation performance. Extensive experiments on multiple open datasets demonstrate the superiority of our framework and the ablation study confirms the importance and effectiveness of each component.",
                "paper_link": "https://www.semanticscholar.org/paper/646c36c1f92bb6a8d4d06a36036351de8fe95a85"
            },
            {
                "title": "Interactive Anomaly Detection on Attributed Networks",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Graph Prototypical Networks for Few-Shot Learning on Attributed Networks",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Adaptive Implicit Friends Identification over Heterogeneous Network for Social Recommendation",
                "abstract": "The explicitly observed social relations from online social platforms have been widely incorporated into recommender systems to mitigate the data sparsity issue. However, the direct usage of explicit social relations may lead to an inferior performance due to the unreliability (e.g., noises) of observed links. To this end, the discovery of reliable relations among users plays a central role in advancing social recommendation. In this paper, we propose a novel approach to adaptively identify implicit friends toward discovering more credible user relations. Particularly, implicit friends are those who share similar tastes but could be distant from each other on the network topology of social relations. Methodologically, to find the implicit friends for each user, we first model the whole system as a heterogeneous information network, and then capture the similarity of users through the meta-path based embedding representation learning. Finally, based on the intuition that social relations have varying degrees of impact on different users, our approach adaptively incorporates different numbers of similar users as implicit friends for each user to alleviate the adverse impact of unreliable social relations for a more effective recommendation. Experimental analysis on three real-world datasets demonstrates the superiority of our method and explain why implicit friends are helpful in improving social recommendation.",
                "paper_link": "https://www.semanticscholar.org/paper/828b45be703bc839118dc0009c8b480e7b3f8f30"
            },
            {
                "title": "EDITS: Modeling and Mitigating Data Bias for Graph Neural Networks",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Individual Fairness for Graph Neural Networks: A Ranking based Approach",
                "abstract": "Recent years have witnessed the pivotal role of Graph Neural Networks (GNNs) in various high-stake decision-making scenarios due to their superior learning capability. Close on the heels of the successful adoption of GNNs in different application domains has been the increasing societal concern that conventional GNNs often do not have fairness considerations. Although some research progress has been made to improve the fairness of GNNs, these works mainly focus on the notion of group fairness regarding different subgroups defined by a protected attribute such as gender, age, and race. Beyond that, it is also essential to study the GNN fairness at a much finer granularity (i.e., at the node level) to ensure that GNNs render similar prediction results for similar individuals to achieve the notion of individual fairness. Toward this goal, in this paper, we make an initial investigation to enhance the individual fairness of GNNs and propose a novel ranking based framework---REDRESS. Specifically, we refine the notion of individual fairness from a ranking perspective, and formulate the ranking based individual fairness promotion problem. This naturally addresses the issue of Lipschitz constant specification and distance calibration resulted from the Lipschitz condition in the conventional individual fairness definition. Our proposed framework REDRESS encapsulates the GNN model utility maximization and the ranking-based individual fairness promotion in a joint framework to enable end-to-end training. It is noteworthy mentioning that REDRESS is a plug-and-play framework and can be easily generalized to any prevalent GNN architectures. Extensive experiments on multiple real-world graphs demonstrate the superiority of REDRESS in achieving a good balance between model utility maximization and individual fairness promotion. Our open source code can be found here: https://github.com/yushundong/REDRESS.",
                "paper_link": "https://www.semanticscholar.org/paper/fa98db551fdec0a4c5c1beb25f8aa3df378b8c02"
            }
        ]
    },
    {
        "Professor": "Wei-Kai Lin",
        "Papers": [
            {
                "title": "OptORAMa: optimal oblivious RAM",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/b68433a3fb84695bb7881042ec86d98bc54d12cb"
            },
            {
                "title": "Oblivious hashing revisited, and applications to asymptotically efficient ORAM and OPRAM",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/d7aded13dbbac4f277fc6ee567c2fa23566a5616"
            },
            {
                "title": "Doubly efficient private information retrieval and fully homomorphic RAM computation from ring LWE",
                "abstract": "A (single server) private information retrieval (PIR) allows a client to read data from a public database held on a remote server, without revealing to the server which locations she is reading. In a doubly efficient PIR (DEPIR), the database is first preprocessed, but the server can subsequently answer any client\u2019s query in time that is sub-linear in the database size. Prior work gave a plausible candidate for a public-key variant of DEPIR, where a trusted party is needed to securely preprocess the database and generate a corresponding public key for the clients; security relied on a new non-standard code-based assumption and a heuristic use of ideal obfuscation. In this work we construct the stronger unkeyed notion of DEPIR, where the preprocessing is a deterministic procedure that the server can execute on its own. Moreover, we prove security under just the standard ring learning-with-errors (RingLWE) assumption. For a database of size N and any constant \u03b5>0, the preprocessing run-time and size is O(N1+\u03b5), while the run-time and communication-complexity of each PIR query is polylog(N). We also show how to update the preprocessed database in time O(N\u03b5). Our approach is to first construct a standard PIR where the server\u2019s computation consists of evaluating a multivariate polynomial; we then convert it to a DEPIR by preprocessing the polynomial to allow for fast evaluation, using the techniques of Kedlaya and Umans (STOC \u201908). Building on top of our DEPIR, we construct general fully homomorphic encryption for random-access machines (RAM-FHE), which allows a server to homomorphically evaluate an arbitrary RAM program P over a client\u2019s encrypted input x and the server\u2019s preprocessed plaintext input y to derive an encryption of the output P(x,y) in time that scales with the RAM run-time of the computation rather than its circuit size. Prior work only gave a heuristic candidate construction of a restricted notion of RAM-FHE. In this work, we construct RAM-FHE under the RingLWE assumption with circular security. For a RAM program P with worst-case run-time T, the homomorphic evaluation runs in time T1+\u03b5 \u00b7 (|x| + |y|).",
                "paper_link": "https://www.semanticscholar.org/paper/08fb24d7e57db7de0da428676b1a0a6903051442"
            },
            {
                "title": "Delegating RAM computations with adaptive soundness and privacy",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/549f76ad2b1ea4c2245afdbaad266d37580966c2"
            },
            {
                "title": "Cryptography for parallel RAM from indistinguishability obfuscation",
                "abstract": "Since many cryptographic schemes are about performing computation on data, it is important to consider a computation model which captures the prominent features of modern system architecture. Parallel random access machine (PRAM) is such an abstraction which not only models multiprocessor platforms, but also new frameworks supporting massive parallel computation such as MapReduce. In this work, we explore the feasibility of designing cryptographic solutions for the PRAM model of computation to achieve security while leveraging the power of parallelism and random data access. We demonstrate asymptotically optimal solutions for a wide-range of cryptographic tasks based on indistinguishability obfuscation. In particular, we construct the first publicly verifiable delegation scheme with privacy in the persistent database setting, which allows a client to privately delegate both computation and data to a server with optimal efficiency. Specifically, the server can perform PRAM computation on private data with parallel efficiency preserved (up to poly-logarithmic overhead). Our results also cover succinct randomized encoding, searchable encryption, functional encryption, secure multiparty computation, and indistinguishability obfuscation for PRAM. We obtain our results in a modular way through a notion of computational-trace indistinguishability obfuscation (CiO), which may be of independent interests.",
                "paper_link": "https://www.semanticscholar.org/paper/04f80a2d8811f2d8ce6ab293611b2f15025bacb0"
            },
            {
                "title": "Cache-oblivious and data-oblivious sorting and applications",
                "abstract": "Although external-memory sorting has been a classical algorithms abstraction and has been heavily studied in the literature, perhaps somewhat surprisingly, when data-obliviousness is a requirement, even very rudimentary questions remain open. Prior to our work, it is not even known how to construct a comparison-based, external-memory oblivious sorting algorithm that is optimal in IO-cost. We make a significant step forward in our understanding of external-memory, oblivious sorting algorithms. Not only do we construct a comparison-based, external-memory oblivious sorting algorithm that is optimal in IO-cost, our algorithm is also cache-agnostic in that the algorithm need not know the storage hierarchy's internal parameters such as the cache and cache-line sizes. Our result immediately implies a cache-agnostic ORAM construction whose asymptotic IO-cost matches the best known cache-aware scheme. Last but not the least, we propose and adopt a new and stronger security notion for external-memory, oblivious algorithms and argue that this new notion is desirable for resisting possible cache-timing attacks. Thus our work also lays a foundation for the study of oblivious algorithms in the cache-agnostic model.",
                "paper_link": "https://www.semanticscholar.org/paper/b1498fb2bee67a72167de03834da310e1d62a2c0"
            },
            {
                "title": "Can We Overcome thenlognBarrier for Oblivious Sorting?",
                "abstract": "It is well-known that non-comparison-based techniques can allow us to sort n elements in o(n log n) time on a Random-Access Machine (RAM). On the other hand, it is a long-standing open question whether (non-comparison-based) circuits can sort n elements from the domain [1..2] with o(kn log n) boolean gates. We consider weakened forms of this question: first, we consider a restricted class of sorting where the number of distinct keys is much smaller than the input length; and second, we explore Oblivious RAMs and probabilistic circuit families, i.e., computational models that are somewhat more powerful than circuits but much weaker than RAM. We show that Oblivious RAMs and probabilistic circuit families can sort o(log n)-bit keys in o(n log n) time or o(kn log n) circuit complexity. Our algorithms work in the indivisible model, i.e., not only can they sort an array of numerical keys \u2014 if each key additionally carries an opaque ball, our algorithms can also move the balls into the correct order. We further show that in such an indivisible model, it is impossible to sort \u03a9(log n)-bit keys in o(n log n) time, and thus the o(log n)-bit-key assumption is necessary for overcoming the n log n barrier. Finally, after optimizing the IO efficiency, we show that even the 1-bit special case can solve open questions: our oblivious algorithms solve tight compaction and selection with optimal IO efficiency for the first time. \u2217This work is supported in part by NSF grants CNS-1314857, CNS-1514261, CNS-1544613, CNS-1561209, CNS1601879, CNS-1617676, an Office of Naval Research Young Investigator Program Award, a Packard Fellowship, a DARPA Safeware grant (subcontractor under IBM), a Sloan Fellowship, Google Faculty Research Awards, a Baidu Research Award, and a VMWare Research Award. \u2020Cornell University. \u2021Cornell University. \u00a7Shanghai Jiao Tong University, work done while visiting Cornell.",
                "paper_link": "https://www.semanticscholar.org/paper/9701bcff071ffc335c8a1d4b7615b97c4ca4d9e0"
            },
            {
                "title": "Optorama: Optimal oblivious ram",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/b68433a3fb84695bb7881042ec86d98bc54d12cb"
            },
            {
                "title": "Optimal Oblivious Parallel RAM\u2217",
                "abstract": "A data-oblivious algorithm is an algorithm whose memory access pattern is independent of the input values. We initiate the study of parallel data oblivious algorithms on realistic multicores, best captured by the binary fork-join model of computation. We present a data-oblivious CREW binary fork-join sorting algorithm with optimal total work and optimal (cache-oblivious) cache complexity, and in O(\u0142og n \u0142og \u0142og n) span (i.e., parallel time); these bounds match the best-known bounds for binary fork-join cache-efficient insecure algorithms. Using our sorting algorithm as a core primitive, we show how to data-obliviously simulate general PRAM algorithms in the binary fork-join model with non-trivial efficiency, and we present data-oblivious algorithms for several applications including list ranking, Euler tour, tree contraction, connected components, and minimum spanning forest. All of our data oblivious algorithms have bounds that either match or improve over the best known bounds for insecure algorithms. Complementing these asymptotically efficient results, we present a practical variant of our sorting algorithm that is self-contained and potentially implementable. It has optimal caching cost, and it is only a \u0142og \u0142og n factor off from optimal work and about a \u0142og n factor off in terms of span. %moreover, it achieves small constant factors in its bounds. We also present an EREW variant with optimal work and caching cost, and with the same asymptotic span.",
                "paper_link": "https://www.semanticscholar.org/paper/9970ad4e3bf45eb5efbe0200d3c81e12a6addf0c"
            },
            {
                "title": "Optimal single-server private information retrieval",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/aa9b24d730357c6ea518d411a4f18cf813e5b4df"
            },
            {
                "title": "Oblivious RAM with worst-case logarithmic overhead",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/8d6b29b3edfd1dc5e3d3fcc92fdaebb24d5509c3"
            },
            {
                "title": "Computation-Trace Indistinguishability Obfuscation and its Applications.",
                "abstract": "We introduce a new, instance-based notion of indistinguishability obfuscation, called computation-trace indistinguishability obfuscation (CiO), for (parallel) RAM computation. CiO only obfuscates a fixed, single computation instance, as opposed to iO which obfuscates a function on all input instances. Specifically, for \u03a0 defined by (P, x) consisting of a (parallel) RAM program P and an input x, the obfuscations of two instances \u03a0 and \u03a0\u2032 are required to be indistinguishable only when the execution of \u03a0 and \u03a0\u2032 generate an identical computation trace; namely, identical sequences of CPU states and memory content. On the other hand, we require the obfuscation to be (i) fully succinct: the runtime of the obfuscator (and thus the size of the obfuscated instance) depends only on the description and input/output size of \u03a0, but is independent of the time and space complexities of \u03a0, and (ii) efficiency preserving: the obfuscated instance is a (parallel) RAM program that preserves parallel/total time and space complexities of \u03a0 up to polylogarithmic factors. As our main results, we construct CiO for parallel RAM (PRAM) computation based on iO for circuits and one-way functions, and demonstrate the power of CiO by the following applications. \u2022 With digital signatures, our CiO for PRAM immediately implies the first two-message (publiclyverifiable) delegation scheme for outsourcing PRAM computation, where the delegator\u2019s runtime depends only on the program description and input/output size, and the server\u2019s complexity matches the PRAM complexity of the computation up to polylogarithmic factors. \u2022 With public-key encryption, our CiO for PRAM, and a specific oblivious PRAM construction, we construct the first fully succinct randomized encoding (RE) for PRAM computation, where the encoder\u2019s runtime (and thus the encoding size) depends only on the program description and input/output size, and the decoding complexity matches PRAM complexity of the computation up to polylogarithmic factors. \u2022 By plugging our fully succinct RE for PRAM into existing transformations, we obtain the first constructions of several cryptographic primitives for PRAM, such as functional encryptions with succinct (PRAM) function keys, succinct reusable garbling schemes, and succinct indistinguishability obfuscations (the later requires sub-exponential security). Notably, this implies that, while CiO is weaker than iO, sub-exponentially secure CiO for PRAM implies sub-exponentially secure iO for PRAM. \u2217Academia Sinica, Taiwan, wycchen@iis.sinica.edu.tw \u2020Chinese University of Hong Kong, sherman@ie.cuhk.edu.hk \u2021Academia Sinica, Taiwan, kmchung@iis.sinica.edu.tw \u00a7Chinese University of Hong Kong, wflai@ie.cuhk.edu.hk \u00b6Academia Sinica, Taiwan, wklin@iis.sinica.edu.tw \u2016Virginia Commonwealth University, hszhou@vcu.edu",
                "paper_link": "https://www.semanticscholar.org/paper/29224780bce1722c75f3909334f115419dea3b96"
            },
            {
                "title": "A logarithmic lower bound for oblivious RAM (for all parameters)",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/9ec36ba3c03b01470bb4363fffdb8ed8dd1a0e11"
            },
            {
                "title": "Game theoretic notions of fairness in multi-party coin toss",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/b2c49e029070b3206691464585ed7f96b0290f70"
            },
            {
                "title": "Oblivious parallel tight compaction",
                "abstract": "In tight compaction one is given an array of balls some of which are marked 0 and the rest are marked 1. The output of the procedure is an array that contains all of the original balls except that now the 0-balls appear before the 1-balls. In other words, tight compaction is equivalent to sorting the array according to 1-bit keys (not necessarily maintaining order within same-key balls). Tight compaction is not only an important algorithmic task by itself, but its oblivious version has also played a key role in recent constructions of oblivious RAM compilers. \nWe present an oblivious deterministic algorithm for tight compaction such that for input arrays of n balls requires O(n) total work and O(log n) depth. Our algorithm is in the Exclusive-Read-Exclusive-Write Parallel-RAM model (i.e., EREW PRAM, the most restrictive PRAM model), and importantly we achieve asymptotical optimality in both total work and depth. To the best of our knowledge no earlier work, even when allowing randomization, can achieve optimality in both total work and depth.",
                "paper_link": "https://www.semanticscholar.org/paper/968b4aff0c5e0c07d1f65e3a76bc3d9c673ba39e"
            },
            {
                "title": "Sorting Short Keys in Circuits of Size",
                "abstract": "We consider the classical problem of sorting an input array containing $n$ elements, where each element is described with a $k$-bit comparison-key and a $w$-bit payload. A long-standing open problem is whether there exist $(k + w) \\cdot o(n \\log n)$-sized boolean circuits for sorting. We show that one can overcome the $n\\log n$ barrier when the keys to be sorted are short. Specifically, we prove that there is a circuit with $(k + w) \\cdot O(n k) \\cdot \\poly(\\log^*n - \\log^* (w + k))$ boolean gates capable of sorting any input array containing $n$ elements, each described with a $k$-bit key and a $w$-bit payload. Therefore, if the keys to be sorted are short, say, $k < o(\\log n)$, our result is asymptotically better than the classical AKS sorting network (ignoring $\\poly\\log^*$ terms); and we also overcome the $n \\log n$ barrier in such cases. Such a result might be surprising initially because it is long known that comparator-based techniques must incur $\\Omega(n \\log n)$ comparator gates even when the keys to be sorted are only $1$-bit long (e.g., see Knuth's \"Art of Programming\" textbook). To the best of our knowledge, we are the first to achieve non-trivial results for sorting circuits using non-comparison-based techniques. We also show that if the Li-Li network coding conjecture is true, our upper bound is optimal, barring $\\poly\\log^*$ terms, for every $k$ as long as $k = O(\\log n)$.",
                "paper_link": "https://www.semanticscholar.org/paper/0c8abbb467f29ec08bdeb8a6a56d72bf44a272d3"
            },
            {
                "title": "Perfectly oblivious (parallel) RAM revisited, and improved constructions",
                "abstract": "Oblivious RAM (ORAM) is a technique for compiling any RAM program to an oblivious counterpart, i.e., one whose access patterns do not leak information about the secret inputs. Similarly, Oblivious Parallel RAM (OPRAM) compiles a parallel RAM program to an oblivious counterpart. In this paper, we care about ORAM/OPRAM with perfect security , i.e., the access patterns must be identically distributed no matter what the program\u2019s memory request sequence is. In the past, two types of perfect ORAMs/OPRAMs have been considered: constructions whose performance bounds hold in expectation (but may occasionally run more slowly); and constructions whose performance bounds hold deterministically (even though the algorithms themselves are randomized). In this paper, we revisit the performance metrics for perfect ORAM/OPRAM, and show novel constructions that achieve asymptotical improvements for all performance metrics. Our \ufb01rst result is a new perfectly secure OPRAM scheme with O (log 3 N/ log log N ) expected overhead. In comparison, prior literature has been stuck at O (log 3 N ) for more than a decade. Next, we show how to construct a perfect ORAM with O (log 3 N/ log log N ) deterministic simulation overhead. We further show how to make the scheme parallel, resulting in an perfect OPRAM with O (log 4 N/ log log N ) deterministic simulation overhead. For perfect ORAMs/OPRAMs with deterministic performance bounds, our results achieve subexponential improvement over the state-of-the-art. Speci\ufb01cally, the best known prior scheme incurs more than \u221a N deterministic simulation overhead (Raskin and Simkin, Asiacrypt\u201919); moreover, their scheme works only for the sequential setting and is not amenable",
                "paper_link": "https://www.semanticscholar.org/paper/1a0af9163eb7b6e054134bd1cd80982024dc89c8"
            },
            {
                "title": "MPC for MPC: secure computation on a massively parallel computing architecture",
                "abstract": "Massively Parallel Computation (MPC) is a model of computation widely believed to best capture realistic parallel computing architectures such as large-scale MapReduce and Hadoop clusters. Motivated by the fact that many data analytics tasks performed on these platforms involve sensitive user data, we initiate the theoretical exploration of how to leverage MPC architectures to enable efficient, privacy-preserving computation over massive data. Clearly if a computation task does not lend itself to an efficient implementation on MPC even without security, then we cannot hope to compute it efficiently on MPC with security. We show, on the other hand, that any task that can be efficiently computed on MPC can also be securely computed with comparable efficiency. Specifically, we show the following results: \n- any MPC algorithm can be compiled to a communication-oblivious counterpart while asymptotically preserving its round and space complexity, where communication-obliviousness ensures that any network intermediary observing the communication patterns learn no information about the secret inputs; \n- assuming the existence of Fully Homomorphic Encryption with a suitable notion of compactness and other standard cryptographic assumptions, any MPC algorithm can be compiled to a secure counterpart that defends against an adversary who controls not only intermediate network routers but additionally up to 1/3 - \u03b7 fraction of machines (for an arbitrarily small constant \u03b7) - moreover, this compilation preserves the round complexity tightly, and preserves the space complexity upto a multiplicative security parameter related blowup. \nAs an initial exploration of this important direction, our work suggests new definitions and proposes novel protocols that blend algorithmic and cryptographic techniques.",
                "paper_link": "https://www.semanticscholar.org/paper/a1a7e29de32bf82346fc9b9fa06df7ad56e19130"
            },
            {
                "title": "NanoGRAM: Garbled RAM withOverhead",
                "abstract": "We propose a new garbled RAM construction called NanoGRAM, which achieves an amortized cost of \u00d5(\u03bb\u00b7(W logN+logN)) bits per memory access, where \u03bb is the security parameter, W is the block size, and N is the total number of blocks, and \u00d5(\u00b7) hides poly log log factors. For sufficiently large blocks where W = \u03a9(logN), our scheme achieves \u00d5(\u03bb \u00b7W logN) cost per memory access, where the dependence on N is optimal (barring poly log log factors), in terms of the evaluator\u2019s runtime. Our asymptotical performance matches even the interactive stateof-the-art (modulo poly log log factors), that is, running Circuit ORAM atop garbled circuit, and yet we remove the logarithmic number of interactions necessary in this baseline. Furthermore, we achieve asymptotical improvement over the recent work of Heath et al. Our scheme adopts the same assumptions as the mainstream literature on practical garbled circuits, i.e., circular correlation-robust hashes or a random oracle. We evaluate the concrete performance of NanoGRAM and compare it with a couple baselines that are asymptotically less efficient. We show that NanoGRAM starts to outperform the n\u00e4\u0131ve linear-scan garbled RAM at a memory size of N = 2 and starts to outperform the recent construction of Heath et al. at N = 2. Finally, as a by product, we also show the existence of a garbled RAM scheme assuming only one-way functions, with an amortized cost of \u00d5(\u03bb \u00b7 (W logN + logN)) per memory access. Again, the dependence on N is nearly optimal for blocks of size W = \u03a9(logN) bits.",
                "paper_link": "https://www.semanticscholar.org/paper/721df17a62a96e2418004b9d8ba5840f03f771b0"
            },
            {
                "title": "Optimal Sorting Circuits for Short Keys\u2217",
                "abstract": "A long-standing open question in the algorithms and complexity literature is whether there exist sorting circuits of size o(n log n). A recent work by Asharov, Lin, and Shi (SODA\u201921) showed that if the elements to be sorted have short keys whose length k = o(log n), then one can indeed overcome the n log n barrier for sorting circuits, by leveraging non-comparison-based techniques. More specifically, Asharov et al. showed that there exist O(n) \u00b7 min(k, log n)-sized sorting circuits for k-bit keys, ignoring poly log\u2217 factors. Interestingly, the recent works by Farhadi et al. (STOC\u201919) and Asharov et al. (SODA\u201921) also showed that the above result is essentially optimal for every key length k, assuming that the famous Li-Li network coding conjecture holds. Note also that proving any unconditional super-linear circuit lower bound for a wide class of problems is beyond the reach of current techniques. Unfortunately, the approach taken by Asharov et al. to achieve optimality in size somewhat crucially relies on sacrificing the depth: specifically, their circuit is super-polylogarithmic in depth even for 1-bit keys. Asharov et al. phrase it as an open question how to achieve optimality both in size and depth. In this paper, we close this important gap in our understanding. We construct a sorting circuit of size O(n)\u00b7min(k, log n) (ignoring poly log\u2217 terms) and depthO(log n). To achieve this, our approach departs significantly from the prior works. Our result can be viewed as a generalization of the landmark result by Ajtai, Koml\u00f3s, and Szemer\u00e9di (STOC\u201983), simultaneously in terms of size and depth. Specifically, for k = o(log n), we achieve asymptotical improvements in size over the AKS sorting circuit, while preserving optimality in depth. *Author ordering is randomly generated. ar X iv :2 10 2. 11 48 9v 1 [ cs .D S] 2 3 Fe b 20 21",
                "paper_link": "https://www.semanticscholar.org/paper/b4e9e75e0c0839733076c1ba90f018cd950e6a28"
            }
        ]
    },
    {
        "Professor": "Madhav Marathe",
        "Papers": [
            {
                "title": "Modelling disease outbreaks in realistic urban social networks",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/1413dbfbbae1b59d52656db3dc48a4ee278e082f"
            },
            {
                "title": "Mobile data offloading through opportunistic communications and social participation",
                "abstract": "3G networks are currently overloaded, due to the increasing popularity of various applications for smartphones. Offloading mobile data traffic through opportunistic communications is a promising solution to partially solve this problem, because there is almost no monetary cost for it. We propose to exploit opportunistic communications to facilitate information dissemination in the emerging Mobile Social Networks (MoSoNets) and thus reduce the amount of mobile data traffic. As a case study, we investigate the target-set selection problem for information delivery. In particular, we study how to select the target set with only k users, such that we can minimize the mobile data traffic over cellular networks. We propose three algorithms, called Greedy, Heuristic, and Random, for this problem and evaluate their performance through an extensive trace-driven simulation study. Our simulation results verify the efficiency of these algorithms for both synthetic and real-world mobility traces. For example, the Heuristic algorithm can offload mobile data traffic by up to 73.66 percent for a real-world mobility trace. Moreover, to investigate the feasibility of opportunistic communications for mobile phones, we implement a proof-of-concept prototype, called Opp-off, on Nokia N900 smartphones, which utilizes their Bluetooth interface for device/service discovery and content transfer.",
                "paper_link": "https://www.semanticscholar.org/paper/2d0eec270872866bb9dcc5b6ae18e85477bdac96"
            },
            {
                "title": "Simple heuristics for unit disk graphs",
                "abstract": "Unit disk graphs are intersection graphs of circles of unit radius in the plane. We present simple and provably good heuristics for a number of classical NP-hard optimization problems on unit disk graphs. The problems considered include maximum independent set, minimum vertex cover, minimum coloring and minimum dominating set. We also present an on-line coloring heuristic which achieves a competitive ratio of 6 for unit disk graphs. Our heuristics do not need a geometric representation of unit disk graphs. Geometric representations are used only in establishing the performance guarantees of the heuristics. Several of our approximation algorithms can be extended to intersection graphs of circles of arbitrary radii in the plane, intersection graphs of regular polygons, and to intersection graphs of higher dimensional regular objects.",
                "paper_link": "https://www.semanticscholar.org/paper/1de1f3f771efe116785f06343c1cffbc4a9de493"
            },
            {
                "title": "Algorithmic aspects of topology control problems for ad hoc networks",
                "abstract": "Topology control problems are concerned with the assignment of power values to the nodes of an ad hoc network so that the power assignment leads to a graph topology satisfying some specified properties. This paper considers such problems under several optimization objectives, including minimizing the maximum power and minimizing the total power. A general approach leading to a polynomial algorithm is presented for minimizing maximum power for a class of graph properties called monotone properties. The difficulty of generalizing the approach to properties that are not monotone is discussed. Problems involving the minimization of total power are known to be NP-complete even for simple graph properties. A general approach that leads to an approximation algorithm for minimizing the total power for some monotone properties is presented. Using this approach, a new approximation algorithm for the problem of minimizing the total power for obtaining a 2-node-connected graph is developed. It is shown that this algorithm provides a constant performance guarantee. Experimental results from an implementation of the approximation algorithm are also presented.",
                "paper_link": "https://www.semanticscholar.org/paper/347743a52ba55feab65f761799975f93a480c8b6"
            },
            {
                "title": "NC-approximation schemes for NP-and PSPACE-hard problems for geometric graphs",
                "abstract": "We present NC-approximation schemes for a number of graph problems when restricted to geometric graphs including unit disk graphs and graphs drawn in a civilized manner. Our approximation schemes exhibit the same time versus performance trade-off as the best known approximation schemes for planar graphs. We also define the concept of ?-precision unit disk graphs and show that for such graphs the approximation schemes have a better time versus performance trade-off than the approximation schemes for arbitrary unit disk graphs. Moreover, compared to unit disk graphs, we show that for ?-precision unit disk graphs many more graph problems have efficient approximation schemes.Our NC-approximation schemes can also be extended to obtain efficient NC-approximation schemes for several PSPACE-hard problems on unit disk graphs specified using a restricted version of the hierarchical specification language of Bentley, Ottmann, and Widmayer. The approximation schemes for hierarchically specified unit disk graphs presented in this paper are among the first approximation schemes in the literature for natural PSPACE-hard optimization problems.",
                "paper_link": "https://www.semanticscholar.org/paper/3be90f190c0fcd38a58002bb3af481016d15064f"
            },
            {
                "title": "Episimdemics: an efficient algorithm for simulating the spread of infectious disease over large realistic social networks",
                "abstract": "Preventing and controlling outbreaks of infectious diseases such as pandemic influenza is a top public health priority. We describe EpiSimdemics - a scalable parallel algorithm to simulate the spread of contagion in large, realistic social contact networks using individual-based models. EpiSimdemics is an interaction-based simulation of a certain class of stochastic reaction-diffusion processes. Straightforward simulations of such process do not scale well, limiting the use of individual-based models to very small populations. EpiSimdemics is specifically designed to scale to social networks with 100 million individuals. The scaling is obtained by exploiting the semantics of disease evolution and disease propagation in large networks. We evaluate an MPI-based parallel implementation of EpiSimdemics on a mid-sized HPC system, demonstrating that EpiSimdemics scales well. EpiSimdemics has been used in numerous sponsor defined case studies targeted at policy planning and course of action analysis, demonstrating the usefulness of EpiSimdemics in practical situations.",
                "paper_link": "https://www.semanticscholar.org/paper/6c73b2f8e15c69822f37fe20cc6413db2234b203"
            },
            {
                "title": "What factors might have led to the emergence of Ebola in West Africa?",
                "abstract": "An Ebola outbreak of unprecedented scope emerged in West Africa in December 2013 and presently continues unabated in the countries of Guinea, Sierra Leone, and Liberia. Ebola is not new to Africa, and outbreaks have been confirmed as far back as 1976. The current West African Ebola outbreak is the largest ever recorded and differs dramatically from prior outbreaks in its duration, number of people affected, and geographic extent. The emergence of this deadly disease in West Africa invites many questions, foremost among these: why now, and why in West Africa? Here, we review the sociological, ecological, and environmental drivers that might have influenced the emergence of Ebola in this region of Africa and its spread throughout the region. Containment of the West African Ebola outbreak is the most pressing, immediate need. A comprehensive assessment of the drivers of Ebola emergence and sustained human-to-human transmission is also needed in order to prepare other countries for importation or emergence of this disease. Such assessment includes identification of country-level protocols and interagency policies for outbreak detection and rapid response, increased understanding of cultural and traditional risk factors within and between nations, delivery of culturally embedded public health education, and regional coordination and collaboration, particularly with governments and health ministries throughout Africa. Public health education is also urgently needed in countries outside of Africa in order to ensure that risk is properly understood and public concerns do not escalate unnecessarily. To prevent future outbreaks, coordinated, multiscale, early warning systems should be developed that make full use of these integrated assessments, partner with local communities in high-risk areas, and provide clearly defined response recommendations specific to the needs of each community.",
                "paper_link": "https://www.semanticscholar.org/paper/626c7dc83ba68ad679a482e84113744b5c988bf7"
            },
            {
                "title": "Cellular traffic offloading through opportunistic communications: a case study",
                "abstract": "Due to the increasing popularity of various applications for smartphones, 3G networks are currently overloaded by mobile data traffic. Offloading cellular traffic through opportunistic communications is a promising solution to partially solve this problem, because there is no monetary cost for it. As a case study, we investigate the target-set selection problem for information delivery in the emerging Mobile Social Networks (MoSoNets). We propose to exploit opportunistic communications to facilitate the information dissemination and thus reduce the amount of cellular traffic. In particular, we study how to select the target set with only k users, such that we can minimize the cellular data traffic.\n In this scenario, initially the content service providers deliver information over cellular networks to only users in the target set. Then through opportunistic communications, target-users will further propagate the information among all the subscribed users. Finally, service providers will send the information to users who fail to receive it before the delivery deadline (i.e., delay-tolerance threshold). We propose three algorithms, called Greedy, Heuristic, and Random, for this problem and evaluate their performance through an extensive trace-driven simulation study. The simulation results verify the efficiency of these algorithms for both synthetic and real-world mobility traces. For example, the Heuristic algorithm can offload cellular traffic by up to 73.66% for a real-world mobility trace.",
                "paper_link": "https://www.semanticscholar.org/paper/af6d4fd231cd5b903ec02197e643272c854daa15"
            },
            {
                "title": "Modeling the impact of interventions on an epidemic of Ebola in Sierra Leone and Liberia",
                "abstract": "Background: An Ebola outbreak of unparalleled size is currently affecting several countries in West Africa, and international efforts to control the outbreak are underway. However, the efficacy of these interventions, and their likely impact on an Ebola epidemic of this size, is unknown. Forecasting and simulation of these interventions may inform public health efforts. Methods: We use existing data from Liberia and Sierra Leone to parameterize a mathematical model of Ebola and use this model to forecast the progression of the epidemic, as well as the efficacy of several interventions, including increased contact tracing, improved infection control practices, the use of a hypothetical pharmaceutical intervention to improve survival in hospitalized patients. Findings: Model forecasts until Dec. 31, 2014 show an increasingly severe epidemic with no sign of having reached a peak. Modeling results suggest that increased contact tracing, improved infection control, or a combination of the two can have a substantial impact on the number of Ebola cases, but these interventions are not sufficient to halt the progress of the epidemic. The hypothetical pharmaceutical intervention, while impacting mortality, had a smaller effect on the forecasted trajectory of the epidemic. Interpretation: Near-term, practical interventions to address the ongoing Ebola epidemic may have a beneficial impact on public health, but they will not result in the immediate halting, or even obvious slowing of the epidemic. A long-term commitment of resources and support will be necessary to address the outbreak.",
                "paper_link": "https://www.semanticscholar.org/paper/16663deb513423d8e9bafe1ba5bbb320d8acfa5a"
            },
            {
                "title": "Bicriteria network design problems",
                "abstract": "We study several bicriteria network design problems phrased as follows: given an undirected graph and two minimization objectives with a budget specified on one objective, find a subgraph satisfying certain connectivity requirements that minimizes the second objective subject to the budget on the first. First, we develop a formalism for bicriteria problems and their approximations. Secondly, we use a simple parametric search technique to provide bicriteria approximation algorithms for problems with two similar criteria, where both criteria are the same measure (such as the diameter or the total cost of a tree) but differ only in the cost function under which the measure is computed. Thirdly, we present an (O(log n), O(log n))-approximation algorithm for finding a diameter-constrained minimum cost spanning tree of an undirected graph on n nodes. Finally, for the class of treewidth-bounded graphs, we provide pseudopolynomial-time algorithms for a number of bicriteria problems using dynamic programming. These pseudopolynomial-time algorithms can be converted to fully polynomialtime approximation schemes using a scaling technique.",
                "paper_link": "https://www.semanticscholar.org/paper/42814955580b876b88f934c2691e1b62f490cae1"
            },
            {
                "title": "Algorithmic aspects of capacity in wireless networks",
                "abstract": "This paper considers two inter-related questions: (i) Given a wireless ad-hoc network and a collection of source-destination pairs {(si,ti)}, what is the maximum throughput capacity of the network, i.e. the rate at which data from the sources to their corresponding destinations can be transferred in the network? (ii) Can network protocols be designed that jointly route the packets and schedule transmissions at rates close to the maximum throughput capacity? Much of the earlier work focused on random instances and proved analytical lower and upper bounds on the maximum throughput capacity. Here, in contrast, we consider arbitrary wireless networks. Further, we study the algorithmic aspects of the above questions: the goal is to design provably good algorithms for arbitrary instances. We develop analytical performance evaluation models and distributed algorithms for routing and scheduling which incorporate fairness, energy and dilation (path-length) requirements and provide a unified framework for utilizing the network close to its maximum throughput capacity.Motivated by certain popular wireless protocols used in practice, we also explore \"shortest-path like\" path selection strategies which maximize the network throughput. The theoretical results naturally suggest an interesting class of congestion aware link metrics which can be directly plugged into several existing routing protocols such as AODV, DSR, etc. We complement the theoretical analysis with extensive simulations. The results indicate that routes obtained using our congestion aware link metrics consistently yield higher throughput than hop-count based shortest path metrics.",
                "paper_link": "https://www.semanticscholar.org/paper/480c866452a0bcc113f6b921baa9bfbf6df6aaec"
            },
            {
                "title": "Spanning trees\u2014short or small",
                "abstract": "We study the problem of finding small trees. Classical network design problems are considered with the additional constraint that only a specified number k of nodes are required to be connected in the solution. A prototypical example is the kMST problem in which we require a tree of minimum weight spanning at least k nodes in an edge-weighted graph. We show that the kMST problem is NP-hard even for points in the Euclidean plane. We provide approximation algorithms with performance ratio 2v/ for the general edge-weighted case and O(k1/4) for the case of points in the plane. Polynomial-time exact solutions are also presented for the class of treewidth-bounded graphs, which includes trees, series-parallel graphs, and bounded bandwidth graphs, and for points on the boundary of a convex region in the Euclidean plane. We also investigate the problem of finding short trees and, more generally, that of finding networks with minimum diameter. A simple technique is used to provide a polynomiM-time solution for finding k-trees of minimum diameter. We identify easy and hard problems arising in finding short networks using a framework due to T. C. Hu.",
                "paper_link": "https://www.semanticscholar.org/paper/dc6cafbc13f9277f6cfdd42a391ede049406ea00"
            },
            {
                "title": "EpiFast: a fast algorithm for large scale realistic epidemic simulations on distributed memory systems",
                "abstract": "Large scale realistic epidemic simulations have recently become an increasingly important application of high-performance computing. We propose a parallel algorithm, EpiFast, based on a novel interpretation of the stochastic disease propagation in a contact network. We implement it using a master-slave computation model which allows scalability on distributed memory systems. EpiFast runs extremely fast for realistic simulations that involve: (i) large populations consisting of millions of individuals and their heterogeneous details, (ii) dynamic interactions between the disease propagation, the individual behaviors, and the exogenous interventions, as well as (iii) large number of replicated runs necessary for statistically sound estimates about the stochastic epidemic evolution. We find that EpiFast runs several magnitude faster than another comparable simulation tool while delivering similar results. EpiFast has been tested on commodity clusters as well as SGI shared memory machines. For a fixed experiment, if given more computing resources, it scales automatically and runs faster. Finally, EpiFast has been used as the major simulation engine in real studies with rather sophisticated settings to evaluate various dynamic interventions and to provide decision support for public health policy makers.",
                "paper_link": "https://www.semanticscholar.org/paper/30365a90b6925ea39356c0e8375be1447ced21e3"
            },
            {
                "title": "Formal-language-constrained path problems",
                "abstract": "Given an alphabet $\\Sigma$, a (directed) graph G whose edges are weighted and $\\Sigma$-labeled, and a formal language $L\\subseteq\\Sigma^*$, the formal-language-constrained shortest/simple path problem consists of finding a shortest (simple) path p in G complying with the additional constraint that l(p) \\in L$. Here l(p) denotes the unique word obtained by concatenating the $\\Sigma$-labels of the edges along the path p. The main contributions of this paper include the following: \nWe show that the formal-language-constrained shortest path problem is solvable efficiently in polynomial time when L is restricted to be a context-free language (CFL). When L is specified as a regular language we provide algorithms with improved space and time bounds. In contrast, we show that the problem of finding a simple path between a source and a given destination is NP-hard, even when L is restricted to fixed simple regular languages and to very simple classes of graphs (e.g., complete grids). For the class of treewidth-bounded graphs, we show that (i) the problem of finding a regular-language-constrained simple path between source and destination is solvable in polynomial time and (ii) the extension to finding CFL-constrained simple paths is NP-complete. \nOur results extend the previous results in [SIAM J. Comput., 24 (1995), pp. 1235--1258; Proceedings of the 76th Annual Meeting of the Transportation Research Board, 1997; and Proceedings of the 9th ACM SIGACT-SIGMOD-SIGART Symposium on Database Systems, 1990, pp. 230--242]. Several additional extensions and applications of our results in the context of transportation problems are presented. For instance, as a corollary of our results, we obtain a polynomial-time algorithm for the best k-similar path problem studied in Proceedings of the 76th Annual Meeting of the Transportation Reasearch Board, 1997]. The previous best algorithm was given by [ Proceedings of the 76th Annual Meeting of the Transportation Research Board, 1997] and takes exponential time in the worst case.",
                "paper_link": "https://www.semanticscholar.org/paper/174abff00624ac9c34ea5559e5cdbc7faf3b570c"
            },
            {
                "title": "A systematic review of studies on forecasting the dynamics of influenza outbreaks",
                "abstract": "Forecasting the dynamics of influenza outbreaks could be useful for decision\u2010making regarding the allocation of public health resources. Reliable forecasts could also aid in the selection and implementation of interventions to reduce morbidity and mortality due to influenza illness. This paper reviews methods for influenza forecasting proposed during previous influenza outbreaks and those evaluated in hindsight. We discuss the various approaches, in addition to the variability in measures of accuracy and precision of predicted measures. PubMed and Google Scholar searches for articles on influenza forecasting retrieved sixteen studies that matched the study criteria. We focused on studies that aimed at forecasting influenza outbreaks at the local, regional, national, or global level. The selected studies spanned a wide range of regions including USA, Sweden, Hong Kong, Japan, Singapore, United Kingdom, Canada, France, and Cuba. The methods were also applied to forecast a single measure or multiple measures. Typical measures predicted included peak timing, peak height, daily/weekly case counts, and outbreak magnitude. Due to differences in measures used to assess accuracy, a single estimate of predictive error for each of the measures was difficult to obtain. However, collectively, the results suggest that these diverse approaches to influenza forecasting are capable of capturing specific outbreak measures with some degree of accuracy given reliable data and correct disease assumptions. Nonetheless, several of these approaches need to be evaluated and their performance quantified in real\u2010time predictions.",
                "paper_link": "https://www.semanticscholar.org/paper/54c0fe9a4dfdfe1011d702f3049f4113f9be2b76"
            },
            {
                "title": "Generation and analysis of large synthetic social contact networks",
                "abstract": "We describe \u201cfirst principles\u201d based methods for developing synthetic urban and national scale social contact networks. Unlike simple random graph techniques, these methods use real world data sources and combine them with behavioral and social theories to synthesize networks. We develop a synthetic population for the United States modeling every individual in the population including household structure, demographics and a 24-hour activity sequence. The process involves collecting and manipulating public and proprietary data sets integrated into a common architecture for data exchange and then using these data sets to generate new relations. A social contact network is derived from the synthetic population based on physical co-location of interacting persons. We use graph measures to compare and contrast the structural characteristics of the social networks that span different urban regions. We then simulate diffusion processes on these networks and analyze similarities and differences in the structure of the networks.",
                "paper_link": "https://www.semanticscholar.org/paper/06ed109b9c92b9a87e81ec98fc75fbe0ef7ca86f"
            },
            {
                "title": "Mathematical models for covid-19 pandemic: a comparative analysis",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/dd74a3a343529174fe7c6485723cf2d5911c18ed"
            },
            {
                "title": "Commentary on Ferguson, et al.,\u201cImpact of non-pharmaceutical interventions (NPIs) to reduce COVID-19 mortality and healthcare demand\u201d",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/b1ce0a1cd9cb416b95a2ca2bd955b6a3be8592b0"
            },
            {
                "title": "The distance-2 matching problem and its relationship to the MAC-layer capacity of ad hoc wireless networks",
                "abstract": "We consider the problem of determining the maximum capacity of the media access (MAC) layer in wireless ad hoc networks. Due to spatial contention for the shared wireless medium, not all nodes can concurrently transmit packets to each other in these networks. The maximum number of possible concurrent transmissions is, therefore, an estimate of the maximum network capacity, and depends on the MAC protocol being used. We show that for a large class of MAC protocols based on virtual carrier sensing using RTS/CTS messages, which includes the popular IEEE 802.11 standard, this problem may be modeled as a maximum Distance-2 matching ( D2EMIS) in the underlying wireless network: Given a graph G(V,E), find a set of edges E'/spl sube/E such that no two edges in E' are connected by another edge in E. D2EMIS is NP-complete. Our primary goal is to show that it can be approximated efficiently in networks that arise in practice. We do this by focusing on an admittedly simplistic, yet natural, graph-theoretic model for ad hoc wireless networks based on disk graphs, where a node can reach all other nodes within some distance (nodes may have unequal reach distances). We show that our approximation yields good capacity bounds. Our work is the first attempt at characterizing an important \"maximum\" measure of wireless network capacity, and can be used to shed light on previous topology formation protocols like Span and GAF that attempt to produce \"good\" or \"capacity-preserving\" topologies, while allowing nodes to alternate between sleep and awake states. Our work shows an efficient way to compute an upper bound on maximum wireless network capacity, thereby allowing topology formation algorithms to determine how close they are to optimal. We also outline a distributed algorithm for the problem for unit disk graphs, and briefly discuss extensions of our results to: 1) different node interference models; 2) directional antennas; and 3) other transceiver connectivity structures besides disk graphs.",
                "paper_link": "https://www.semanticscholar.org/paper/f392d0c33b12f655dd109774ea1e054fecea6235"
            },
            {
                "title": "Many birds with one stone: Multi-objective approximation algorithms",
                "abstract": "We study network-design problems with multiple design objectives. In particular, we look at two cost measures to be minimized simultaneously: the total cost of the network and the maximum degree of any node in the network. Our main result can be roughly stated as follows: given an integer $b$, we present approximation algorithms for a variety of network-design problems on an $n$-node graph in which the degree of the output network is $O(b \\log (\\frac{n}{b}))$ and the cost of this network is $O(\\log n)$ times that of the minimum-cost degree-$b$-bounded network. These algorithms can handle costs on nodes as well as edges. Moreover, we can construct such networks so as to satisfy a variety of connectivity specifications including spanning trees, Steiner trees and generalized Steiner forests. The performance guarantee on the cost of the output network is nearly best-possible unless $NP = \\tilde{P}$. We also address the special case in which the costs obey the triangle inequality. In this case, we obtain a polynomial-time approximation algorithm with a stronger performance guarantee. Given a bound $b$ on the degree, the algorithm finds a degree-$b$-bounded network of cost at most a constant time optimum. There is no algorithm that does as well in the absence of triangle inequality unless $P = NP$. We also show that in the case of spanning networks, we can simultaneously approximate within a constant factor yet another objective: the maximum cost of any edge in the network, also called the bottleneck cost of the network. We extend our algorithms to find TSP tours and $k$-connected spanning networks for any fixed $k$ that simultaneously approximate all these three cost measures.",
                "paper_link": "https://www.semanticscholar.org/paper/f0f1222c174e377a4f9f44c385fe8ddd92a22f21"
            }
        ]
    },
    {
        "Professor": "Paul \"Will\" McBurney",
        "Papers": [
            {
                "title": "Automatic documentation generation via source code summarization of method context",
                "abstract": "A documentation generator is a programming tool that creates documentation for software by analyzing the statements and comments in the software's source code. While many of these tools are manual, in that they require specially-formatted metadata written by programmers, new research has made inroads towards automatic generation of documentation. These approaches work by stitching together keywords from the source code into readable natural language sentences. These approaches have been shown to be effective, but carry a key limitation: the generated documents do not explain the source code's context. They can describe the behavior of a Java method, but not why the method exists or what role it plays in the software. In this paper, we propose a technique that includes this context by analyzing how the Java methods are invoked. In a user study, we found that programmers benefit from our generated documentation because it includes context information.",
                "paper_link": "https://www.semanticscholar.org/paper/6081ceb60d07fa0a2f0037ece6e540228e4edf73"
            },
            {
                "title": "Improving automated source code summarization via an eye-tracking study of programmers",
                "abstract": "Source Code Summarization is an emerging technology for automatically generating brief descriptions of code. Current summarization techniques work by selecting a subset of the statements and keywords from the code, and then including information from those statements and keywords in the summary. The quality of the summary depends heavily on the process of selecting the subset: a high-quality selection would contain the same statements and keywords that a programmer would choose. Unfortunately, little evidence exists about the statements and keywords that programmers view as important when they summarize source code. In this paper, we present an eye-tracking study of 10 professional Java programmers in which the programmers read Java methods and wrote English summaries of those methods. We apply the findings to build a novel summarization tool. Then, we evaluate this tool and provide evidence to support the development of source code summarization systems.",
                "paper_link": "https://www.semanticscholar.org/paper/67649d6e32fcf7573da91e685cfda173775708d5"
            },
            {
                "title": "Automatic source code summarization of context for java methods",
                "abstract": "Source code summarization is the task of creating readable summaries that describe the functionality of software. Source code summarization is a critical component of documentation generation, for example as Javadocs formed from short paragraphs attached to each method in a Java program. At present, a majority of source code summarization is manual, in that the paragraphs are written by human experts. However, new automated technologies are becoming feasible. These automated techniques have been shown to be effective in select situations, though a key weakness is that they do not explain the source code's context. That is, they can describe the behavior of a Java method, but not why the method exists or what role it plays in the software. In this paper, we propose a source code summarization technique that writes English descriptions of Java methods by analyzing how those methods are invoked. We then performed two user studies to evaluate our approach. First, we compared our generated summaries to summaries written manually by experts. Then, we compared our summaries to summaries written by a state-of-the-art automatic summarization tool. We found that while our approach does not reach the quality of human-written summaries, we do improve over the state-of-the-art summarization tool in several dimensions by a statistically-significant margin.",
                "paper_link": "https://www.semanticscholar.org/paper/b2a1a100367a802fba0793196516a5047871f622"
            },
            {
                "title": "An eye-tracking study of java programmers and application to source code summarization",
                "abstract": "Source Code Summarization is an emerging technology for automatically generating brief descriptions of code. Current summarization techniques work by selecting a subset of the statements and keywords from the code, and then including information from those statements and keywords in the summary. The quality of the summary depends heavily on the process of selecting the subset: a high-quality selection would contain the same statements and keywords that a programmer would choose. Unfortunately, little evidence exists about the statements and keywords that programmers view as important when they summarize source code. In this paper, we present an eye-tracking study of 10 professional Java programmers in which the programmers read Java methods and wrote English summaries of those methods. We apply the findings to build a novel summarization tool. Then, we evaluate this tool. Finally, we further analyze the programmers' method summaries to explore specific keyword usage and provide evidence to support the development of source code summarization systems.",
                "paper_link": "https://www.semanticscholar.org/paper/6053bc195e3d2778239f9020e9226c30bddc89f4"
            },
            {
                "title": "Improving topic model source code summarization",
                "abstract": "In this paper, we present an emerging source code summarization technique that uses topic modeling to select keywords and topics as summaries for source code. Our approach organizes the topics in source code into a hierarchy, with more general topics near the top of the hierarchy. In this way, we present the software's highest-level functionality first, before lower-level details. This is an advantage over previous approaches based on topic models, that only present groups of related keywords without a hierarchy. We conducted a preliminary user study that found our approach selects keywords and topics that the participants found to be accurate in a majority of cases.",
                "paper_link": "https://www.semanticscholar.org/paper/f68de166cc0b175a4874d0f83062a71f9ed2cf44"
            },
            {
                "title": "An empirical study of the textual similarity between source code and source code summaries",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/ac1f7334b59c6200e776c2b225790e3fb24554d8"
            },
            {
                "title": "Towards prioritizing documentation effort",
                "abstract": "Programmers need documentation to comprehend software, but they often lack the time to write it. Thus, programmers must prioritize their documentation effort to ensure that sections of code important to program comprehension are thoroughly explained. In this paper, we explore the possibility of automatically prioritizing documentation effort. We performed two user studies to evaluate the effectiveness of static source code attributes and textual analysis of source code towards prioritizing documentation effort. The first study used open-source API Libraries while the second study was conducted using closed-source industrial software from ABB. Our findings suggest that static source code attributes are poor predictors of documentation effort priority, whereas textual analysis of source code consistently performed well as a predictor of documentation effort priority.",
                "paper_link": "https://www.semanticscholar.org/paper/c6460aafccd0c7cee3f271e998ecda4724eab9cf"
            },
            {
                "title": "Automatic documentation generation via source code summarization",
                "abstract": "Programmers need software documentation. However, documentation is expensive to produce and maintain, and often becomes outdated over time. Programmers often lack the time and resources to write documentation. Therefore, automated solutions are desirable. Designers of automatic documentation tools are limited because there is not yet a clear understanding of what characteristics are important to generating high quality summaries. I propose three specific research objectives to improving automatic documentation generation. I propose to study the similarity between source code and summary. Second, I propose studying whether or not including contextual information about source code improves summary quality. Finally, I propose to study the problem of similarity in source code structure and source code documentation. This paper discusses my work on these three objectives towards my Ph.D. dissertation, including my preliminary and proposed work.",
                "paper_link": "https://www.semanticscholar.org/paper/330b8c76af03296bec5746cd4e16143f35a94e73"
            },
            {
                "title": "Automated feature discovery via sentence selection and source code summarization",
                "abstract": "Programs are, in essence, a collection of implemented features. Feature discovery in software engineering is the task of identifying key functionalities that a program implements. Manual feature discovery can be time consuming and expensive, leading to automatic feature discovery tools being developed. However, these approaches typically only describe features using lists of keywords, which can be difficult for readers who are not already familiar with the source code. An alternative to keyword lists is sentence selection, in which one sentence is chosen from among the sentences in a text document to describe that document. Sentence selection has been widely studied in the context of natural language summarization but is only beginning to be explored as a solution to feature discovery. In this paper, we compare four sentence selection strategies for the purpose of feature discovery. Two are off\u2010the\u2010shelf approaches, while two are adaptations we propose. We present our findings as guidelines and recommendations to designers of feature discovery tools. Copyright \u00a9 2016 John Wiley & Sons, Ltd.",
                "paper_link": "https://www.semanticscholar.org/paper/7f1983421a84c39b853b827d145ebe80c8daeb7d"
            },
            {
                "title": "Improving program comprehension via automatic documentation generation",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/af50cb600bc410f0ffb23a3602d3818616f2219e"
            },
            {
                "title": "Tracelab components for reproducing source code summarization experiments",
                "abstract": "This artifact is a reproducibility package for experiments in source code summarization. The artifact is implemented as a set of components for the TraceLab research infrastructure. We have converted two implementations of state-of-the-art source code summarization into prepackaged and easily-reusable TraceLab components. Prior to this conversion, the implementations were accessible but difficult to use, being scattered across numerous scripts in various languages with many dependencies. We provide the components, detailed tutorials, and two example virtual machine images via our online appendix.",
                "paper_link": "https://www.semanticscholar.org/paper/90272561048c823303cbeb95d53ea78316253d71"
            },
            {
                "title": "Understanding Search-Based Software Engineering in Defect Prediction",
                "abstract": "Software defect prediction models are essential for understanding quality attributes relevant for software organization to deliver better software reliability. This paper focuses mainly based on the selection of attributes in the perspective of software quality estimation for incremental database. A new dimensionality reduction method Wilk\u2019s Lambda Average Threshold (WLAT) is presented for selection of optimal features which are used for classifying modules as fault prone or not. This paper uses software metrics and defect data collected from benchmark data sets. The comparative results confirm that the statistical search algorithm (WLAT) outperforms the other relevant feature selection methods for most classifiers. The main advantage of the proposed WLAT method is: The selected features can be reused when there is increase or decrease in database size, without the need of extracting features afresh. In addition, performances of the defect prediction models either remains unchanged or improved even after eliminating 85% of the software metrics.",
                "paper_link": "https://www.semanticscholar.org/paper/110e8c1de2533f4cfee217c858de4a1b581e233c"
            }
        ]
    },
    {
        "Professor": "Yu Meng",
        "Papers": [
            {
                "title": "Weakly-supervised neural text classification",
                "abstract": "Deep neural networks are gaining increasing popularity for the classic text classification task, due to their strong expressive power and less requirement for feature engineering. Despite such attractiveness, neural text classification models suffer from the lack of training data in many real-world applications. Although many semi-supervised and weakly-supervised text classification models exist, they cannot be easily applied to deep neural models and meanwhile support limited supervision types. In this paper, we propose a weakly-supervised method that addresses the lack of training data in neural text classification. Our method consists of two modules: (1) a pseudo-document generator that leverages seed information to generate pseudo-labeled documents for model pre-training, and (2) a self-training module that bootstraps on real unlabeled data for model refinement. Our method has the flexibility to handle different types of weak supervision and can be easily integrated into existing deep neural models for text classification. We have performed extensive experiments on three real-world datasets from different domains. The results demonstrate that our proposed method achieves inspiring performance without requiring excessive training data and outperforms baseline methods significantly.",
                "paper_link": "https://www.semanticscholar.org/paper/89644d9869df2df4d26500ccdd46f6326856534d"
            },
            {
                "title": "Text classification using label names only: A language model self-training approach",
                "abstract": "Current text classification methods typically require a good number of human-labeled documents as training data, which can be costly and difficult to obtain in real applications. Humans can perform classification without seeing any labeled examples but only based on a small set of words describing the categories to be classified. In this paper, we explore the potential of only using the label name of each class to train classification models on unlabeled data, without using any labeled documents. We use pre-trained neural language models both as general linguistic knowledge sources for category understanding and as representation learning models for document classification. Our method (1) associates semantically related words with the label names, (2) finds category-indicative words and trains the model to predict their implied categories, and (3) generalizes the model via self-training. We show that our model achieves around 90% accuracy on four benchmark datasets including topic and sentiment classification without using any labeled documents but learning from unlabeled data supervised by at most 3 words (1 in most cases) per class as the label name.",
                "paper_link": "https://www.semanticscholar.org/paper/b30195763eb103e2e5564228119f3810ab423b2e"
            },
            {
                "title": "COCO-LM: Correcting and contrasting text sequences for language model pretraining",
                "abstract": "We present a self-supervised learning framework, COCO-LM, that pretrains Language Models by COrrecting and COntrasting corrupted text sequences. Following ELECTRA-style pretraining, COCO-LM employs an auxiliary language model to corrupt text sequences, upon which it constructs two new tasks for pretraining the main model. The first token-level task, Corrective Language Modeling, is to detect and correct tokens replaced by the auxiliary model, in order to better capture token-level semantics. The second sequence-level task, Sequence Contrastive Learning, is to align text sequences originated from the same source input while ensuring uniformity in the representation space. Experiments on GLUE and SQuAD demonstrate that COCO-LM not only outperforms recent state-of-the-art pretrained models in accuracy, but also improves pretraining efficiency. It achieves the MNLI accuracy of ELECTRA with 50% of its pretraining GPU hours. With the same pretraining steps of standard base/large-sized models, COCO-LM outperforms the previous best models by 1+ GLUE average points.",
                "paper_link": "https://www.semanticscholar.org/paper/19537be34dbadbcaa4fffcf028a8ada5095b1b5c"
            },
            {
                "title": "Generating training data with language models: Towards zero-shot language understanding",
                "abstract": "Pretrained language models (PLMs) have demonstrated remarkable performance in various natural language processing tasks: Unidirectional PLMs (e.g., GPT) are well known for their superior text generation capabilities; bidirectional PLMs (e.g., BERT) have been the prominent choice for natural language understanding (NLU) tasks. While both types of models have achieved promising few-shot learning performance, their potential for zero-shot learning has been underexplored. In this paper, we present a simple approach that uses both types of PLMs for fully zero-shot learning of NLU tasks without requiring any task-specific data: A unidirectional PLM generates class-conditioned texts guided by prompts, which are used as the training data for fine-tuning a bidirectional PLM. With quality training data selected based on the generation probability and regularization techniques (label smoothing and temporal ensembling) applied to the fine-tuning stage for better generalization and stability, our approach demonstrates strong performance across seven classification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and 92.8 on SST-2), significantly outperforming zero-shot prompting methods and achieving even comparable results to strong few-shot approaches using 32 training samples per class.",
                "paper_link": "https://www.semanticscholar.org/paper/23c265ba884b92ecbd9d18641078d964697e4590"
            },
            {
                "title": "Weakly-supervised hierarchical text classification",
                "abstract": "Hierarchical text classification, which aims to classify text documents into a given hierarchy, is an important task in many real-world applications. Recently, deep neural models are gaining increasing popularity for text classification due to their expressive power and minimum requirement for feature engineering. However, applying deep neural networks for hierarchical text classification remains challenging, because they heavily rely on a large amount of training data and meanwhile cannot easily determine appropriate levels of documents in the hierarchical setting. In this paper, we propose a weakly-supervised neural method for hierarchical text classification. Our method does not require a large amount of training data but requires only easy-to-provide weak supervision signals such as a few class-related documents or keywords. Our method effectively leverages such weak supervision signals to generate pseudo documents for model pre-training, and then performs self-training on real unlabeled data to iteratively refine the model. During the training process, our model features a hierarchical neural structure, which mimics the given hierarchy and is capable of determining the proper levels for documents with a blocking mechanism. Experiments on three datasets from different domains demonstrate the efficacy of our method compared with a comprehensive set of baselines.",
                "paper_link": "https://www.semanticscholar.org/paper/5a814b1b114a8eb0dc60276af467d76e6250ca73"
            },
            {
                "title": "Large language model as attributed training data generator: A tale of diversity and bias",
                "abstract": "Large language models (LLMs) have been recently leveraged as training data generators for various natural language processing (NLP) tasks. While previous research has explored different approaches to training models using generated data, they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of LLM. Thus, we investigate training data generation with diversely attributed prompts (e.g., specifying attributes like length and style), which have the potential to yield diverse and attributed generated data. Our investigation focuses on datasets with high cardinality and diverse domains, wherein we demonstrate that attributed prompts outperform simple class-conditional prompts in terms of the resulting model's performance. Additionally, we present a comprehensive empirical study on data generation encompassing vital aspects like bias, diversity, and efficiency, and highlight three key observations: firstly, synthetic datasets generated by simple prompts exhibit significant biases, such as regional bias; secondly, attribute diversity plays a pivotal role in enhancing model performance; lastly, attributed prompts achieve the performance of simple class-conditional prompts while utilizing only 5\\% of the querying cost of ChatGPT associated with the latter. The data and code are available on \\url{https://github.com/yueyu1030/AttrPrompt}.",
                "paper_link": "https://www.semanticscholar.org/paper/78c488e2d84bd193a40006b1fceb03e3845b81d4"
            },
            {
                "title": "Spherical text embedding",
                "abstract": "Unsupervised text embedding has shown great power in a wide range of NLP tasks. While text embeddings are typically learned in the Euclidean space, directional similarity is often more effective in tasks such as word similarity and document clustering, which creates a gap between the training stage and usage stage of text embedding. To close this gap, we propose a spherical generative model based on which unsupervised word and paragraph embeddings are jointly learned. To learn text embeddings in the spherical space, we develop an efficient optimization algorithm with convergence guarantee based on Riemannian optimization. Our model enjoys high efficiency and achieves state-of-the-art performances on various text embedding tasks including word similarity and document clustering.",
                "paper_link": "https://www.semanticscholar.org/paper/ed83aec58e526fc7969f203f40df60d0bf55e364"
            },
            {
                "title": "SimPO: Simple preference optimization with a reference-free reward",
                "abstract": "Direct Preference Optimization (DPO) is a widely used offline preference optimization algorithm that reparameterizes reward functions in reinforcement learning from human feedback (RLHF) to enhance simplicity and training stability. In this work, we propose SimPO, a simpler yet more effective approach. The effectiveness of SimPO is attributed to a key design: using the average log probability of a sequence as the implicit reward. This reward formulation better aligns with model generation and eliminates the need for a reference model, making it more compute and memory efficient. Additionally, we introduce a target reward margin to the Bradley-Terry objective to encourage a larger margin between the winning and losing responses, further improving the algorithm's performance. We compare SimPO to DPO and its latest variants across various state-of-the-art training setups, including both base and instruction-tuned models such as Mistral, Llama 3, and Gemma 2. We evaluate on extensive chat-based evaluation benchmarks, including AlpacaEval 2, MT-Bench, and Arena-Hard. Our results demonstrate that SimPO consistently and significantly outperforms existing approaches without substantially increasing response length. Specifically, SimPO outperforms DPO by up to 6.4 points on AlpacaEval 2 and by up to 7.5 points on Arena-Hard. Our top-performing model, built on Gemma-2-9B-it, achieves a 72.4% length-controlled win rate on AlpacaEval 2, a 59.1% win rate on Arena-Hard, and ranks 1st on Chatbot Arena among<10B models with real user votes.",
                "paper_link": "https://www.semanticscholar.org/paper/c3f1fae241a3c2449e675ab750873d800f95513c"
            },
            {
                "title": "Evaluating large language models at evaluating instruction following",
                "abstract": "As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever increasing list of models. This paper investigates the efficacy of these ``LLM evaluators'', particularly in using them to assess instruction following, a metric that gauges how closely generated text adheres to the given instruction. We introduce a challenging meta-evaluation benchmark, LLMBar, designed to test the ability of an LLM evaluator in discerning instruction-following outputs. The authors manually curated 419 pairs of outputs, one adhering to instructions while the other diverging, yet may possess deceptive qualities that mislead an LLM evaluator, e.g., a more engaging tone. Contrary to existing meta-evaluation, we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBar and even the highest-scoring ones have substantial room for improvement. We also present a novel suite of prompting strategies that further close the gap between LLM and human evaluators. With LLMBar, we hope to offer more insight into LLM evaluators and foster future research in developing better instruction-following models.",
                "paper_link": "https://www.semanticscholar.org/paper/6f217d984f36499d88ab8a3d89572171552e6f3f"
            },
            {
                "title": "Discriminative topic mining via category-name guided text embedding",
                "abstract": "Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users\u2019 particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.",
                "paper_link": "https://www.semanticscholar.org/paper/1fbb61a72a5a3970b235e0782b7fadcf2ecbf3aa"
            },
            {
                "title": "Hierarchical topic mining via joint spherical tree and text embedding",
                "abstract": "Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",
                "paper_link": "https://www.semanticscholar.org/paper/67afc9564e85a38c7f786b959614088a97d4787c"
            },
            {
                "title": "Distantly-supervised named entity recognition with noise-robust learning and language model augmented self-training",
                "abstract": "We study the problem of training named entity recognition (NER) models using only distantly-labeled data, which can be automatically obtained by matching entity mentions in the raw text with entity types in a knowledge base. The biggest challenge of distantly-supervised NER is that the distant supervision may induce incomplete and noisy labels, rendering the straightforward application of supervised learning ineffective. In this paper, we propose (1) a noise-robust learning scheme comprised of a new loss function and a noisy label removal step, for training NER models on distantly-labeled data, and (2) a self-training method that uses contextualized augmentations created by pre-trained language models to improve the generalization ability of the NER model. On three benchmark datasets, our method achieves superior performance, outperforming existing distantly-supervised NER models by significant margins.",
                "paper_link": "https://www.semanticscholar.org/paper/e199f191e54e662ea6a10a39d524ad767b5445cf"
            },
            {
                "title": "TaxoClass: Hierarchical multi-label text classification using only class names",
                "abstract": "Hierarchical multi-label text classification (HMTC) aims to tag each document with a set of classes from a taxonomic class hierarchy. Most existing HMTC methods train classifiers using massive human-labeled documents, which are often too costly to obtain in real-world applications. In this paper, we explore to conduct HMTC based on only class surface names as supervision signals. We observe that to perform HMTC, human experts typically first pinpoint a few most essential classes for the document as its \u201ccore classes\u201d, and then check core classes\u2019 ancestor classes to ensure the coverage. To mimic human experts, we propose a novel HMTC framework, named TaxoClass. Specifically, TaxoClass (1) calculates document-class similarities using a textual entailment model, (2) identifies a document\u2019s core classes and utilizes confident core classes to train a taxonomy-enhanced classifier, and (3) generalizes the classifier via multi-label self-training. Our experiments on two challenging datasets show TaxoClass can achieve around 0.71 Example-F1 using only class names, outperforming the best previous method by 25%.",
                "paper_link": "https://www.semanticscholar.org/paper/15e100120f080b9ef4230b4cbb8e107b76e2b839"
            },
            {
                "title": "Topic discovery via latent space clustering of pretrained language model representations",
                "abstract": "Topic models have been the prominent tools for automatic topic discovery from text corpora. Despite their effectiveness, topic models suffer from several limitations including the inability of modeling word ordering information in documents, the difficulty of incorporating external linguistic knowledge, and the lack of both accurate and efficient inference methods for approximating the intractable posterior. Recently, pretrained language models (PLMs) have brought astonishing performance improvements to a wide variety of tasks due to their superior representations of text. Interestingly, there have not been standard approaches to deploy PLMs for topic discovery as better alternatives to topic models. In this paper, we begin by analyzing the challenges of using PLM representations for topic discovery, and then propose a joint latent space learning and clustering framework built upon PLM embeddings. In the latent space, topic-word and document-topic distributions are jointly modeled so that the discovered topics can be interpreted by coherent and distinctive terms and meanwhile serve as meaningful summaries of the documents. Our model effectively leverages the strong representation power and superb linguistic features brought by PLMs for topic discovery, and is conceptually simpler than topic models. On two benchmark datasets in different domains, our model generates significantly more coherent and diverse topics than strong topic models, and offers better topic-wise document representations, based on both automatic and human evaluations.1",
                "paper_link": "https://www.semanticscholar.org/paper/424b08ba24a7c3a23d877d61eb02c184e6cc50c9"
            },
            {
                "title": "Tuning language models as training data generators for augmentation-enhanced few-shot learning",
                "abstract": "Recent studies have revealed the intriguing few-shot learning ability of pretrained language models (PLMs): They can quickly adapt to a new task when fine-tuned on a small amount of labeled data formulated as prompts, without requiring abundant task-specific annotations. Despite their promising performance, most existing few-shot approaches that only learn from the small training set still underperform fully supervised training by nontrivial margins. In this work, we study few-shot learning with PLMs from a different perspective: We first tune an autoregressive PLM on the few-shot samples and then use it as a generator to synthesize a large amount of novel training samples which augment the original training set. To encourage the generator to produce label-discriminative samples, we train it via weighted maximum likelihood where the weight of each token is automatically adjusted based on a discriminative meta-learning objective. A classification PLM can then be fine-tuned on both the few-shot and the synthetic samples with regularization for better generalization and stability. Our approach FewGen achieves an overall better result across seven classification tasks of the GLUE benchmark than existing few-shot learning methods, improving no-augmentation methods by 5+ average points, and outperforming augmentation methods by 3+ average points.",
                "paper_link": "https://www.semanticscholar.org/paper/7f3bc301ae0e2bbb78a0d42f074865e87d908f9a"
            },
            {
                "title": "HiGitClass: Keyword-driven hierarchical classification of github repositories",
                "abstract": "GitHub has become an important platform for code sharing and scientific exchange. With the massive number of repositories available, there is a pressing need for topic-based search. Even though the topic label functionality has been introduced, the majority of GitHub repositories do not have any labels, impeding the utility of search and topic-based analysis. This work targets the automatic repository classification problem as keyword-driven hierarchical classification. Specifically, users only need to provide a label hierarchy with keywords to supply as supervision. This setting is flexible, adaptive to the users' needs, accounts for the different granularity of topic labels and requires minimal human effort. We identify three key challenges of this problem, namely (1) the presence of multi-modal signals; (2) supervision scarcity and bias; (3) supervision format mismatch. In recognition of these challenges, we propose the HiGitClass framework, comprising of three modules: heterogeneous information network embedding; keyword enrichment; topic modeling and pseudo document generation. Experimental results on two GitHub repository collections confirm that HiGitClass is superior to existing weakly-supervised and dataless hierarchical classification methods, especially in its ability to integrate both structured and unstructured data for repository classification. Code and datasets related to this paper are available at https://github.com/yuzhimanhua/HiGitClass.",
                "paper_link": "https://www.semanticscholar.org/paper/975551547fef77605fb85a551bbd7523b77746b7"
            },
            {
                "title": "Weakly-supervised aspect-based sentiment analysis via joint aspect-sentiment topic embedding",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "CoRel: Seed-guided topical taxonomy construction by concept learning and relation transferring",
                "abstract": "Taxonomy is not only a fundamental form of knowledge representation, but also crucial to vast knowledge-rich applications, such as question answering and web search. Most existing taxonomy construction methods extract hypernym-hyponym entity pairs to organize a \"universal\" taxonomy. However, these generic taxonomies cannot satisfy user's specific interest in certain areas and relations. Moreover, the nature of instance taxonomy treats each node as a single word, which has low semantic coverage for people to fully understand. In this paper, we propose a method for seed-guided topical taxonomy construction, which takes a corpus and a seed taxonomy described by concept names as input, and constructs a more complete taxonomy based on user's interest, wherein each node is represented by a cluster of coherent terms. Our framework, CoRel, has two modules to fulfill this goal. A relation transferring module learns and transfers the user's interested relation along multiple paths to expand the seed taxonomy structure in width and depth. A concept learning module enriches the semantics of each concept node by jointly embedding the taxonomy and text. Comprehensive experiments conducted on real-world datasets show that CoRel generates high-quality topical taxonomies and outperforms all the baselines significantly.",
                "paper_link": "https://www.semanticscholar.org/paper/756fff358008e7a5d04cc99910c19ce05d5dc00e"
            },
            {
                "title": "Patton: Language model pretraining on text-rich networks",
                "abstract": "A real-world text corpus sometimes comprises not only text documents, but also semantic links between them (e.g., academic papers in a bibliographic network are linked by citations and co-authorships).Text documents and semantic connections form a text-rich network, which empowers a wide range of downstream tasks such as classification and retrieval.However, pretraining methods for such structures are still lacking, making it difficult to build one generic model that can be adapted to various tasks on text-rich networks.Current pretraining objectives, such as masked language modeling, purely model texts and do not take inter-document structure information into consideration.To this end, we propose our PretrAining on TexT-Rich NetwOrk framework Patton.Patton includes two pretraining strategies: network-contextualized masked language modeling and masked node prediction, to capture the inherent dependency between textual attributes and network structure.We conduct experiments on four downstream tasks in five datasets from both academic and e-commerce domains, where Patton outperforms baselines significantly and consistently.",
                "paper_link": "https://www.semanticscholar.org/paper/3a755f8dcc9af9304c2cbd3a00e42e66feec1d5d"
            },
            {
                "title": "UCPhrase: Unsupervised context-aware quality phrase tagging",
                "abstract": "Identifying and understanding quality phrases from context is a fundamental task in text mining. The most challenging part of this task arguably lies in uncommon, emerging, and domain-specific phrases. The infrequent nature of these phrases significantly hurts the performance of phrase mining methods that rely on sufficient phrase occurrences in the input corpus. Context-aware tagging models, though not restricted by frequency, heavily rely on domain experts for either massive sentence-level gold labels or handcrafted gazetteers. In this work, we propose UCPhrase, a novel unsupervised context-aware quality phrase tagger. Specifically, we induce high-quality phrase spans as silver labels from consistently co-occurring word sequences within each document. Compared with typical context-agnostic distant supervision based on existing knowledge bases (KBs), our silver labels root deeply in the input domain and context, thus having unique advantages in preserving contextual completeness and capturing emerging, out-of-KB phrases. Training a conventional neural tagger based on silver labels usually faces the risk of overfitting phrase surface names. Alternatively, we observe that the contextualized attention maps generated from a transformer-based neural language model effectively reveal the connections between words in a surface-agnostic way. Therefore, we pair such attention maps with the silver labels to train a lightweight span prediction model, which can be applied to new input to recognize (unseen) quality phrases regardless of their surface names or frequency. Thorough experiments on various tasks and datasets, including corpus-level phrase ranking, document-level keyphrase extraction, and sentence-level phrase tagging, demonstrate the superiority of our design over state-of-the-art pre-trained, unsupervised, and distantly supervised methods.",
                "paper_link": "https://www.semanticscholar.org/paper/bd78bb32e183d38c60c248c54362a30ac7bcc65d"
            }
        ]
    },
    {
        "Professor": "Briana Morrison",
        "Papers": [
            {
                "title": "Khan academy gamifies computer science",
                "abstract": "Gamification is the buzzword for adding gaming elements such as points or badges to learning experiences to make them more engaging and to increase motivation. In this paper we explore how Khan Academy has incorporated gaming elements into its CS learning platform. By mapping the literature on motivational processes to popular games we critically analyze how successful Khan Academy is at gamifying their site.",
                "paper_link": "https://www.semanticscholar.org/paper/63bc8f9c583e0f4d18a6ffe9de0372a632fd9d20"
            },
            {
                "title": "Subgoals, context, and worked examples in learning computing problem solving",
                "abstract": "Recent empirical results suggest that the instructional material used to teach computing may actually overload students' cognitive abilities. Better designed materials may enhance learning by reducing unnecessary load. Subgoal labels have been shown to be effective at reducing the cognitive load during problem solving in both mathematics and science. Until now, subgoal labels have been given to students to learn passively. We report on a study to determine if giving learners subgoal labels is more or less effective than asking learners to generate subgoal labels within an introductory CS programming task. The answers are mixed and depend on other features of the instructional materials. We found that student performance gains did not replicate as expected in the introductory CS task for those who were given subgoal labels. Computer science may require different kinds of problem-solving or may generate different cognitive demands than mathematics or science.",
                "paper_link": "https://www.semanticscholar.org/paper/86c5a7cd1ddec48fb818756d2043fde4ed802908"
            },
            {
                "title": "Measuring cognitive load in introductory CS: adaptation of an instrument",
                "abstract": "A student's capacity to learn a concept is directly related to how much cognitive load is used to comprehend the material. The central problem identified by Cognitive Load Theory is that learning is impaired when the total amount of processing requirements exceeds the limited capacity of working memory. Instruction can impose three different types of cognitive load on a student's working memory: intrinsic load, extraneous load, and germane load. Since working memory is a fixed size, instructional material should be designed to minimize the extraneous and intrinsic loads in order to increase the amount of memory available for the germane load. This will improve learning. To effectively design instruction to minimize cognitive load we must be able to measure the specific load components for any pedagogical intervention. This paper reports on a study that adapts a previously developed instrument to measure cognitive load. We report on the adaptation of the instrument to a new discipline, introductory computer science, and the results of measuring the cognitive load factors of specific lectures. We discuss the implications for the ability to measure specific cognitive load components and use of the tool in future studies.",
                "paper_link": "https://www.semanticscholar.org/paper/201a124fa0f849e49be7496dc016cc9ff79e9205"
            },
            {
                "title": "Subgoals help students solve Parsons problems",
                "abstract": "We report on a study that used subgoal labels to teach students how to write while loops with a Parsons problem learning assessment. Subgoal labels were used to aid learning of programming while not overloading students' cognitive abilities. We wanted to compare giving learners subgoal labels versus asking learners to generate subgoal labels. As an assessment for learning we asked students to solve a Parsons problem -- to place code segments in the correct order. We found that students who were given subgoal labels performed statistically better than the groups that did not receive subgoal labels or were asked to generate subgoal labels. We conclude that a low cognitive load assessment, Parsons problems, can be more sensitive to student learning gains than traditional code generation problems.",
                "paper_link": "https://www.semanticscholar.org/paper/28528b2b630a80db4b845d4e0ab71a1ccd0cb285"
            },
            {
                "title": "Analysis of interactive features designed to enhance learning in an ebook",
                "abstract": "Educational psychology findings indicate that active processing (such as self-testing) is more effective for learning than passive reading or even rereading. Electronic books (ebooks) can include much more than static pictures and text. Ebooks can promote better learning by increasing the reader's interaction with the material through multi-modal learning supports, worked examples, and low cognitive load practice activities. For example, multiple choice questions with immediate feedback can help identify misconceptions and gaps in knowledge. Parsons problems, which are mixed up code segments that have to be put in the correct order, require learners to think about the order of the statements in a solution without having to worry about syntax errors. Our research group has been applying concepts from educational psychology to make learning from ebooks more effective and efficient. This paper reports on an observational study and log file analysis on the use of an ebook that incorporates interactive activities. We provide evidence that learners engaged in the interactive activities, but used some types of activities more than others. We also found evidence that learners encountered some \"desirable difficulties\" which can improve learning. This descriptive study informs a research agenda to improve the quality of instruction in computing education.",
                "paper_link": "https://www.semanticscholar.org/paper/0a0be1544ef5dcaf6078b35a0ac1c2b94e25d707"
            },
            {
                "title": "Women catch up: gender differences in learning programming concepts",
                "abstract": "This paper describes a multi-institutional study that used categorization exercises (known as constrained card sorts) to investigate gender differences in graduating computer science students' learning and perceptions of programming concepts. Our results show that female subjects had significantly less pre-college programming experience than their male counterparts. However, for both males and females, we found no correlation between previous experience and success in the major, as measured by computer science grade point average at graduation. Data also indicated that, by the time students completed their introductory courses, females reported nearly equal levels of mastery as males of the programming concepts. Furthermore, females generally considered the programming concepts to be no more difficult than did the men.",
                "paper_link": "https://www.semanticscholar.org/paper/4ea5f5d3eb33d9f7f715aa7b34939fdfd7155074"
            },
            {
                "title": "Engagement: Gaming throughout the curriculum",
                "abstract": "This paper considers how gaming has been infused into the computing curriculum of institutions in the United States. To increase motivation of students and improve retention, many programs have begun using gaming in their introductory courses, as upper level electives, or as separate degree programs. The authors review the current use of gaming within curricula and analyze the content of game development degree programs. Finally, the authors describe plans at their institution to incorporate gaming throughout the computing curriculum and present initial results.",
                "paper_link": "https://www.semanticscholar.org/paper/0344534b316c680d4c795ed46bd5a28e2bd0f004"
            },
            {
                "title": "Reducing withdrawal and failure rates in introductory programming with subgoal labeled worked examples",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/68e8f9d04599d995f6fd520288d2a6067e83ae3f"
            },
            {
                "title": "Growing computer science education into a STEM education discipline",
                "abstract": "Seeking to make computing education as available as mathematics or science education.",
                "paper_link": "https://www.semanticscholar.org/paper/6b6fbc3a07557a0e94e94db7e40c0aaf360aa3ad"
            },
            {
                "title": "Building a community to support HS CS teachers: the disciplinary commons for computing educators",
                "abstract": "In this paper, we describe our experience in supporting high school CS teachers by building a local community through the Disciplinary Commons for Computing Educators (DCCE) project. The DCCE project is an effort to explore ways of supporting these CS teachers through the creation of a local community and by promoting teacher reflection. DCCE achieved this goal through an academic-year-long program where a cohort of CS teachers engaged in collaborative portfolio creation and peer observation of classroom teaching. We describe the design of the DCCE activities and present preliminary results from initial evaluations. Our short-term evaluations indicate that this project was successful in creating a supportive community, promoting teacher reflection, and advancing change in teaching practices among a group of computing educators.",
                "paper_link": "https://www.semanticscholar.org/paper/fb73c611a57726afb338c2ade6a0da546d06b6f9"
            },
            {
                "title": "Identifying design principles for CS teacher Ebooks through design-based research",
                "abstract": "Several countries are trying to provide access to computing education for all secondary students. However, there are not enough teachers who are prepared to teach computer science. Interactive electronic books (ebooks) are a promising approach for providing low-cost professional development in computer science. Over the last four years, our research group has been conducting design-based research by iteratively developing and testing versions of a teacher ebook to help secondary teachers with no programming experience learn to teach an introductory programming course. The interactive elements in the ebook were designed based on research results from educational psychology and are intended to make learning more efficient and effective. Our goals for this effort are to increase teachers' knowledge of computer science concepts and to improve teachers' confidence in their ability to teach computer science. In this paper we summarize our previous work and report on a large-scale study of version two of the teacher ebook. We also recommend several design principles for interactive ebooks for computing teachers based on feedback from teachers, log file analyses, and randomized controlled studies.",
                "paper_link": "https://www.semanticscholar.org/paper/8254fbbcf627fd786914d6202ead6bde8ad431ab"
            },
            {
                "title": "A multi\u2010institutional, multinational study of programming concepts using card sort data",
                "abstract": "Abstract: This paper presents a case study of the use of a repeated single\u2010criterion card sort with an unusually large, diverse participant group. The study, whose goal was to elicit novice programmers' knowledge of programming concepts, involved over 20 researchers from four continents and 276 participants drawn from 20 different institutions. In this paper we present the design of the study and the unexpected result that there were few discernible systematic differences in the population. The study was one of the activities of the National Science Foundation funded Bootstrapping Research in Computer Science Education project (2003).",
                "paper_link": "https://www.semanticscholar.org/paper/a49e1635cbf4b5db8968393c0e9256f2c89a4f4c"
            },
            {
                "title": "The dimensions of variation in the teaching of data structures",
                "abstract": "The current debate about the teaching of data structures is hampered because, as a community, we usually debate specifics about data structure implementations and libraries, when the real level of disagreement remains implicit -- the intent behind our teaching. This paper presents a phenomenographic study of the intent of CS educators for teaching data structures in CS2. Based on interviews with Computer Science educators and analysis of CS literature, we identified five categories of intent: developing transferable thinking, improving students' programming skills, knowing \"what's under the hood\", knowledge of software libraries, and component thinking. The CS community needs to first debate at the level of these categories before moving to more specific issues. This study also serves as an example of how phenomenographic analysis can be used to inform debate on syllabus design in general.",
                "paper_link": "https://www.semanticscholar.org/paper/924b82d0ce45a077b35284d4145a2795631ef101"
            },
            {
                "title": "Information seeking practices of parents: Exploring skills, face threats and social networks",
                "abstract": "Parents are often responsible for finding, selecting, and facilitating their children's out-of-school learning experiences. One might expect that the recent surge in online educational tools and the vast online network of information about informal learning would make this easier for all parents. Instead, the increase in these free, accessible resources is contributing to an inequality of use between children from lower and higher socio-economic status (SES). Through over 60 interviews with a diverse group of parents, we explored parents' ability to find learning opportunities and their role in facilitating educational experiences for their children. We identified differences in the use of online social networks in finding learning opportunities for their children based on SES. Building upon these findings, we conducted a national survey in partnership with ACT, an educational testing services organization, to understand if these differences were generalizable to and consistent among a broader audience.",
                "paper_link": "https://www.semanticscholar.org/paper/e7a39a0d7c7dbc2d7f0fd364a2edfd7cb8d80c56"
            },
            {
                "title": "Parsons problems and beyond: Systematic literature review and empirical study designs",
                "abstract": "Programming is a complex task that requires the development of many skills including knowledge of syntax, problem decomposition, algorithm development, and debugging. Code-writing activities are commonly used to help students develop these skills, but the difficulty of writing code from a blank page can overwhelm many novices. Parsons problems offer a simpler alternative to writing code by providing scrambled code blocks that must be placed in the correct order to solve a problem. In the 16 years since their introduction to the computing education community, an expansive body of literature has emerged that documents a range of tools, novel problem variations and makes numerous claims of benefits to learners. In this work, we track the origins of Parsons problems, outline their defining characteristics, and conduct a comprehensive review of the literature to document the evidence of benefits to learners and to identify gaps that require exploration. To facilitate future work, we design empirical studies and develop associated resources that are ready for deployment at a large scale. Collectively, this review and the provided experimental resources will serve as a focal point for researchers interested in advancing our understanding of Parsons problems and their benefits to learners.",
                "paper_link": "https://www.semanticscholar.org/paper/11d2a5c502d2e1f78ff5e40855142d3a38e6602f"
            },
            {
                "title": "Requirements and design strategies for open source interactive computer science eBooks",
                "abstract": "Online education supported by digital courseware will radically alter higher education in ways that we cannot predict. New technologies such as MOOCs and Khan Academy have generated interest in new models for knowledge delivery. The nature of Computer Science content provides special opportunities for computer-supported delivery in both traditional and online classes. Traditional CS textbooks are likely to be replaced by online materials that tightly integrate content with visualizations and automatically assessed exercises. We refer to these new textbook-like artifacts as icseBooks (pronounced \\ice books\"), for interactive computer science electronic books. IcseBook technology will in turn impact the pedagogy used in CS courses. This report surveys the state of the field, addresses new use cases for CS pedagogy with icseBooks, and lays out a series of research questions for future study.",
                "paper_link": "https://www.semanticscholar.org/paper/0e94f57e6f2f8a523e1a0535fc28f683b674de70"
            },
            {
                "title": "Cognitive sciences for computing education",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/1cde5bf31094de56ac3e235e7d19140c60c1e745"
            },
            {
                "title": "Design and pilot testing of subgoal labeled worked examples for five core concepts in CS1",
                "abstract": "Subgoal learning has improved student problem-solving performance in programming, but it has been tested for only one-to-two hours of instruction at a time. Our work pioneers implementing subgoal learning throughout an entire introductory programming course. In this paper we discuss the protocol that we used to identify subgoals for core programming procedures, present the subgoal labels created for the course, and outline the subgoal-labeled instructional materials that were designed for a Java-based course. To examine the effect of subgoal labeled materials on student performance in the course, we compared quiz and exam grades between students who learned using subgoal labels and those who learned using conventional materials. Initial results indicate that learning with subgoals improves performance on early applications of concepts. Moreover, variance in performance was lower and persistence in the course was higher for students who learned with subgoals compared to those who learned with conventional materials, suggesting that learning with subgoal labels may uniquely benefit students who would normally receive low grades or dropout of the course.",
                "paper_link": "https://www.semanticscholar.org/paper/f13b2a915d3d2e84b97ef7f13b530a404b08b70f"
            },
            {
                "title": "Making sense of data structures exams",
                "abstract": "Is there consensus on what students should learn in CS2? Should they learn to use data structures, understand their specific implementation details, or both? Finally, has the computing education community's answer to the second question changed over time? In this paper, we begin to explore these questions based on an analysis of a key artifact instructors use to assess their students' performance: their final exams. Specifically, we look at two CS2 concepts as covered in those exams: stacks and hashtables.\n Our dataset includes 76 exams from 14 institutions around the world spanning 1973-2009 that were gathered as part of the DCER project, which is investigating the feasibility of a repository for computing education research data; to our knowledge this is a novel dataset in computing education. We begin by giving a general feel for this extensive dataset by describing the formats and difficulty level of the stack and hashtable questions and the computing skill students must possess to answer them. Next, we look at the questions' assessment of implementation knowledge versus interface or application knowledge. Despite a number of calls for modern CS2 to focus more on application than implementation, we found no evidence of such a trend. We note, however, that there are institutional differences in the data, and that there are alternative ways in which application may be assessed in a course.",
                "paper_link": "https://www.semanticscholar.org/paper/28e55199942ce6625075a8e42333119c4895a741"
            },
            {
                "title": "Adapting the disciplinary commons model for high school teachers: improving recruitment, creating community",
                "abstract": "The Disciplinary Commons (DC) is a model of teacher professional development that encourages members of the group to reflect upon their teaching practices, develop a community, and, more broadly, to become more scholarly about their teaching. The DC involves a series of monthly meetings where university faculty members examine their course in detail while producing a course portfolio. Evaluation of the early DC's suggests that they successfully created a sense of community and sharing among the participants. We have adapted the original model to a new audience, high school computing teachers. The adapted model maintains the key aspects of the original model while adding two new, important goals for this new audience: improving recruitment and creating community. The high school teacher audience particularly needed strategies for recruiting students and was in greater need of community. We present evaluation evidence suggesting that we achieved the design goals in a replicable model, including a substantial increase (over 300%) in recruiting students.",
                "paper_link": "https://www.semanticscholar.org/paper/e3030861610d7c6438e5ed61909918c7c984a588"
            }
        ]
    },
    {
        "Professor": "N. Rich Nguyen",
        "Papers": [
            {
                "title": "Detection of cracks in nuclear power plant using spatial-temporal grouping of local patches",
                "abstract": "Robust inspection is important to ensure the safety of nuclear power plant components. An automated approach would require detecting often low contrast cracks that could be surrounded by or even within textures with similar appearances such as welding, scratches and grind marks. We propose a crack detection method for nuclear power plant inspection videos by fine tuning a deep neural network for detecting local patches containing cracks which are then grouped in spatial-temporal space for group-level classification. We evaluate the proposed method on a data set consisting of 17 videos consisting of nearly 150,000 frames of inspection video and provide comparison to prior methods.",
                "paper_link": "https://www.semanticscholar.org/paper/355b743f1a20691ca6ca791a0dedb8ab329ba767"
            },
            {
                "title": "Thinking about computational thinking:Origins of computational thinking in educational computing",
                "abstract": "The recent focus upon Computational Thinking and suggestions that Computational Thinking (CT) should be included in preK-12 education as well as teacher education brings back memories of earlier times in the area of technology in teacher education. Looking back at the history of our field, some of us remember an early (1980\u2019s) emphasis on CT. Although most did not call it CT at the time, many of the early leaders and followers in the Logo movement were interested and inspired by the possibilities of the computer as \u201can object to think with\u201d and an environment where children could learn important problem solving skills in an active, student centered way. As Glen Bull, Joe Garfalo, and Rich Nguyen mention in their article in this issue: \u201cThinking About Computational Thinking: Origins of Computational Thinking in Educational Computing,\u201d Seymour Papert provided a clear vision for using technology to provide problem solving environments where children could develop skills that we now call CT. Papert\u2019s visions for technology use in schools were clearly articulated in Mindstorms (1980) and these visions motivated many of us to begin to define and explore uses of computers in classrooms. Much of this early work involved designing environments for using the Logo language in classrooms. More than 40 years have passed since this early work with Logo and much of the early enthusiasm for Logo as a tool for classroom teachers waned after the initial excitement over Papert\u2019s vision. In fact, younger researchers and educators may view the current emphasis on CT as a new area of emphasis for technology in teacher education rather than the reemergence of a significant piece of our history. Honoring, understanding and using the history of Logo and Papert\u2019s visions for computer use in classrooms needs to be an important part of the current work in CT and the \u201cThinking about Computational Thinking\u201d article in this issue should be a required read for those working on CT research and development today. Forty years ago Papert presented a clear vision of the importance of CT in schools and specific ideas for how to incorporate CT in classrooms. After initial enthusiasm for Papert\u2019s approach, however, classroom use of computers turned toward using computers to augment and enhance existing curriculum and methods. It is important to understand what happened to the early efforts to incorporate CT in schools and the use of the computer as \u201can object to think with.\u201d Looking back, we suggest there are four major reasons that our initial uses of computers in schools focused upon integrating computers into existing curriculum and methods and not on creating environments for learning CT.",
                "paper_link": "https://www.semanticscholar.org/paper/625a8649f9a7a52ab75a0d6fa7968afd5e306a98"
            },
            {
                "title": "Hydrogen sulfide modulates sinusoidal constriction and contributes to hepatic micorcirculatory dysfunction during endotoxemia",
                "abstract": "Hydrogen sulfide (H\u2082S) affects vascular resistance; however, its effect on the hepatic microcirculation has not been investigated. Hepatic sinusoidal perfusion is dysregulated during sepsis, contributing to liver injury. Therefore, the present study determined the effect of H\u2082S on the hepatic microcirculation and the contribution of endogenous H\u2082S to hepatic microcirculatory dysfunction in an endotoxin model of sepsis. Portal infusion of H\u2082S increased portal pressure in vivo (6.8 \u00b1 0.2 mmHg before H\u2082S vs. 8.6 \u00b1 0.8 mmHg peak during H\u2082S infusion, P < 0.05). Using intravital microscopy, we observed decreased sinusoidal diameter (6.2 \u00b1 0.27 \u03bcm before H\u2082S vs. 5.7 \u00b1 0.3 \u03bcm after H\u2082S, P < 0.05) and increased sinusoidal heterogeneity during H\u2082S infusion (P < 0.05) and net constriction. Since hepatic H\u2082S levels are elevated during sepsis, we used the cystathionine \u03b3 lyase inhibitor DL-propargylglycine (PAG) to determine the contribution of H\u2082S to the hypersensitization of the sinusoid to the vasoconstrictor effect of endothelin-1 (ET-1). PAG treatment significantly attenuated the sinusoidal sensitization to ET-1 in endotoxin-treated animals. ET-1 infusion increased portal pressure to 175% of baseline in endotoxemic animals, which was reduced to 143% following PAG treatment (P < 0.05). PAG abrogated the increase in sinusoidal constriction after ET-1 infusion in LPS-treated rats (30.9% reduction in LPS rats vs. 11.6% in PAG/LPS rats, P < 0.05). Moreover, PAG treatment significantly attenuated the increase in NADH fluorescence following ET-1 exposure during endotoxemia (61 grayscale units LPS vs. 21 units in PAG/LPS, P < 0.05), suggesting an improvement in hepatic oxygen availability. This study is the first to demonstrate a vasoconstrictor action of H\u2082S on the hepatic sinusoid and provides a possible mechanism for the protective effect of PAG treatment during sepsis.",
                "paper_link": "https://www.semanticscholar.org/paper/e6aae002e9099a531e3696814aef96d036711ac6"
            },
            {
                "title": "Improving pollen classification with less training effort",
                "abstract": "The pollen grains of different plant taxa exhibit various shapes and sizes. This structural diversity has made the identification and classification of pollen grains an important tool in many fields. Despite the myriad of applications, the classification of pollen grains is still a tedious and time-consuming process that must be performed by highly skilled specialists. In this paper, we propose an automatic classification method to discriminate pollen grains coming from a variety of taxonomic types. First, we develop a new feature that captures the spikes of pollen to improve the classification accuracy. Second, we take advantage of the classification rules extracted from the existing pollen types and apply them to the new types. Third, we introduce a new selection criterion to obtain the most valuable training samples from the unlabeled data and therefore reduce the number of needed training samples. Our experiment demonstrates that the proposed method reduces the training effort of a human expert up to 80% compared to other classification methods while achieving 92% accuracy in pollen classification.",
                "paper_link": "https://www.semanticscholar.org/paper/36eb756d4e7447ffdc11fa8a5f5389f9026bf624"
            },
            {
                "title": "Tracking colliding cells in vivo microscopy",
                "abstract": "Leukocyte motion represents an important component in the innate immune response to infection. Intravital microscopy is a powerful tool as it enables in vivo imaging of leukocyte motion. Under inflammatory conditions, leukocytes may exhibit various motion behaviors, such as flowing, rolling, and adhering. With many leukocytes moving at a wide range of speeds, collisions occur. These collisions result in abrupt changes in the motion and appearance of leukocytes. Manual analysis is tedious, error prone, time consuming, and could introduce technician-related bias. Automatic tracking is also challenging due to the noise inherent in in vivo images and abrupt changes in motion and appearance due to collision. This paper presents a method to automatically track multiple cells undergoing collisions by modeling the appearance and motion for each collision state and testing collision hypotheses of possible transitions between states. The tracking results are demonstrated using in vivo intravital microscopy image sequences. We demonstrate that 1) 71% of colliding cells are correctly tracked; (2) the improvement of the proposed method is enhanced when the duration of collision increases; and (3) given good detection results, the proposed method can correctly track 88% of colliding cells. The method minimizes the tracking failures under collisions and, therefore, allows more robust analysis in the study of leukocyte behaviors responding to inflammatory conditions.",
                "paper_link": "https://www.semanticscholar.org/paper/c3323dcbae4e96a7929770a651a1b94e34ac4052"
            },
            {
                "title": "Automatic detection of cracks during power plant inspection",
                "abstract": "Robust inspection is important to ensure the safety of nuclear power plant components. Manually inspecting 100+ hours of video for rarely occurring cracks is a tedious process. However, automatic inspection is challenging as the images often contain highly textured area including weld and concrete surface which causes fragmented and noisy segmentations. Moreover, lack of crack samples cause challenges in training classification methods. In this paper, we propose to improve the detection of cracks by (1) reducing the fragmentation of segmentation by iteratively linking of possibly broken short lines that we call \u201clinelets,\u201d (2) minimize the false positive rate by filtering out area with weld, and (3) using anomaly measure to improve the classification. Testing of 42 real images demonstrates 38% improvement over prior method.",
                "paper_link": "https://www.semanticscholar.org/paper/43b091a7d98501352d6436a0e6098d0f79fa6ffc"
            },
            {
                "title": "Reducing leukocyte trafficking preserves hepatic function after sepsis",
                "abstract": "INTRODUCTION\nLeukocyte trafficking may induce hepatic dysfunction in sepsis. Herein, we hypothesize that reduction in leukocyte adhesion and, hence, leukocyte-endothelial interaction by activated protein C (aPC) may preserve hepatic function after sepsis.\n\n\nMETHODS\nRats underwent sham or cecal ligation and puncture, followed by saline or aPC (1 mg/kg intravenously) infusion, twice daily for 4 days. Cytokine levels were determined by enzyme-linked immunosorbent assay. Liver function and injury were assessed by bile production and plasma aspartate transaminase, respectively. In parallel experiments, neutrophils were labeled with Rhodamine 6G, and trafficking determined by cell motion tracking using intravital microscopy. Leukocyte trafficking and traveling velocity were computed at baseline and at 10 minutes and 40 minutes after endothelin-1 infusion.\n\n\nRESULTS\nSepsis induced 90% mortality and elevated levels of interleukin (IL)-2 (167 pg/mL +/- 39 pg/mL vs. 68 pg/mL +/- 2 pg/mL, p < 0.05), IL-6 (5,806 pg/mL +/- 3,389 pg/mL vs. 0 pg/mL +/- 0 pg/mL, p < 0.05), and IL-8 (492 pg/mL +/- 22 pg/mL vs. 21 pg/mL +/- 17 pg/mL, p < 0.05). Aspartate transaminase levels increased (227 IU/L +/- 14 IU/L vs. 51 IU/L +/- 7 IU/L, p < 0.05) in cecal ligation and puncture animals, whereas bile production decreased by fivefold compared with sham (436 microg/kg/h +/- 247 microg/kg/h vs. 2,357 microg/kg/h +/- 147 microg/kg/h, p < 0.05). Hepatic leukocyte adhesion increased threefold in septic animals (42.7 WBC per image +/- 7.3 WBC per image vs. 14.8 WBC per image +/- 3.8 WBC per image, p < 0.01), whereas leukocyte velocity decreased compared with sham (10.5 microm/s +/- 2.2 microm/s vs. 22.3 microm/s +/- 2.4 microm/s, p < 0.01). By contrast, aPC treatment reduced mortality to 60%, attenuated inflammatory cytokines, reduced leukocyte trafficking, and preserved hepatic function.\n\n\nCONCLUSIONS\nOur data demonstrate that sepsis may, in part, induce hepatic dysfunction by augmenting leukocyte trafficking into hepatic sinusoids. Treatment with aPC attenuated leukocyte trafficking and, in doing so, preserved hepatic function and improved survival. Collectively, these data suggest an important role for protein C-dependent leukocyte-endothelial interaction in sepsis.",
                "paper_link": "https://www.semanticscholar.org/paper/5cb90aaa400091b85c326916d62fe450356d69c9"
            },
            {
                "title": "Rapidly adaptive cell detection using transfer learning with a global parameter",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/45aaf242bc90b2c8fe3c1609cb6ff9d73a0a0beb"
            },
            {
                "title": "Automated Structural Evaluation of Block-based Coding Assignments",
                "abstract": "As computer science is integrated into a wider variety of fields, block-based programming languages like Snap!, which assemble code with visual blocks rather than text syntax, are increasingly used to teach computational thinking (CT) to students from diverse backgrounds. Although automated evaluators (autograders) for programming assignments usually focus on runtime efficiency and output accuracy, effective evaluation of a student's CT skills requires assessing coding best practices, such as decomposition, abstraction, and algorithm design. While autograders are commonplace for text languages like Python, we present a machine learning approach to assess how effectively block-based code demonstrates understanding of CT fundamentals. Our dataset consists of Snap! programs written by students new to coding and evaluated by instructors using a CT rubric. We explore how to best transform these programs into low-dimensional features to allow encapsulation and repetition patterns to emerge. Experimentation involves comparing the effectiveness of a suite of clustering models and similarity metrics by analyzing how directly automated feedback correlates to the course staff's manual evaluation. Lastly, we demonstrate the practical application of the autograder in a classroom setting and discuss scalability and feasibility in other domains of CS education.",
                "paper_link": "https://www.semanticscholar.org/paper/94f2cab6877301582e029666b26675a361c7a0ec"
            },
            {
                "title": "Deep Learning Risk Prediction of Bloodstream Infection in the Intensive Care Unit",
                "abstract": "Bloodstream infection is a leading cause of mortality in patients in the intensive care unit (ICU). Early detection and treatment of bloodstream infection is associated with significantly better clinical outcomes. However, the detection of bloodstream infection requires a time-consuming blood culture. Far more blood cultures are ordered than return positive; this excess of blood cultures can cause delays in result and therefore treatment. In this study, we aim to move towards a continuous monitoring tool to help doctors and nurses in the ICU identify which patients require a blood culture, with the goal of supporting earlier detection and treatment of bloodstream infection. We formulated this goal as a multivariate time-series classification problem and applied powerful predictive deep learning approaches to model multivariate time-series data. We used a variety of model validation and explainability techniques to help understand the decisions of these deep learning models and promote trust in their predictions.",
                "paper_link": "https://www.semanticscholar.org/paper/caa54b9707e195a1145ba6da90e27f4e185e853b"
            },
            {
                "title": "Toward an open-source toolkit for machine learning education",
                "abstract": "Machine Learning (ML) has become one of the highly participated courses at the undergraduate level in Computer Science. Open-source ML libraries make it easy for students to implement papers, share ideas, and conduct experiments on large scale datasets. With the emergence of public dataset portals (such as Kaggle, Amazon Datasets, and Google Datasets Search), the open-source community has produced many useful, high-quality libraries (such as Scikit-Learn, PyTorch, Keras, and Tensorflow among others). These open-source tools aim to make state-of-the-art ML algorithms and large-scale datasets accessible to all. While these ML libraries and datasets can benefit many undergraduate students in their pursuit of data-related careers, the task of choosing them for instructional purposes can be daunting for two reasons. First, all of these tools have advantages, drawbacks, and many overlapping issues. There is no single tool or dataset that covers all of the ML instructional needs. Second, due to the rapid advancements in the field, instructors often find a lack of comprehensive guidelines or standards on evaluating the instructional usability and real-world performance of open-source tools. How can these libraries and tools be integrated to aid the instructional activities of both classical machine learning as well as deep learning? This BOF will provide a platform for the discussion of the development of an open-source toolkit to support the teaching and learning of ML at the undergraduate level.",
                "paper_link": "https://www.semanticscholar.org/paper/ae7b4342331d442e84f85f91d204a07f4822ecd0"
            },
            {
                "title": "CodeNC: integrating computational thinking into K-12 instructional activities using animated videos",
                "abstract": "Increasing the representation of minorities in computer science (CS) has become a national priority. One of the many reasons minority students nationwide choose not to study CS is that they often lack mentors and role models to encourage them early in their learning. In her seminal article, Jeannette Wing argues that computational thinking (CT) is an emerging essential skill that should become an integral part of K-12 education. However, a big issue facing this initiative is that many K-12 teachers find themselves lack (1) relevant materials, (2) systematic training, and (3) a supportive community. The democratization of media, such as photos and videos, has provided a great variety of options to educate a broad audience on myriad topics. In this poster, we will describe the challenges and successes of using animated videos including its beauty, soundness, and utility as critical elements in establishing a strong CT comprehension while engaging K-12 teachers in a non-threatening way. Using a process of iterative design, we have found ways to integrate CT concepts in six non-CS disciplines in the K-12 curriculum. The teachers who have collaborated with us respond positively that this design approach provides them with a greater comprehension of the CT concepts while giving them exciting instructional activities. Therefore, this poster may be of interest to any CS educator who wishes to improve the engagement of K-12 teachers while sustaining a CT training program at their institution.",
                "paper_link": "https://www.semanticscholar.org/paper/25473ca33031eaed1778468afcc51c00021ed9ac"
            },
            {
                "title": "TUNESCOPE Creating Digital Music in Snap!",
                "abstract": "Steganography is a method that has been used for years to communicate information in secret. These covert communications are placed into truthful carriers. Digital photos, music files, video data, and so on may all be carriers. The study introduces RSTEG (Retransmission STEGanography), a novel stenographic technique meant for a wide range of protocols that make use of rebroadcast techniques. RSTEG\u2019s primary novelty is its deliberate use of retransmission to force a packet to be acknowledged even after it has been completely acknowledged. The payload field of the transmitted packet contains a steganogram rather than user data. We examine the properties of RSTEG in this study. As part of the recipient\u2019s covert communication strategy, a selected segment\u2019s payload has an Identifying Sequence (IS) strategically inserted within it. After delivery is successful, the recipient does not send back an acknowledgment (ACK). The sending phase includes queuing steganograms from the application layer, and upon cover data arrival, an IS is inserted for retransmission identification. In the receiving phase, the receiver identifies IS-containing segments, withholding ACK for RSTEG requests and acknowledging non-RSTEG data. The crucial retransmission phase involves creating counters for marked segments; if triggered, retransmission occurs with a steganogram in place or unchanged payload based on the counter status. The experimentation involves a client/server architecture with a basic TCP implementation relying on Retransmission Time Outs (RTO), incorporating the functionalities of RSTEG and by implementing the compensating algorithm to increase the stealthiness of RSTEG.",
                "paper_link": "https://www.semanticscholar.org/paper/1ea8402112e0a2372edd5d64d1b0fddf47d4d1cf"
            },
            {
                "title": "Tunescope: Engaging novices to computational thinking through music",
                "abstract": "To accelerate the adoption of computational thinking (CT), we have developed TuneScope, an online platform for introducing novices to programming in the context of music. TuneScope combines a sound analysis & synthesis tool with Snap!, a computing language developed at the University of California, Berkeley. This demo explores CT concepts such as decomposition, patterns, abstraction, and algorithms in TuneScope while also exploring the creation of four cascading musical components from (1) sequences of notes, (2) musical chords, (3) sampled sounds, and (4) synthesized sounds. The challenge is to design activities that include authentic music learning as well as genuine computational thinking. In this demo, we show concepts around sequence (the order in which musical notes appear in time; and the order of statements in a computer program) and repetition (includes repeats as well as the structure of melodies; and computing loops and recursion). The instructional activities in this demo have been piloted three times in an associated course at the University of Virginia. Data collected from the course suggest a positive effect on both the understanding of CT concepts and the comprehension of music. More detail on TuneScope can be found at https://maketolearn.org/tunescope/.",
                "paper_link": "https://www.semanticscholar.org/paper/6cbe00b323a718a3ce97e08106435896dff3b577"
            },
            {
                "title": "Tracking colliding cells",
                "abstract": "Leukocyte motion represents an important component in the innate immune response to infection. Intravital microscopy is a powerful tool as it enables in vivo imaging of leukocyte motion. Under inflammatory conditions, leukocytes may exhibit various motion behaviors, such as flowing, rolling, and adhering. With many leukocytes moving at a wide range of speeds, collisions occur. These collisions result in abrupt changes in the motion and appearance of leukocytes. Manual analysis is tedious, error prone, time consuming, and could introduce technician-related bias. Automatic tracking is also challenging due to the noise inherent in in vivo images and abrupt changes in motion and appearance due to collision. This paper presents a method to automatically track multiple cells undergoing collisions by modeling the appearance and motion for each collision state and testing collision hypotheses of possible transitions between states. The tracking results are demonstrated using in vivo intravital microscopy image sequences. We demonstrate that 1) 71% of colliding cells are correctly tracked; (2) the improvement of the proposed method is enhanced when the duration of collision increases; and (3) given good detection results, the proposed method can correctly track 88% of colliding cells. The method minimizes the tracking failures under collisions and, therefore, allows more robust analysis in the study of leukocyte behaviors responding to inflammatory conditions.",
                "paper_link": "https://www.semanticscholar.org/paper/c3323dcbae4e96a7929770a651a1b94e34ac4052"
            },
            {
                "title": "Segmentation of vessels cluttered with cells using a physics based model",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/e290eb1841f7eb194342d30f637f14b51d79a899"
            },
            {
                "title": "Pathophysiological Responses to Bloodstream Infection in Critically Ill Transplant Recipients Compared With Non-Transplant Recipients",
                "abstract": "BACKGROUND\nIdentification of bloodstream infection (BSI) in transplant recipients may be difficult due to immunosuppression. Accordingly, we aimed to compare responses to BSI in critically ill transplant and non-transplant recipients and to modify systemic inflammatory response syndrome (SIRS) criteria for transplant recipients.\n\n\nMETHODS\nWe analyzed univariate risks and developed multivariable models of BSI with 27 clinical variables from adult intensive care unit (ICU) patients at the University of Virginia (UVA) and at the University of Pittsburgh (Pitt). We used Bayesian inference to adjust SIRS criteria for transplant recipients.\n\n\nRESULTS\nWe analyzed 38.7 million hourly measurements from 41,725 patients at UVA, including 1,897 transplant recipients with 193 episodes of BSI, and 53,608 patients at Pitt, including 1,614 transplant recipients with 768 episodes of BSI. The univariate responses to BSI were comparable in transplant and non-transplant recipients. The area under the receiver operating characteristic curve (AUC) was 0.82 (95% confidence interval [CI], 0.80-0.83) for the model using all UVA patient data and 0.80 (95% CI, 0.76-0.83) when using only transplant recipient data. The UVA all-patient model had an AUC of 0.77 (95%CI, 0.76-0.79) in non-transplant recipients and 0.75 (95% CI, 0.71-0.79) in transplant recipients at Pitt. The relative importance of the 27 predictors was similar in transplant and non-transplant models. An upper temperature of 37.5\u00b0C in SIRS criteria improved reclassification performance in transplant recipients.\n\n\nCONCLUSION\nCritically ill transplant and non-transplant recipients had similar responses to BSI. An upper temperature of 37.5\u00b0C in SIRS criteria improved BSI screening in transplant recipients.",
                "paper_link": "https://www.semanticscholar.org/paper/65c012b25b435c35298e4699c29fe041115f3e6e"
            },
            {
                "title": "Does Musical Context Improve Computational Thinking Skills?",
                "abstract": "The Make-To-Learn Lab at the University of Virginia developed TuneScope, a tool to facilitate computational thinking (CT) through music, to help democratize computer science education. Built upon the existing framework of Snap! (University of California, Berkeley), TuneScope leverages sound analysis, design, and music composition to engage novices with CT fundamentals. Existing research shows the benefits of using familiar contexts to teach CT, and TuneScope builds upon this with musical contexts. We have designed a course centered around the use of TuneScope to teach fundamentals of CT through music. In this paper, we investigate how students use TuneScope to develop sequential melodies, build chords, and sample recorded sounds while simultaneously learning fundamental programming principles such as algorithm design and abstraction. Using sentiment analysis, we demonstrate that students have had a largely positive experience in learning and grasping CT fundamentals throughout a diverse curriculum. As of today, the total enrollment of the course consists of over 45% female students with a 100% retention rate. TuneScope has also been accepted as an official Snap! library to be used by thousands of Snap! users worldwide.",
                "paper_link": "https://www.semanticscholar.org/paper/b511256fe6b202d15f9b899833fa4f808251ac91"
            },
            {
                "title": "Extracting Features for Computational Thinking from Block-Based Code",
                "abstract": "To support undertrained instructors in introductory computer science classes, we proposed an automated evaluator (autograder) for Snap! , a block-based programming language whose colorful visual interface is more beginner-friendly. Our approach is not only novel in working natively on a non-textual language but also in its assessment of the computational thinking (CT) reflected in the structure of a student\u2019s submission rather than the accuracy or run-time of its execution. This relies on assessing demonstrated knowledge of abstraction and iteration from an XML tree representation of a student\u2019s Snap! program. Approaches supported by literature involve clustering trees with similar structures together; however, methods such as path matching were too generalized and inadequate at reflecting specific CT elements. To this end, we explore how to tailor our feature extraction to capture such elements, including consecutive repetition and encapsulation of functional blocks. Unlike proprietary autograders, our approach integrates the academic community into the research and development of the optimal feature embedding of Snap! programs; thus, we present both successful and unsuccessful endeavors to inform replications of this work. We also highlight avenues for feature tuning and scalability of the autograding model to larger, more diverse classrooms.",
                "paper_link": "https://www.semanticscholar.org/paper/310ed497ef5ad4af90fac9f4134ce8121cea5d71"
            },
            {
                "title": "Reflection:\u201cTwenty Things to Do With a Computer\u201d Revisited",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            }
        ]
    },
    {
        "Professor": "Angela Orebaugh",
        "Papers": []
    },
    {
        "Professor": "Yanjun Qi",
        "Papers": [
            {
                "title": "Opportunities and obstacles for deep learning in biology and medicine",
                "abstract": "Deep learning, which describes a class of machine learning algorithms, has recently showed impressive results across a variety of domains. Biology and medicine are data rich, but the data are complex and often ill-understood. Problems of this nature may be particularly well-suited to deep learning techniques. We examine applications of deep learning to a variety of biomedical problems -- patient classification, fundamental biological processes, and treatment of patients -- to predict whether deep learning will transform these tasks or if the biomedical sphere poses unique challenges. We find that deep learning has yet to revolutionize or definitively resolve any of these problems, but promising advances have been made on the prior state of the art. Even when improvement over a previous baseline has been modest, we have seen signs that deep learning methods may speed or aid human investigation. More work is needed to address concerns related to interpretability and how to best model each problem. Furthermore, the limited amount of labeled data for training presents problems in some domains, as can legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning powering changes at the bench and bedside with the potential to transform several areas of biology and medicine.",
                "paper_link": "https://www.semanticscholar.org/paper/dc0c84b7c5e6521216da789f8171544709120cf0"
            },
            {
                "title": "Feature squeezing: Detecting adversarial examples in deep neural networks",
                "abstract": "Although deep neural networks (DNNs) have achieved great success in many tasks, they can often be fooled by \\emph{adversarial examples} that are generated by adding small but purposeful distortions to natural examples. Previous studies to defend against adversarial examples mostly focused on refining the DNN models, but have either shown limited success or required expensive computation. We propose a new strategy, \\emph{feature squeezing}, that can be used to harden DNN models by detecting adversarial examples. Feature squeezing reduces the search space available to an adversary by coalescing samples that correspond to many different feature vectors in the original space into a single sample. By comparing a DNN model's prediction on the original input with that on squeezed inputs, feature squeezing detects adversarial examples with high accuracy and few false positives. This paper explores two feature squeezing methods: reducing the color bit depth of each pixel and spatial smoothing. These simple strategies are inexpensive and complementary to other defenses, and can be combined in a joint detection framework to achieve high detection rates against state-of-the-art attacks.",
                "paper_link": "https://www.semanticscholar.org/paper/9fec45e1ff97ffb0e0cf9f039e39b46043430301"
            },
            {
                "title": "Random forest for bioinformatics",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/638342beccc1922285c81499e3e67fb65b32d6e8"
            },
            {
                "title": "Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp",
                "abstract": "While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack\u2019s modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness.TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at https://github.com/QData/TextAttack.",
                "paper_link": "https://www.semanticscholar.org/paper/c9b56cb026a38e39bb0228faac57accd6f65e6f7"
            },
            {
                "title": "Black-box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers",
                "abstract": "Although various techniques have been proposed to generate adversarial samples for white-box attacks on text, little attention has been paid to a black-box attack, which is a more realistic scenario. In this paper, we present a novel algorithm, DeepWordBug, to effectively generate small text perturbations in a black-box setting that forces a deep-learning classifier to misclassify a text input. We develop novel scoring strategies to find the most important words to modify such that the deep classifier makes a wrong prediction. Simple character-level transformations are applied to the highest-ranked words in order to minimize the edit distance of the perturbation. We evaluated DeepWordBug on two real-world text datasets: Enron spam emails and IMDB movie reviews. Our experimental results indicate that DeepWordBug can reduce the classification accuracy from 99% to 40% on Enron and from 87% to 26% on IMDB. Our results strongly demonstrate that the generated adversarial sequences from a deep-learning model can similarly evade other deep models.",
                "paper_link": "https://www.semanticscholar.org/paper/fa12574c228542151ccd7d4e3f42cc4896cd274a"
            },
            {
                "title": "Automatically evading classifiers",
                "abstract": "Machine learning is widely used to develop classifiers for security tasks. However, the robustness of these methods against motivated adversaries is uncertain. In this work, we propose a generic method to evaluate the robustness of classifiers under attack. The key idea is to stochastically manipulate a malicious sample to find a variant that preserves the malicious behavior but is classified as benign by the classifier. We present a general approach to search for evasive variants and report on results from experiments using our techniques against two PDF malware classifiers, PDFrate and Hidost. Our method is able to automatically find evasive variants for both classifiers for all of the 500 malicious seeds in our study. Our results suggest a general method for evaluating classifiers used in security applications, and raise serious doubts about the effectiveness of classifiers based on superficial features in the presence of adversaries.",
                "paper_link": "https://www.semanticscholar.org/paper/a0833bf06cadb612a2600f4c4442d003d521d78e"
            },
            {
                "title": "Evaluation of different biological data and computational classification methods for use in protein interaction prediction",
                "abstract": "Protein\u2013protein interactions play a key role in many biological systems. High\u2010throughput methods can directly detect the set of interacting proteins in yeast, but the results are often incomplete and exhibit high false\u2010positive and false\u2010negative rates. Recently, many different research groups independently suggested using supervised learning methods to integrate direct and indirect biological data sources for the protein interaction prediction task. However, the data sources, approaches, and implementations varied. Furthermore, the protein interaction prediction task itself can be subdivided into prediction of (1) physical interaction, (2) co\u2010complex relationship, and (3) pathway co\u2010membership. To investigate systematically the utility of different data sources and the way the data is encoded as features for predicting each of these types of protein interactions, we assembled a large set of biological features and varied their encoding for use in each of the three prediction tasks. Six different classifiers were used to assess the accuracy in predicting interactions, Random Forest (RF), RF similarity\u2010based k\u2010Nearest\u2010Neighbor, Na\u00efve Bayes, Decision Tree, Logistic Regression, and Support Vector Machine. For all classifiers, the three prediction tasks had different success rates, and co\u2010complex prediction appears to be an easier task than the other two. Independently of prediction task, however, the RF classifier consistently ranked as one of the top two classifiers for all combinations of feature sets. Therefore, we used this classifier to study the importance of different biological datasets. First, we used the splitting function of the RF tree structure, the Gini index, to estimate feature importance. Second, we determined classification accuracy when only the top\u2010ranking features were used as an input in the classifier. We find that the importance of different features depends on the specific prediction task and the way they are encoded. Strikingly, gene expression is consistently the most important feature for all three prediction tasks, while the protein interactions identified using the yeast\u20102\u2010hybrid system were not among the top\u2010ranking features under any condition. Proteins 2006. \u00a9 2006 Wiley\u2010Liss, Inc.",
                "paper_link": "https://www.semanticscholar.org/paper/2c98473604b67f8cd7b4820134203473671e0705"
            },
            {
                "title": "Systems and methods for semi-supervised relationship extraction",
                "abstract": "Question classification, an important component of question answering systems, has a direct impact on the answer extraction accuracy. In this paper, a question classification method is proposed by combined the question feature extracting of term relevance with semi-supervised classification. In detail, the method extracts structure terms in interrogative sentences as the feature space through statistical means, and calculates the relevance among terms by literal similarity method, besides, feature vectors of question classification are obtained by using term similarity relationship to build the questions\u2019 feature value in feature space. And then, utilizing unlabeled samples classify questions with the help of Co-training style and semisupervised learning algorithm. Experimented on 20,000 questions in Yunnan tourism domain, the results show that more remarkable effects have been achieved by adopting the method above. The classification accuracy rate reaches 82.34%, which is higher than the TFIDF feature extraction methods and supervised learning methods by 15.4 percentage points and 1.4 percentage points separately.",
                "paper_link": "https://www.semanticscholar.org/paper/3ba8ffbbdee4e14793dd1399277860d451a63c39"
            },
            {
                "title": "Curriculum labeling: Revisiting pseudo-labeling for semi-supervised learning",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "General multi-label image classification with transformers",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "DeepChrome: deep-learning for predicting gene expression from histone modifications",
                "abstract": "\" deepchrome-eccb \" \u2014 2016/7/8 \u2014 page 1 \u2014 #1 Bioinformatics Abstract Motivation: Histone modifications are among the most important factors that control gene regulation. Computational methods that predict gene expression from histone modification signals are highly desirable for understanding their combinatorial effects in gene regulation. This knowledge can help in developing 'epigenetic drugs' for diseases like cancer. Previous studies for quantifying the relationship between histone modifications and gene expression levels either failed to capture combinatorial effects or relied on multiple methods that separate predictions and combinatorial analysis. This paper develops a unified discriminative framework using a deep convolutional neural network to classify gene expression using histone modification data as input. Our system, called DeepChrome, allows automatic extraction of complex interactions among important features. To simultaneously visualize the combinatorial interactions among histone modifications, we propose a novel optimization-based technique that generates feature pattern maps from the learnt deep model. This provides an intuitive description of underlying epigenetic mechanisms that regulate genes. Results: We show that DeepChrome outperforms state-of-the-art models like Support Vector Machines and Random Forests for gene expression classification task on 56 different cell-types from REMC database. The output of our visualization technique not only validates the previous observations but also allows novel insights about combinatorial interactions among histone modification marks, some of which have recently been observed by experimental studies.",
                "paper_link": "https://www.semanticscholar.org/paper/de8192ad2ef3bedfef0bf6c3646df8c0675568b8"
            },
            {
                "title": "A critical assessment ofMus musculusgene function prediction using integrated genomic evidence",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/230444cd55ea81d4210dd6980a46cc0fa782c330"
            },
            {
                "title": "Random forest similarity for protein-protein interaction prediction from multiple sources",
                "abstract": "One of the most important, but often ignored, parts of any clustering and classification algorithm is the computation of the similarity matrix. This is especially important when integrating high throughput biological data sources because of the high noise rates and the many missing values. In this paper we present a new method to compute such similarities for the task of classifying pairs of proteins as interacting or not. Our method uses direct and indirect information about interaction pairs to constructs a random forest (a collection of decision tress) from a training set. The resulting forest is used to determine the similarity between protein pairs and this similarity is used by a classification algorithm (a modified kNN) to classify protein pairs. Testing the algorithm on yeast data indicates that it is able to improve coverage to 20% of interacting pairs with a false positive rate of 50%. These results compare favorably with all previously suggested methods for this task indicating the importance of robust similarity estimates.",
                "paper_link": "https://www.semanticscholar.org/paper/46b09a6095d02a0f239fb890c13dba7fbceef0bc"
            },
            {
                "title": "Cas9-chromatin binding information enables more accurate CRISPR off-target prediction",
                "abstract": "The CRISPR system has become a powerful biological tool with a wide range of applications. However, improving targeting specificity and accurately predicting potential off-targets remains a significant goal. Here, we introduce a web-based CRISPR/Cas9 Off-target Prediction and Identification Tool (CROP-IT) that performs improved off-target binding and cleavage site predictions. Unlike existing prediction programs that solely use DNA sequence information; CROP-IT integrates whole genome level biological information from existing Cas9 binding and cleavage data sets. Utilizing whole-genome chromatin state information from 125 human cell types further enhances its computational prediction power. Comparative analyses on experimentally validated datasets show that CROP-IT outperforms existing computational algorithms in predicting both Cas9 binding as well as cleavage sites. With a user-friendly web-interface, CROP-IT outputs scored and ranked list of potential off-targets that enables improved guide RNA design and more accurate prediction of Cas9 binding or cleavage sites.",
                "paper_link": "https://www.semanticscholar.org/paper/00c8767bf43b086d26d151c8ff40d4c0a9ba2441"
            },
            {
                "title": "Sentiment classification based on supervised latent n-gram analysis",
                "abstract": "In this paper, we propose an efficient embedding for modeling higher-order (n-gram) phrases that projects the n-grams to low-dimensional latent semantic space, where a classification function can be defined. We utilize a deep neural network to build a unified discriminative framework that allows for estimating the parameters of the latent space as well as the classification function with a bias for the target classification task at hand. We apply the framework to large-scale sentimental classification task. We present comparative evaluation of the proposed method on two (large) benchmark data sets for online product reviews. The proposed method achieves superior performance in comparison to the state of the art.",
                "paper_link": "https://www.semanticscholar.org/paper/73cf7c10f1b7a30089b201d924f73f342b64c2bc"
            },
            {
                "title": "Prediction of interactions between HIV-1 and human proteins by information integration",
                "abstract": "Human immunodeficiency virus-1 (HIV-1) in acquired immune deficiency syndrome (AIDS) relies on human host cell proteins in virtually every aspect of its life cycle. Knowledge of the set of interacting human and viral proteins would greatly contribute to our understanding of the mechanisms of infection and subsequently to the design of new therapeutic approaches. This work is the first attempt to predict the global set of interactions between HIV-1 and human host cellular proteins. We propose a supervised learning framework, where multiple information data sources are utilized, including co-occurrence of functional motifs and their interaction domains and protein classes, gene ontology annotations, posttranslational modifications, tissue distributions and gene expression profiles, topological properties of the human protein in the interaction network and the similarity of HIV-1 proteins to human proteins' known binding partners. We trained and tested a Random Forest (RF) classifier with this extensive feature set. The model's predictions achieved an average Mean Average Precision (MAP) score of 23%. Among the predicted interactions was for example the pair, HIV-1 protein tat and human vitamin D receptor. This interaction had recently been independently validated experimentally. The rank-ordered lists of predicted interacting pairs are a rich source for generating biological hypotheses. Amongst the novel predictions, transcription regulator activity, immune system process and macromolecular complex were the top most significant molecular function, process and cellular compartments, respectively. Supplementary material is available at URL www.cs.cmu.edu/\u00f5znur/hiv/hivPPI.html",
                "paper_link": "https://www.semanticscholar.org/paper/fc933b747ed1be4f3242ea26fb0e89307f31a09b"
            },
            {
                "title": "Recurrent chimeric fusion RNAs in non-cancer tissues and cells",
                "abstract": "Gene fusions and their products (RNA and protein) were once thought to be unique features to cancer. However, chimeric RNAs can also be found in normal cells. Here, we performed, curated and analyzed nearly 300 RNA-Seq libraries covering 30 different non-neoplastic human tissues and cells as well as 15 mouse tissues. A large number of fusion transcripts were found. Most fusions were detected only once, while 291 were seen in more than one sample. We focused on the recurrent fusions and performed RNA and protein level validations on a subset. We characterized these fusions based on various features of the fusions, and their parental genes. They tend to be expressed at higher levels relative to their parental genes than the non-recurrent ones. Over half of the recurrent fusions involve neighboring genes transcribing in the same direction. A few sequence motifs were found enriched close to the fusion junction sites. We performed functional analyses on a few widely expressed fusions, and found that silencing them resulted in dramatic reduction in normal cell growth and/or motility. Most chimeras use canonical splicing sites, thus are likely products of \u2018intergenic splicing\u2019. We also explored the implications of these non-pathological fusions in cancer and in evolution.",
                "paper_link": "https://www.semanticscholar.org/paper/97139f2414d936f4ee3d725ba5722f867f12c506"
            },
            {
                "title": "Deep motif dashboard: visualizing and understanding genomic sequences using deep neural networks",
                "abstract": "Deep neural network (DNN) models have recently obtained state-of-the-art prediction accuracy for the transcription factor binding (TFBS) site classification task. However, it remains unclear how these approaches identify meaningful DNA sequence signals and give insights as to why TFs bind to certain locations. In this paper, we propose a toolkit called the Deep Motif Dashboard (DeMo Dashboard) which provides a suite of visualization strategies to extract motifs, or sequence patterns from deep neural network models for TFBS classification. We demonstrate how to visualize and understand three important DNN models: convolutional, recurrent, and convolutional-recurrent networks. Our first visualization method is finding a test sequence's saliency map which uses first-order derivatives to describe the importance of each nucleotide in making the final prediction. Second, considering recurrent models make predictions in a temporal manner (from one end of a TFBS sequence to the other), we introduce temporal output scores, indicating the prediction score of a model over time for a sequential input. Lastly, a class-specific visualization strategy finds the optimal input sequence for a given TFBS positive class via stochastic gradient optimization. Our experimental results indicate that a convolutional-recurrent architecture performs the best among the three architectures. The visualization techniques indicate that CNN-RNN makes predictions by modeling both motifs as well as dependencies among them.",
                "paper_link": "https://www.semanticscholar.org/paper/b6287a88ceb7f98e52c9142c61432ab91d0f0f19"
            },
            {
                "title": "Protein complex identification by supervised graph local clustering",
                "abstract": "Motivation: Protein complexes integrate multiple gene products to coordinate many biological functions. Given a graph representing pairwise protein interaction data one can search for subgraphs representing protein complexes. Previous methods for performing such search relied on the assumption that complexes form a clique in that graph. While this assumption is true for some complexes, it does not hold for many others. New algorithms are required in order to recover complexes with other types of topological structure. Results: We present an algorithm for inferring protein complexes from weighted interaction graphs. By using graph topological patterns and biological properties as features, we model each complex subgraph by a probabilistic Bayesian network (BN). We use a training set of known complexes to learn the parameters of this BN model. The log-likelihood ratio derived from the BN is then used to score subgraphs in the protein interaction graph and identify new complexes. We applied our method to protein interaction data in yeast. As we show our algorithm achieved a considerable improvement over clique based algorithms in terms of its ability to recover known complexes. We discuss some of the new complexes predicted by our algorithm and determine that they likely represent true complexes. Availability: Matlab implementation is available on the supporting website: www.cs.cmu.edu/~qyj/SuperComplex Contact: zivbj@cs.cmu.edu",
                "paper_link": "https://www.semanticscholar.org/paper/03fe3bc4cfbaf3bd716000e57a22b25102418936"
            },
            {
                "title": "Semi-supervised multi-task learning for predicting interactions between HIV-1 and human proteins",
                "abstract": "Motivation: Protein\u2013protein interactions (PPIs) are critical for virtually every biological function. Recently, researchers suggested to use supervised learning for the task of classifying pairs of proteins as interacting or not. However, its performance is largely restricted by the availability of truly interacting proteins (labeled). Meanwhile, there exists a considerable amount of protein pairs where an association appears between two partners, but not enough experimental evidence to support it as a direct interaction (partially labeled). Results: We propose a semi-supervised multi-task framework for predicting PPIs from not only labeled, but also partially labeled reference sets. The basic idea is to perform multi-task learning on a supervised classification task and a semi-supervised auxiliary task. The supervised classifier trains a multi-layer perceptron network for PPI predictions from labeled examples. The semi-supervised auxiliary task shares network layers of the supervised classifier and trains with partially labeled examples. Semi-supervision could be utilized in multiple ways. We tried three approaches in this article, (i) classification (to distinguish partial positives with negatives); (ii) ranking (to rate partial positive more likely than negatives); (iii) embedding (to make data clusters get similar labels). We applied this framework to improve the identification of interacting pairs between HIV-1 and human proteins. Our method improved upon the state-of-the-art method for this task indicating the benefits of semi-supervised multi-task learning using auxiliary information. Availability: http://www.cs.cmu.edu/\u223cqyj/HIVsemi Contact: qyj@cs.cmu.edu",
                "paper_link": "https://www.semanticscholar.org/paper/b4fd3ce53f4bbb8d2f8c63072dbfa4d8276235f1"
            }
        ]
    },
    {
        "Professor": "Kun Qian",
        "Papers": [
            {
                "title": "Widar3.0: Zero-Effort Cross-Domain Gesture Recognition with Wi-Fi",
                "abstract": "With the development of signal processing technology, the ubiquitous Wi-Fi devices open an unprecedented opportunity to solve the challenging human gesture recognition problem by learning motion representations from wireless signals. Wi-Fi-based gesture recognition systems, although yield good performance on specific data domains, are still practically difficult to be used without explicit adaptation efforts to new domains. Various pioneering approaches have been proposed to resolve this contradiction but extra training efforts are still necessary for either data collection or model re-training when new data domains appear. To advance cross-domain recognition and achieve fully zero-effort recognition, we propose Widar3.0, a Wi-Fi-based zero-effort cross-domain gesture recognition system. The key insight of Widar3.0 is to derive and extract domain-independent features of human gestures at the lower signal level, which represent unique kinetic characteristics of gestures and are irrespective of domains. On this basis, we develop a one-fits-all general model that requires only one-time training but can adapt to different data domains. Experiments on various domain factors (i.e. environments, locations, and orientations of persons) demonstrate the accuracy of 92.7% for in-domain recognition and 82.6%-92.4% for cross-domain recognition without model re-training, outperforming the state-of-the-art solutions.",
                "paper_link": "https://www.semanticscholar.org/paper/db88f53e1037bcc269aa1a5a407a8d72afd7bb55"
            },
            {
                "title": "Widar2.0: Passive human tracking with a single Wi-Fi link",
                "abstract": "This paper presents Widar2.0, the first WiFi-based system that enables passive human localization and tracking using a single link on commodity off-the-shelf devices. Previous works based on either specialized or commercial hardware all require multiple links, preventing their wide adoption in scenarios like homes where typically only one single AP is installed. The key insight underlying Widar2.0 to circumvent the use of multiple links is to leverage multi-dimensional signal parameters from one single link. To this end, we build a unified model accounting for Angle-of-Arrival, Time-of-Flight, and Doppler shifts together and devise an efficient algorithm for their joint estimation. We then design a pipeline to translate the erroneous raw parameters into precise locations, which first finds parameters corresponding to the reflections of interests, then refines range estimates, and ultimately outputs target locations. Our implementation and evaluation on commodity WiFi devices demonstrate that Widar2.0 achieves better or comparable performance to state-of-the-art localization systems, which either use specialized hardwares or require 2 to 40 Wi-Fi links.",
                "paper_link": "https://www.semanticscholar.org/paper/957e4c03e7846a1f8670b250f5f9661c91dfcb75"
            },
            {
                "title": "Widar: Decimeter-level passive tracking via velocity monitoring with commodity Wi-Fi",
                "abstract": "Various pioneering approaches have been proposed for Wi-Fi-based sensing, which usually employ learning-based techniques to seek appropriate statistical features, yet do not support precise tracking without prior training. Thus to advance passive sensing, the ability to track fine-grained human mobility information acts as a key enabler. In this paper, we propose Widar, a Wi-Fi-based tracking system that simultaneously estimates a human's moving velocity (both speed and direction) and location at a decimeter level. Instead of applying statistical learning techniques, Widar builds a theoretical model that geometrically quantifies the relationships between CSI dynamics and the user's location and velocity. On this basis, we propose novel techniques to identify frequency components related to human motion from noisy CSI readings and then derive a user's location in addition to velocity. We implement Widar on commercial Wi-Fi devices and validate its performance in real environments. Our results show that Widar achieves decimeter-level accuracy, with a median location error of 25 cm given initial positions and 38 cm without them and a median relative velocity error of 13%.",
                "paper_link": "https://www.semanticscholar.org/paper/46248ccea6f87c91571148bd0be86a7160df0de1"
            },
            {
                "title": "PADS: Passive detection of moving targets with dynamic speed using PHY layer information",
                "abstract": "Device-free passive detection is an emerging technology to detect whether there exists any moving entities in the area of interests without attaching any device to them. It is an essential primitive for a broad range of applications including intrusion detection for safety precautions, patient monitoring in hospitals, child and elder care at home, etc. Despite of the prevalent signal feature Received Signal Strength (RSS), most robust and reliable solutions resort to finer-grained channel descriptor at physical layer, e.g., the Channel State Information (CSI) in the 802.11n standard. Among a large body of emerging techniques, however, few of them have explored full potentials of CSI for human detection. Moreover, space diversity supported by nowadays popular multi-antenna systems are not investigated to the comparable extent as frequency diversity. In this paper, we propose a novel scheme for device-free PAssive Detection of moving humans with dynamic Speed (PADS). Both amplitude and phase information of CSI are extracted and shaped into sensitive metrics for target detection; and CSI across multi-antennas in MIMO systems are further exploited to improve the detection accuracy and robustness. We prototype PADS on commercial WiFi devices and experiment results in different scenarios demonstrate that PADS achieves great performance improvement in spite of dynamic human movements.",
                "paper_link": "https://www.semanticscholar.org/paper/beceebfe257c237f4e62b0ec4dfb0c52d71b8814"
            },
            {
                "title": "Inferring motion direction using commodity Wi-Fi for interactive exergames",
                "abstract": "In-air interaction acts as a key enabler for ambient intelligence and augmented reality. As an increasing popular example, exergames, and the alike gesture recognition applications, have attracted extensive research in designing accurate, pervasive and low-cost user interfaces. Recent advances in wireless sensing show promise for a ubiquitous gesture-based interaction interface with Wi-Fi. In this work, we extract complete information of motion-induced Doppler shifts with only commodity Wi-Fi. The key insight is to harness antenna diversity to carefully eliminate random phase shifts while retaining relevant Doppler shifts. We further correlate Doppler shifts with motion directions, and propose a light-weight pipeline to detect, segment, and recognize motions without training. On this basis, we present WiDance, a Wi-Fi-based user interface, which we utilize to design and prototype a contactless dance-pad exergame. Experimental results in typical indoor environment demonstrate a superior performance with an accuracy of 92%, remarkably outperforming prior approaches.",
                "paper_link": "https://www.semanticscholar.org/paper/7525e66405099bed4eca4d6f238f68ba37fc8262"
            },
            {
                "title": "PhaseU: Real-time LOS identification with WiFi",
                "abstract": "WiFi technology has fostered numerous mobile computing applications, such as adaptive communication, finegrained localization, gesture recognition, etc., which often achieve better performance or rely on the availability of Line-Of-Sight (LOS) signal propagation. Thus the awareness of LOS and Non-Line-Of-Sight (NLOS) plays as a key enabler for them. Realtime LOS identification on commodity WiFi devices, however, is challenging due to limited bandwidth of WiFi and resulting coarse multipath resolution. In this work, we explore and exploit the phase feature of PHY layer information, harnessing both space diversity with antenna elements and frequency diversity with OFDM subcarriers. On this basis, we propose PhaseU, a real-time LOS identification scheme that works in both static and mobile scenarios on commodity WiFi infrastructure. Experimental results in various indoor scenarios demonstrate that PhaseU consistently outperforms previous approaches, achieving overall LOS and NLOS detection rates of 94.35% and 94.19% in static cases and both higher than 80% in mobile contexts. Furthermore, PhaseU achieves real-time capability with millisecond-level delay for a connected AP and 1-second delay for unconnected APs, which is far beyond existing approaches.",
                "paper_link": "https://www.semanticscholar.org/paper/4354724b5e464906a2dc0a11184e446c36db4fff"
            },
            {
                "title": "Robust multimodal vehicle detection in foggy weather using complementary lidar and radar signals",
                "abstract": "Vehicle detection with visual sensors like lidar and camera is one of the critical functions enabling autonomous driving. While they generate fine-grained point clouds or high-resolution images with rich information in good weather conditions, they fail in adverse weather (e.g., fog) where opaque particles distort lights and significantly reduce visibility. Thus, existing methods relying on lidar or camera experience significant performance degradation in rare but critical adverse weather conditions. To remedy this, we resort to exploiting complementary radar, which is less impacted by adverse weather and becomes prevalent on vehicles. In this paper, we present Multimodal Vehicle Detection Network (MVDNet), a two-stage deep fusion detector, which first generates proposals from two sensors and then fuses region-wise features between multimodal sensor streams to improve final detection results. To evaluate MVDNet, we create a procedurally generated training dataset based on the collected raw lidar and radar signals from the open-source Oxford Radar Robotcar. We show that the proposed MVDNet surpasses other state-of-the-art methods, notably in terms of Average Precision (AP), especially in adverse weather conditions. The code and data are available at https://github.com/qiank10/MVDNet.",
                "paper_link": "https://www.semanticscholar.org/paper/03a3f5159394856aac1b7d481e576000818d452d"
            },
            {
                "title": "Enabling contactless detection of moving humans with dynamic speeds using CSI",
                "abstract": "Device-free passive detection is an emerging technology to detect whether there exist any moving entities in the areas of interest without attaching any device to them. It is an essential primitive for a broad range of applications including intrusion detection for safety precautions, patient monitoring in hospitals, child and elder care at home, and so forth. Despite the prevalent signal feature Received Signal Strength (RSS), most robust and reliable solutions resort to a finer-grained channel descriptor at the physical layer, e.g., the Channel State Information (CSI) in the 802.11n standard. Among a large body of emerging techniques, however, few of them have explored the full potential of CSI for human detection. Moreover, space diversity supported by nowadays popular multiantenna systems are not investigated to a comparable extent as frequency diversity. In this article, we propose a novel scheme for device-free PAssive Detection of moving humans with dynamic Speed (PADS). Both full information (amplitude and phase) of CSI and space diversity across multiantennas in MIMO systems are exploited to extract and shape sensitive metrics for accuracy and robust target detection. We prototype PADS on commercial WiFi devices, and experiment results in different scenarios demonstrate that PADS achieves great performance improvement in spite of dynamic human movements.",
                "paper_link": "https://www.semanticscholar.org/paper/2710115227165ec823535764968503324a810576"
            },
            {
                "title": "Acousticcardiogram: Monitoring heartbeats using acoustic signals on smart devices",
                "abstract": "Vital signs such as heart rate and heartbeat interval are currently measured by electrocardiograms (ECG) or wearable physiological monitors. These techniques either require contact with the patient's skin or are usually uncomfortable to wear, rendering them too expensive and user-unfriendly for daily monitoring. In this paper, we propose a new noninvasive technology to generate an Acousticcardiogram (ACG) that precisely monitors heartbeats using inaudible acoustic signals. ACG uses only commodity microphones and speakers commonly equipped on ubiquitous off-the-shelf devices, such as smartphones and laptops. By transmitting an acoustic signal and analyzing its reflections off human body, ACG is capable of recognizing the heart rate as well as heartbeat rhythm. We employ frequency-modulated sound signals to separate reflection of heart from that of background motions and breath, and continuously track the phase changes of the acoustic data. To translate these acoustic data into heart and breath rates, we leverage the dual microphone design on COTS mobile devices to suppress direct echo from speaker to microphones, identify heart rate in frequency domain, and adopt an advanced algorithm to extract individual heartbeats. We implement ACG on commercial devices and validate its performance in real environments. Experimental results demonstrate ACG monitors user's heartbeat accurately, with median heart rate estimation error of 0.6 beat per minute (bpm), and median heartbeat interval estimation error of 19 ms.",
                "paper_link": "https://www.semanticscholar.org/paper/330f270de5ac8784fc86a961d571bd53ed368428"
            },
            {
                "title": "3D point cloud generation with millimeter-wave radar",
                "abstract": "Emerging autonomous driving systems require reliable perception of 3D surroundings. Unfortunately, current mainstream perception modalities, i.e., camera and Lidar, are vulnerable under challenging lighting and weather conditions. On the other hand, despite their all-weather operations, today's vehicle Radars are limited to location and speed detection. In this paper, we introduce MILLIPOINT, a practical system that advances the Radar sensing capability to generate 3D point clouds. The key design principle of MILLIPOINT lies in enabling synthetic aperture radar (SAR) imaging on low-cost commodity vehicle Radars. To this end, MILLIPOINT models the relation between signal variations and Radar movement, and enables self-tracking of Radar at wavelength-scale precision, thus realize coherent spatial sampling. Furthermore, MILLIPOINT solves the unique problem of specular reflection, by properly focusing on the targets with post-imaging processing. It also exploits the Radar's built-in antenna array to estimate the height of reflecting points, and eventually generate 3D point clouds. We have implemented MILLIPOINT on a commodity vehicle Radar. Our evaluation results show that MILLIPOINT effectively combats motion errors and specular reflections, and can construct 3D point clouds with much higher density and resolution compared with the existing vehicle Radar solutions.",
                "paper_link": "https://www.semanticscholar.org/paper/f76d6652a06a704b494b8c7dfbeebeeadb9eaa4d"
            },
            {
                "title": "M-cube: A millimeter-wave massive MIMO software radio",
                "abstract": "Millimeter-wave (mmWave) technologies represent a cornerstone for emerging wireless network infrastructure, and for RF sensing systems in security, health, and automotive domains. Through a MIMO array of phased arrays with hundreds of antenna elements, mmWave can boost wireless bit-rates to 100+ Gbps, and potentially achieve near-vision sensing resolution. However, the lack of an experimental platform has been impeding research in this field. This paper fills the gap with M3 (M-Cube), the first mmWave massive MIMO software radio. M3 features a fully reconfigurable array of phased arrays, with up to 8 RF chains and 288 antenna elements. Despite the orders of magnitude larger antenna arrays, its cost is orders of magnitude lower, even when compared with state-of-the-art single RF chain mmWave software radios. The key design principle behind M3 is to hijack a low-cost commodity 802.11ad radio, separate the control path and data path inside, regenerate the phased array control signals, and recreate the data signals using a programmable baseband. Extensive experiments have demonstrated the effectiveness of the M3 design, and its usefulness for research in mmWave massive MIMO communication and sensing.",
                "paper_link": "https://www.semanticscholar.org/paper/f75d48032b74d6f4426a0658dd99a878c101ff4a"
            },
            {
                "title": "Indoor acoustic localization: A survey",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/795fa906c4cf3f124d5c282622363543176da969"
            },
            {
                "title": "Decimeter level passive tracking with WiFi",
                "abstract": "Pioneer approaches for WiFi-based sensing usually employ learning-based techniques to seek appropriate statistical features, but do not support precise tracking without prior training. Thus to advance passive sensing, the ability to track fine-grained human mobility information acts as a key enabler. In this paper, we proposed Widar, a WiFi-based tracking system that simultaneously estimates human's moving velocity (both speed and direction) and locations at decimeter level. Instead of applying statistical learning techniques, Widar builds a theoretical model that geometrically quantifies the relationships between CSI dynamics and user's location and velocity. On this basis, we propose novel techniques to identify PLCR components related to human movements from noisy CSIs and then derive a user's locations in addition to velocities. We implement Widar on commercial WiFi devices and validate its performance in real environments. Our results show that Widar achieves decimeter-level accuracy, with a median location error of 24cm given initial positions and 36cm without them and a mean relative velocity error of 11%.",
                "paper_link": "https://www.semanticscholar.org/paper/e561b35471265ea7619fb16f6e1feec9a530cbca"
            },
            {
                "title": "RoS: passive smart surface for roadside-to-vehicle communication",
                "abstract": "Modern autonomous vehicles are commonly instrumented with radars for all-weather perception. Yet the radar functionality is limited to identifying the positions of reflectors in the environment. In this paper, we investigate the feasibility of smartening transportation infrastructure for the purpose of conveying richer information to automotive radars. We propose RoS, a passive PCB-fabricated smart surface which can be reconfigured to embed digital bits, and inform the radar much like visual road signs do to cameras. We design the RoS signage to act as a retrodirective reflector which can reflect signals back to the radar from wide viewing angles. We further introduce a spatial encoding scheme, which piggybacks information in the reflected analog signals based on the geometrical layout of the retroreflective elements. Our prototype fabrication and experimentation verifies the effectiveness of RoS as an RF ''barcode'' which is readable by radar in practical transportation environment.",
                "paper_link": "https://www.semanticscholar.org/paper/51577c5238e47d3bde57ae0de2f4cdb78668e0b4"
            },
            {
                "title": "ivr: Integrated vision and radio localization with zero human effort",
                "abstract": "Smartphone localization is essential to a wide range of applications in shopping malls, museums, office buildings, and other public places. Existing solutions relying on radio fingerprints and/or inertial sensors suffer from large location errors and considerable deployment efforts. We observe an opportunity in the recent trend of increasing numbers of security surveillance cameras installed in indoor spaces to overcome these limitations and revisit the problem of smartphone localization with a fresh perspective. However, fusing vision-based and radio-based systems is non-trivial due to the absence of absolute location, incorrespondence of identification and looseness of sensor fusion. This study proposes iVR, an integrated vision and radio localization system that achieves sub-meter accuracy with indoor semantic maps automatically generated from only two surveillance cameras, superior to precedent systems that require manual map construction or plentiful captured images. iVR employs a particle filter to fuse raw estimates from multiple systems, including vision, radio, and inertial sensor systems. By doing so, iVR outputs enhanced accuracy with zero start-up costs, while overcoming the respective drawbacks of each individual sub-system. We implement iVR on commodity smartphones and validate its performance in five different scenarios. The results show that iVR achieves a remarkable localization accuracy of 0.7m, outperforming the state-of-the-art systems by > 70%. CCS Concepts: \u2022",
                "paper_link": "https://www.semanticscholar.org/paper/45c66a2d250e3076ab25bacfc34ae512a29e02bb"
            },
            {
                "title": "MilliMirror: 3D printed reflecting surface for millimeter-wave coverage expansion",
                "abstract": "Next generation wireless networks embrace mmWave technology for its high capacity. Yet, mmWave radios bear a fundamental coverage limitation due to the high directionality and propagation artifacts. In this paper, we explore an economical paradigm based on 3D printing technology for mmWave coverage expansion. We propose MilliMirror, a fully passive metasurface, which can reshape and resteer mmWave beams to anomalous directions to illuminate the coverage blind spots. We develop a closed-form model to efficiently synthesize the MilliMirror design with thousands of unit elements and across a wide frequency band. We further develop an economical process based on 3D printing and metal deposition to fabricate MilliMirror. Our field test results show that MilliMirror can effectively fill the coverage holes and operate transparently to the standard mmWave beam management protocols.",
                "paper_link": "https://www.semanticscholar.org/paper/d4ea8492ae72e864ef49453e16d4b868b8d95e20"
            },
            {
                "title": "GaitSense: Towards ubiquitous gait-based human identification with Wi-Fi",
                "abstract": "\n Gait, the walking manner of a person, has been perceived as a physical and behavioral trait for human identification. Compared with cameras and wearable sensors, Wi-Fi-based gait recognition is more attractive because Wi-Fi infrastructure is almost available everywhere and is able to sense passively without the requirement of on-body devices. However, existing Wi-Fi sensing approaches impose strong assumptions of fixed user walking trajectories, sufficient training data, and identification of already known users. In this article, we present\n GaitSense\n , a Wi-Fi-based human identification system, to overcome the above unrealistic assumptions. To deal with various walking trajectories and speeds,\n GaitSense\n first extracts target specific features that best characterize gait patterns and applies novel normalization algorithms to eliminate gait irrelevant perturbation in signals. On this basis,\n GaitSense\n reduces the training efforts in new deployment scenarios by transfer learning and data augmentation techniques.\n GaitSense\n also enables a distinct feature of illegal user identification by anomaly detection, making the system readily available for real-world deployment. Our implementation and evaluation with commodity Wi-Fi devices demonstrate a consistent identification accuracy across various deployment scenarios with little training samples, pushing the limit of gait recognition with Wi-Fi signals.\n",
                "paper_link": "https://www.semanticscholar.org/paper/aea548117e1e09d05258a5e76e72b318b65950ba"
            },
            {
                "title": "Enabling phased array signal processing for mobile WiFi devices",
                "abstract": "Modern mobile devices are equipped with multiple antennas, which brings various wireless sensing applications such as accurate localization, contactless human detection, and wireless human-device interaction. A key enabler for these applications is phased array signal processing, especially Angle of Arrival (AoA) estimation. However, accurate AoA estimation on commodity devices is non-trivial due to limited number of antennas and uncertain phase offsets. Previous works either rely on elaborate calibration or involve contrived human interactions. In this paper, we aim to enable practical AoA measurements on commodity off-the-shelf (COTS) mobile devices. The key insight is to involve users\u2019 natural rotation to formulate a virtual spatial-temporal antenna array and conduce a relative incident signal of measurements at two orientations. Then by taking the differential phase, it is feasible to remove the phase offsets and derive the accurate AoA of the equivalent incoming signal, while the rotation angle can also be captured by built-in inertial sensors. On this basis, we propose Differential MUSIC (<italic>D-MUSIC</italic>), a relative form of the standard MUSIC algorithm that eliminates the unknown phase offsets and achieves accurate AoA estimation on COTS mobile devices with only one rotation. We further extend <italic>D-MUSIC</italic> to 3-D space, integrate extra measurements during rotations for higher estimation accuracy, and fortify it in multipath-rich scenarios. We prototype <italic>D-MUSIC</italic> on commodity WiFi infrastructure and evaluate it in typical indoor environments. Experimental results demonstrate a superior performance with average AoA estimation errors of 13<inline-formula> <tex-math notation=\"LaTeX\">${}^{\\circ}$</tex-math><alternatives><inline-graphic xlink:href=\"qian-ieq1-2778155.gif\"/> </alternatives></inline-formula> with only three measurements and 5<inline-formula><tex-math notation=\"LaTeX\"> ${}^{\\circ}$</tex-math><alternatives><inline-graphic xlink:href=\"qian-ieq2-2778155.gif\"/></alternatives> </inline-formula> with at most 10 measurements. Requiring no modifications or calibration, <italic>D-MUSIC</italic> is envisioned as a promising scheme for practical AoA estimation on COTS mobile devices.",
                "paper_link": "https://www.semanticscholar.org/paper/b5682c000f194958d8cd35d69ff6e4701086a94c"
            },
            {
                "title": "Passenger demand prediction with cellular footprints",
                "abstract": "Accurate forecast of citywide passenger demand helps online car-hailing service providers to better schedule driver supplies. Previous research either uses only passenger order history and fails to capture the deep dependency of passenger demand, or is restricted on grid region partition that loses physical context. Recent advance in mobile traffic analysis has fostered understanding of city functions. In this article, we propose FlowFlexDP, a demand prediction model that integrates regional crowd flow and applies to flexible region partition. Analysis on a cellular dataset covering 1.5 million users in a major city in China reveals strong correlation between passenger demand and crowd flow. FlowFlexDP extracts both order history and crowd flow from cellular data, and adopts Graph Convolutional Neural Network to adapt prediction for regions of arbitrary shapes and sizes in a city. Evaluation on a large scale data set of 6 online car-hailing applications from cellular data shows that FlowFlexDP accurately predicts passenger demand and outperforms the state-of-the-art demand prediction methods.",
                "paper_link": "https://www.semanticscholar.org/paper/77050665418a3d1e8455db8304a90e1c8276fb2d"
            },
            {
                "title": "Gaitid: Robust wi-fi based gait recognition",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/8a9e08114808c5ca714f0672b0e6a2a3c87bee05"
            }
        ]
    },
    {
        "Professor": "Daniel J. Rosenkrantz",
        "Papers": [
            {
                "title": "An analysis of several heuristics for the traveling salesman problem",
                "abstract": "Several polynomial time algorithms finding \u201cgood,\u201d but not necessarily optimal, tours for the traveling salesman problem are considered. We measure the closeness of a tour by the ratio of the obtained tour length to the minimal tour length. For the nearest neighbor method, we show the ratio is bounded above by a logarithmic function of the number of nodes. We also provide a logarithmic lower bound on the worst case. A class of approximation methods we call insertion methods are studied, and these are also shown to have a logarithmic upper bound. For two specific insertion methods, which we call nearest insertion and cheapest insertion, the ratio is shown to have a constant upper bound of 2, and examples are provided that come arbitrarily close to this upper bound. It is also shown that for any n\u22658, there are traveling salesman problems with n nodes having tours which cannot be improved by making n/4 edge changes, but for which the ratio is 2(1\u22121/n).",
                "paper_link": "https://www.semanticscholar.org/paper/ac5e93895ab03bf47070dab04f62f58717442f0d"
            },
            {
                "title": "Simple heuristics for unit disk graphs",
                "abstract": "Unit disk graphs are intersection graphs of circles of unit radius in the plane. We present simple and provably good heuristics for a number of classical NP-hard optimization problems on unit disk graphs. The problems considered include maximum independent set, minimum vertex cover, minimum coloring and minimum dominating set. We also present an on-line coloring heuristic which achieves a competitive ratio of 6 for unit disk graphs. Our heuristics do not need a geometric representation of unit disk graphs. Geometric representations are used only in establishing the performance guarantees of the heuristics. Several of our approximation algorithms can be extended to intersection graphs of circles of arbitrary radii in the plane, intersection graphs of regular polygons, and to intersection graphs of higher dimensional regular objects.",
                "paper_link": "https://www.semanticscholar.org/paper/1de1f3f771efe116785f06343c1cffbc4a9de493"
            },
            {
                "title": "System level concurrency control for distributed database systems",
                "abstract": "A distributed database system is one in which the database is spread among several sites and application programs \u201cmove\u201d from site to site to access and update the data they need. The concurrency control is that portion of the system that responds to the read and write requests of the application programs. Its job is to maintain the global consistency of the distributed database while ensuring that the termination of the application programs is not prevented by phenomena such as deadlock. We assume each individual site has its own local concurrency control which responds to requests at that site and can only communicate with concurrency controls at other sites when an application program moves from site to site, terminates, or aborts.\nThis paper presents designs for several distributed concurrency controls and demonstrates that they work correctly. It also investigates some of the implications of global consistency of a distributed database and discusses phenomena that can prevent termination of application programs.",
                "paper_link": "https://www.semanticscholar.org/paper/74dbf7b13fb2870966165f084a972cc29847d17d"
            },
            {
                "title": "NC-approximation schemes for NP-and PSPACE-hard problems for geometric graphs",
                "abstract": "We present NC-approximation schemes for a number of graph problems when restricted to geometric graphs including unit disk graphs and graphs drawn in a civilized manner. Our approximation schemes exhibit the same time versus performance trade-off as the best known approximation schemes for planar graphs. We also define the concept of ?-precision unit disk graphs and show that for such graphs the approximation schemes have a better time versus performance trade-off than the approximation schemes for arbitrary unit disk graphs. Moreover, compared to unit disk graphs, we show that for ?-precision unit disk graphs many more graph problems have efficient approximation schemes.Our NC-approximation schemes can also be extended to obtain efficient NC-approximation schemes for several PSPACE-hard problems on unit disk graphs specified using a restricted version of the hierarchical specification language of Bentley, Ottmann, and Widmayer. The approximation schemes for hierarchically specified unit disk graphs presented in this paper are among the first approximation schemes in the literature for natural PSPACE-hard optimization problems.",
                "paper_link": "https://www.semanticscholar.org/paper/3be90f190c0fcd38a58002bb3af481016d15064f"
            },
            {
                "title": "Properties of deterministic top down grammars",
                "abstract": "The class of context free grammars that can be deterministically parsed in a top down manner with a fixed amount of look-ahead is investigated. These grammars, called LL(k) grammars where k is the amount of look-ahead are first defined and a procedure is given for determining if a context free grammar is LL(k) for a given value of k. It is shown that &egr;-rules can be eliminated from an LL(k) grammar, at the cost of increasing the value of k by one, and a description is given of a canonical pushdown machine for recognizing LL(k) languages. It is shown that for each value of k there are LL(k+l) languages that are not LL(k) languages. It is shown that the equivalence problem is decidable for LL(k) grammars. Additional properties are also given.",
                "paper_link": "https://www.semanticscholar.org/paper/c8869aecbbb01a688902ee82594e83f2806fb2a2"
            },
            {
                "title": "Heuristic and special case algorithms for dispersion problems",
                "abstract": "The dispersion problem arises in selecting facilities to maximize some function of the distances between the facilities. The problem also arises in selecting nondominated solutions for multiobjective decision making. It is known to be NP-hard under two objectives: maximizing the minimum distance (MAX-MIN) between any pair of facilities and maximizing the average distance (MAX-AVG). We consider the question of obtaining near-optimal solutions. for MAX-MIN, we show that if the distances do not satisfy the triangle inequality, there is no polynomial-time relative approximation algorithm unless P = NP. When the distances satisfy the triangle inequality, we analyze an efficient heuristic and show that it provides a performance guarantee of two. We also prove that obtaining a performance guarantee of less than two is NP-hard. for MAX-AVG, we analyze an efficient heuristic and show that it provides a performance guarantee of four when the distances satisfy the triangle inequality. We also present a polynomial-ti...",
                "paper_link": "https://www.semanticscholar.org/paper/1634b7d3dd67daf1067d97185319c44f8a7d227e"
            },
            {
                "title": "Programmed grammars and classes of formal languages",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/0586f6093848e8514be4484f1dedcc2ddcfa0a6f"
            },
            {
                "title": "Bicriteria network design problems",
                "abstract": "We study several bicriteria network design problems phrased as follows: given an undirected graph and two minimization objectives with a budget specified on one objective, find a subgraph satisfying certain connectivity requirements that minimizes the second objective subject to the budget on the first. First, we develop a formalism for bicriteria problems and their approximations. Secondly, we use a simple parametric search technique to provide bicriteria approximation algorithms for problems with two similar criteria, where both criteria are the same measure (such as the diameter or the total cost of a tree) but differ only in the cost function under which the measure is computed. Thirdly, we present an (O(log n), O(log n))-approximation algorithm for finding a diameter-constrained minimum cost spanning tree of an undirected graph on n nodes. Finally, for the class of treewidth-bounded graphs, we provide pseudopolynomial-time algorithms for a number of bicriteria problems using dynamic programming. These pseudopolynomial-time algorithms can be converted to fully polynomialtime approximation schemes using a scaling technique.",
                "paper_link": "https://www.semanticscholar.org/paper/42814955580b876b88f934c2691e1b62f490cae1"
            },
            {
                "title": "Spanning trees\u2014short or small",
                "abstract": "We study the problem of finding small trees. Classical network design problems are considered with the additional constraint that only a specified number k of nodes are required to be connected in the solution. A prototypical example is the kMST problem in which we require a tree of minimum weight spanning at least k nodes in an edge-weighted graph. We show that the kMST problem is NP-hard even for points in the Euclidean plane. We provide approximation algorithms with performance ratio 2v/ for the general edge-weighted case and O(k1/4) for the case of points in the plane. Polynomial-time exact solutions are also presented for the class of treewidth-bounded graphs, which includes trees, series-parallel graphs, and bounded bandwidth graphs, and for points on the boundary of a convex region in the Euclidean plane. We also investigate the problem of finding short trees and, more generally, that of finding networks with minimum diameter. A simple technique is used to provide a polynomiM-time solution for finding k-trees of minimum diameter. We identify easy and hard problems arising in finding short networks using a framework due to T. C. Hu.",
                "paper_link": "https://www.semanticscholar.org/paper/dc6cafbc13f9277f6cfdd42a391ede049406ea00"
            },
            {
                "title": "Approximate algorithms for the traveling salesperson problem",
                "abstract": "Several polynomial time algorithms finding \"good,\" but not necessarily optimal, tours for the traveling salesman problem are considered. For the nearest neighbor method, the worst case ratio of the obtained tour to the optimal tour is shown to increase logarithmically with the number of cities. For another method, which we call the nearest insertion method, the worst case ratio is shown to approach 2 as the number of cities increases. It is also shown that for any n \u2265 8, there are traveling salesman problems with n cities having tours which are k-optimal for all k \u2264 n/4, and for which the ratio with respect to the optimal is 2(1 -1/n).",
                "paper_link": "https://www.semanticscholar.org/paper/8958513bd43be653c964b73956b88577ab3f5f16"
            },
            {
                "title": "Compiler design theory",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Attributed translations",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/b4a5c3481853ae034a242ea752fcddfb311e4fe2"
            },
            {
                "title": "Many birds with one stone: Multi-objective approximation algorithms",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Deterministic left corner parsing",
                "abstract": "Left corner parsing refers to a class of parsing procedures in which the productions are recognized in a particular order which is different than both bottom up and top down. Each production is recognized after its left descendant but before its other descendants. Procedures in this class have occurred frequently in the compiler literature. In this paper a class of grammars, called LC (k) grammars, is defined and shown to be exactly those grammars for which a certain canonical pushdown machine that does left corner parsing is deterministic. A subset of LC(k) grammars, called strong LC(k) grammars, is defined and shown to be those grammars that can be deterministically left corner parsed by a simplified canonical machine. It is shown that when a particular grammatical rewriting procedure is applied to a grammar, the resulting grammar is (strong) LL(k) if and only if the original grammar is (strong) LC(k). This implies that the class of LC(k) languages is identical to the class of LL(k) languages. The syntax directed translations on LC(k) grammars that can be performed by the canonical pushdown machine are also discussed.",
                "paper_link": "https://www.semanticscholar.org/paper/ff696826e2faadd5d6353b8a075c8e15a2f38f3e"
            },
            {
                "title": "Processing conjunctive predicates and queries",
                "abstract": "Several aspects of relational database systems require the processing of predicates. For example, predicates can be tested for satisfiability (as in processing predicate locks), and predicates occurring in queries may be preprocessed in order to reduce the number of database operations when the query is answered. Here we study predicates consisting of conjunctions of comparisons. First, we consider predicates that are conjunctions of =, , and >= comparisons, where a variable can be compared with a constant or with another variable, possibly offset by a constant. Efficient algorithms are given for satisfiability, equivalence, and minimizing the number of comparisons in a predicate. Second, we show that when unequal comparisons between variables are allowed, satisfiability, equivalence, and minimization are NP-hard. Most approximation problems corresponding to minimization are also NP-hard. The preceding efficient algorithms show that the unequal comparison operation is the sole cause of this complexity.",
                "paper_link": "https://www.semanticscholar.org/paper/843f629a77288dd8de3466132d0620d06251f73e"
            },
            {
                "title": "On the equivalence, containment, and covering problems for the regular and context-free languages",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/1b6df88a86c19dfe87e37477c6839718a0d81321"
            },
            {
                "title": "Concurrency control for database systems",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Approximation algorithms for degree-constrained minimum-cost network-design problems",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/a09ccc85c6225590e5b3f2b7b31eb647394f52e8"
            },
            {
                "title": "Distributed database concurrency controls using before-values",
                "abstract": "Associated with the write of a database entity is both the \"before\" or old value, and the \"after\" or new value. Concurrency can be increased by allowing other transactions to read the before values of a given transaction. The ramifications of allowing this, particularly on a distributed system in which limited communications is desirable, are investigated. A careful distinction is made between design decisions concerning communications and design decisions concerning the responses to read/write requests. Two schemes for producing such controls are given, one scheme for systems where processes are committed on termination, and the other for systems where committment is made later.",
                "paper_link": "https://www.semanticscholar.org/paper/82ee3407b0dfe8352ee779884a827fb01587a689"
            },
            {
                "title": "Observations on self-stabilizing graph algorithms for anonymous networks",
                "abstract": "We investigate the existence of deterministic uniform self-stabilizing algorithms (DUSSAs) for a number or problems on connected undirected graphs. This investigation is carried out under three models of parallelism, namely central daemon, restricted parallelism, and maximal paral-lelism. We observe that for several problems including 2-coloring odd-degree complete bipartite graphs, 2-coloring trees, nding maximal independent sets in general graphs, and obtaining a valid coloring of planar graphs, no DUSSAs exist under the maximal parallelism model. A DUSSA for the 6-coloring problem for planar graphs under the central daemon model was presented in GK93]. For the other problems listed above, we present DUSSAs under the central daemon model. We observe that these DUSSAs work correctly under a restricted parallelism model as well. This fact enables us to apply a technique in SRR94] to obtain randomized USSAs (RUSSAs) under the maximal paral-lelism model for all the above problems. These RUSSAs achieve self-stabilization with probability 1. We also observe that techniques due to Angluin Ang80] lead to general results that establish the non-existence of DUSSAs for a large collection of graph problems under any of the parallelism models. The problems in this collection include determining the parity of the number of nodes in a graph and membership testing for various graph classes (for example, planar graphs, chordal graphs, and interval graphs).",
                "paper_link": "https://www.semanticscholar.org/paper/3c49fc66c135bb92aaa81096f630b703f0e2a611"
            }
        ]
    },
    {
        "Professor": "Haiying Shen",
        "Papers": [
            {
                "title": "A review of communication, driver characteristics, and controls aspects of cooperative adaptive cruise control (CACC)",
                "abstract": "Cooperative adaptive cruise control (CACC) systems have the potential to increase traffic throughput by allowing smaller headway between vehicles and moving vehicles safely in a platoon at a harmonized speed. CACC systems have been attracting significant attention from both academia and industry since connectivity between vehicles will become mandatory for new vehicles in the USA in the near future. In this paper, we review three basic and important aspects of CACC systems: communications, driver characteristics, and controls to identify the most challenging issues for their real-world deployment. Different routing protocols that support the data communication requirements between vehicles in the CACC platoon are reviewed. Promising and suitable protocols are identified. Driver characteristics related issues, such as how to keep drivers engaged in driving tasks during CACC operations, are discussed. To achieve mass acceptance, the control design needs to depict real-world traffic variability such as communication effects, driver behavior, and traffic composition. Thus, this paper also discusses the issues that existing CACC control modules face when considering close to ideal driving conditions.",
                "paper_link": "https://www.semanticscholar.org/paper/a0cc1e952809c0e29a29990d89d07abc9f9efd32"
            },
            {
                "title": "A manifesto for future generation cloud computing: Research directions for the next decade",
                "abstract": "The Cloud computing paradigm has revolutionised the computer science horizon during the past decade and has enabled the emergence of computing as the fifth utility. It has captured significant attention of academia, industries, and government bodies. Now, it has emerged as the backbone of modern economy by offering subscription-based services anytime, anywhere following a pay-as-you-go model. This has instigated (1) shorter establishment times for start-ups, (2) creation of scalable global enterprise applications, (3) better cost-to-value associativity for scientific and high performance computing applications, and (4) different invocation/execution models for pervasive and ubiquitous applications. The recent technological developments and paradigms such as serverless computing, software-defined networking, Internet of Things, and processing at network edge are creating new opportunities for Cloud computing. However, they are also posing several new challenges and creating the need for new approaches and research strategies, as well as the re-evaluation of the models that were developed to address issues such as scalability, elasticity, reliability, security, sustainability, and application models. The proposed manifesto addresses them by identifying the major open challenges in Cloud computing, emerging trends, and impact areas. It then offers research directions for the next decade, thus helping in the realisation of Future Generation Cloud Computing.",
                "paper_link": "https://www.semanticscholar.org/paper/1bdc35d453f4be2e63227cc28a8b69cf38860d4d"
            },
            {
                "title": "Task failure prediction in cloud data centers using deep learning",
                "abstract": "A large-scale cloud data center needs to provide high service reliability and availability with low failure occurrence probability. However, current large-scale cloud data centers still face high failure rates due to many reasons such as hardware and software failures, which often result in task and job failures. Such failures can severely reduce the reliability of cloud services and also occupy huge amount of resources to recover the service from failures. Therefore, it is important to predict task or job failures before occurrence with high accuracy to avoid unexpected wastage. Many machine learning and deep learning based methods have been proposed for the task or job failure prediction by analyzing past system message logs and identifying the relationship between the data and the failures. In order to further improve the failure prediction accuracy of the previous machine learning and deep learning based methods, in this paper, we propose a failure prediction algorithm based on multi-layer Bidirectional Long Short Term Memory (Bi-LSTM) to identify task and job failures in the cloud. The goal of Bi-LSTM prediction algorithm is to predict whether the tasks and jobs are failed or completed. The trace-driven experiments show that our algorithm outperforms other state-of-art prediction methods with 93% accuracy and 87% for task failure and job failures respectively.",
                "paper_link": "https://www.semanticscholar.org/paper/4c607a5fd28211fe093ed53cdb869e9d3d0daff0"
            },
            {
                "title": "Machine learning based workload prediction in cloud computing",
                "abstract": "As a widely used IT service, more and more companies shift their services to cloud datacenters. It is important for cloud service providers (CSPs) to provide cloud service resources with high elasticity and cost-effectiveness and then achieve good quality of service (QoS) for their clients. However, meeting QoS with cost-effective resource is a challenging problem for CSPs because the workloads of Virtual Machines (VMs) experience variation over time. It is highly necessary to provide an accurate VMs workload prediction method for resource provisioning to efficiently manage cloud resources. In this paper, we first compare the performance of representative state-of-the-art workload prediction methods. We suggest a method to conduct the prediction a certain time before the predicted time point in order to allow sufficient time for task scheduling based on predicted workload. To further improve the prediction accuracy, we introduce a clustering based workload prediction method, which first clusters all the tasks into several categories and then trains a prediction model for each category respectively. The trace-driven experiments based on Google cluster trace demonstrates that our clustering based workload prediction methods outperform other comparison methods and improve the prediction accuracy to around 90% both in CPU and memory.",
                "paper_link": "https://www.semanticscholar.org/paper/5dde02f7ae47d0e7b0b39313c335a8c0ec484c2b"
            },
            {
                "title": "A survey of mobile crowdsensing techniques: A critical component for the internet of things",
                "abstract": "Mobile crowdsensing serves as a critical building block for the emerging Internet of Things (IoT) applications. However, the sensing devices continuously generate a large amount of data, which consumes much resources (e.g., bandwidth, energy and storage), and may sacrifice the quality-of-service (QoS) of applications. Prior work has demonstrated that there is significant redundancy in the content of the sensed data. By judiciously reducing the redundant data, the data size and the load can be significantly reduced, thereby reducing resource cost, facilitating the timely delivery of unique, probably critical information and enhancing QoS. This paper presents a survey of existing works for the mobile crowdsensing strategies with emphasis on reducing the resource cost and achieving high QoS. We start by introducing the motivation for this survey, and present the necessary background of crowdsensing and IoT. We then present various mobile crowdsensing strategies and discuss their strengths and limitations. Finally, we discuss the future research directions for mobile crowdsensing. The survey addresses a broad range of techniques, methods, models, systems and applications related to mobile crowdsensing and IoT. Our goal is not only to analyze and compare the strategies proposed in the prior works but also to discuss their applicability towards the IoT, and provide the guidance on the future research direction of mobile crowdsensing.",
                "paper_link": "https://www.semanticscholar.org/paper/66d784fbd47285c8ed4d79cf9075c4e907a48125"
            },
            {
                "title": "Smartly handling renewable energy instability in supporting a cloud datacenter",
                "abstract": "The size and energy consumption of datacenters have been increasing significantly over the past years. As a result, datacenters\u2019 increasing electricity monetary cost, energy consumption and energy harmful gas emissions have become a severe problem. Renewable energy supply is widely seen as a promising solution. However, the instability of renewable energy brings about a new challenge since insufficient energy supply may lead to job running interruptions or failures. Though previous works attempt to more accurately predict the amount of produced renewable energy, due to the instability of its influencing factors (e.g., wind, temperature), sufficient renewable energy supply cannot be always guaranteed. To handle this problem, in this paper, we propose allocating jobs with the same service-level-objective (SLO) level to the same physical machine (PM) group, and power each PM group with renewable energy generators that have probability no less than its SLO to produce the amount no less than its energy demand. It ensures that insufficient renewable energy supply will not lead to SLO violations. We use a deep learning technique to predict the probability of producing amount no less than each value of each renewable energy source and predict the energy demands of each PM area. We formulate an optimization problem: how to match renewable energy resources with different instabilities to different PM groups as energy supply in order to minimize the number of SLO violations (due to interruption from insufficient renewable energy supply), total energy monetary cost and total carbon emission. We then use reinforcement learning method and linear programming method to solve the optimization problem. The real trace driven experiments show that our method can achieve much lower SLO violations, total energy monetary cost and total carbon emission compared to other methods.",
                "paper_link": "https://www.semanticscholar.org/paper/f3094c57aef51b8ad82c126ea10a272e1506dd0f"
            },
            {
                "title": "Load rebalancing for distributed file systems in clouds",
                "abstract": "Distributed file systems are key building blocks for cloud computing applications based on the MapReduce programming paradigm. In such file systems, nodes simultaneously serve computing and storage functions; a file is partitioned into a number of chunks allocated in distinct nodes so that MapReduce tasks can be performed in parallel over the nodes. However, in a cloud computing environment, failure is the norm, and nodes may be upgraded, replaced, and added in the system. Files can also be dynamically created, deleted, and appended. This results in load imbalance in a distributed file system; that is, the file chunks are not distributed as uniformly as possible among the nodes. Emerging distributed file systems in production systems strongly depend on a central node for chunk reallocation. This dependence is clearly inadequate in a large-scale, failure-prone environment because the central load balancer is put under considerable workload that is linearly scaled with the system size, and may thus become the performance bottleneck and the single point of failure. In this paper, a fully distributed load rebalancing algorithm is presented to cope with the load imbalance problem. Our algorithm is compared against a centralized approach in a production system and a competing distributed solution presented in the literature. The simulation results indicate that our proposal is comparable with the existing centralized approach and considerably outperforms the prior distributed algorithm in terms of load imbalance factor, movement cost, and algorithmic overhead. The performance of our proposal implemented in the Hadoop distributed file system is further investigated in a cluster environment.",
                "paper_link": "https://www.semanticscholar.org/paper/b8351bd1c9d158f81de2aa616e31546555f1f39e"
            },
            {
                "title": "ALERT: an anonymous location-based efficient routing protocol in MANETs",
                "abstract": "Mobile Ad Hoc Networks (MANETs) use anonymous routing protocols that hide node identities and/or routes from outside observers in order to provide anonymity protection. However, existing anonymous routing protocols relying on either hop-by-hop encryption or redundant traffic, either generate high cost or cannot provide full anonymity protection to data sources, destinations, and routes. The high cost exacerbates the inherent resource constraint problem in MANETs especially in multimedia wireless applications. To offer high anonymity protection at a low cost, we propose an Anonymous Location-based Efficient Routing proTocol (ALERT). ALERT dynamically partitions the network field into zones and randomly chooses nodes in zones as intermediate relay nodes, which form a nontraceable anonymous route. In addition, it hides the data initiator/receiver among many initiators/receivers to strengthen source and destination anonymity protection. Thus, ALERT offers anonymity protection to sources, destinations, and routes. It also has strategies to effectively counter intersection and timing attacks. We theoretically analyze ALERT in terms of anonymity and efficiency. Experimental results exhibit consistency with the theoretical analysis, and show that ALERT achieves better route anonymity protection and lower cost compared to other anonymous routing protocols. Also, ALERT achieves comparable routing efficiency to the GPSR geographical routing protocol.",
                "paper_link": "https://www.semanticscholar.org/paper/f32747b695348c7f094d8d17fee1f8a68c0ed3c9"
            },
            {
                "title": "Game-theoretic analysis of cooperation incentive strategies in mobile ad hoc networks",
                "abstract": "In mobile ad hoc networks (MANETs), tasks are conducted based on the cooperation of nodes in the networks. However, since the nodes are usually constrained by limited computation resources, selfish nodes may refuse to be cooperative. Reputation systems and price-based systems are two main solutions to the node noncooperation problem. A reputation system evaluates node behaviors by reputation values and uses a reputation threshold to distinguish trustworthy nodes and untrustworthy nodes. A price-based system uses virtual cash to control the transactions of a packet forwarding service. Although these two kinds of systems have been widely used, very little research has been devoted to investigating the effectiveness of the node cooperation incentives provided by the systems. In this paper, we use game theory to analyze the cooperation incentives provided by these two systems and by a system with no cooperation incentive strategy. We find that the strategies of using a threshold to determine the trustworthiness of a node in the reputation system and of rewarding cooperative nodes in the price-based system may be manipulated by clever or wealthy but selfish nodes. Illumined by the investigation results, we propose and study an integrated system. Theoretical and simulation results show the superiority of the integrated system over an individual reputation system and a price-based system in terms of the effectiveness of cooperation incentives and selfish node detection.",
                "paper_link": "https://www.semanticscholar.org/paper/11ff8ad08fa3249b689dfba9871e5c4ad6aa4fa6"
            },
            {
                "title": "Cycloid: A constant-degree and lookup-efficient P2P overlay network",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/b693cf2cc19ee2fc4b83eb83782b63f722b889e0"
            },
            {
                "title": "Analyzing and predicting news popularity on Twitter",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/ea5842ee2fce2eee7a6c3c11a635c94280557fa3"
            },
            {
                "title": "Stochastic load balancing for virtual resource management in datacenters",
                "abstract": "Cloud computing offers a cost-effective and elastic computing paradigm that facilitates large scale data storage and analytics. By deploying virtualization technologies in the datacenter, cloud enables efficient resource management and isolation for various big data applications. Since the hotspots (i.e., overloaded machines) can degrade the performance of these applications, virtual machine migration has been utilized to perform load balancing in the datacenters to eliminate hotspots and guarantee Service Level Agreements (SLAs). However, the previous load balancing schemes make migration decisions based on deterministic resource demand estimation and workload characterization, without considering their stochastic properties. By studying real world traces, we show that the resource demand and workload of virtual machines are highly dynamic and bursty, which can cause these schemes to make inefficient migrations for load balancing. To address this problem, in this paper we propose a stochastic load balancing scheme which aims to provide probabilistic guarantee against the resource overloading with virtual machine migration, while minimizing the total migration overhead. Our scheme effectively addresses the prediction of the distribution of resource demand and the multidimensional resource requirements with stochastic characterization. Moreover, as opposed to the previous works that measure the migration cost without considering the network topology, our scheme explicitly takes into account the distance between the source physical machine and the destination physical machine for a virtual machine migration. The trace-driven experiments show that our scheme outperforms the previous schemes in terms of SLA violation and the migration cost.",
                "paper_link": "https://www.semanticscholar.org/paper/3c9b1717e6fabf6bca1fd420f214cf953d113dc3"
            },
            {
                "title": "A review of sensing and communication, human factors, and controller aspects for information-aware connected and automated vehicles",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "An efficient and adaptive decentralized file replication algorithm in P2P file sharing systems",
                "abstract": "In peer-to-peer file sharing systems, file replication technology is widely used to reduce hot spots and improve file query efficiency. Most current file replication methods replicate files in all nodes or two end points on a client-server query path. However, these methods either have low effectiveness or come at a cost of high overhead. File replication in server side enhances replica hit rate, hence, lookup efficiency but produces overloaded nodes and cannot significantly reduce query path length. File replication in client side could greatly reduce query path length, but cannot guarantee high replica hit rate to fully utilize replicas. Though replication along query path solves these problems, it comes at a high cost of overhead due to more replicas and produces underutilized replicas. This paper presents an Efficient and Adaptive Decentralized (EAD) file replication algorithm that achieves high query efficiency and high replica utilization at a significantly low cost. EAD enhances the utilization of file replicas by selecting query traffic hubs and frequent requesters as replica nodes, and dynamically adapting to nonuniform and time-varying file popularity and node interest. Unlike current methods, EAD creates and deletes replicas in a decentralized self-adaptive manner while guarantees high replica utilization. Theoretical analysis shows the high performance of EAD. Simulation results demonstrate the efficiency and effectiveness of EAD in comparison with other approaches in both static and dynamic environments. It dramatically reduces the overhead of file replication, and yields significant improvements on the efficiency and effectiveness of file replication in terms of query efficiency, replica hit rate, and overloaded nodes reduction.",
                "paper_link": "https://www.semanticscholar.org/paper/caefa014e160b456eb2edf0975ffc1c1df6dfa9f"
            },
            {
                "title": "CloudFog: Leveraging fog to extend cloud gaming for thin-client MMOG with high quality of service",
                "abstract": "With the increasing popularity of Massively Multiplayer Online Game (MMOG) and fast growth of mobile gaming, cloud gaming exhibits great promises over the conventional MMOG gaming model as it frees players from the requirement of hardware and game installation on their local computers. However, as the graphics rendering is offloaded to the cloud, the data transmission between the end-users and the cloud significantly increases the response latency and limits the user coverage, thus preventing cloud gaming to achieve high user Quality of Service (QoS). To solve this problem, previous research suggested deploying more datacenters, but it comes at a prohibitive cost. We propose a lightweight system called CloudFog, which incorporates \u201cfog\u201d consisting of supernodes that are responsible for rendering game videos and streaming them to their nearby players. Fog enables the cloud to be only responsible for the intensive game state computation and sending update information to supernodes, which significantly reduce the traffic hence the latency and bandwidth consumption. To further enhance QoS, we propose the reputation based supernode selection strategy to assign each player with a suitable supernode that can provide satisfactory game video streaming service, the receiver-driven encoding rate adaptation strategy to increase the playback continuity, the social network based server assignment strategy to avoid the communication interaction between servers in a datacenter to reduce latency, and the dynamic supernode provisioning strategy to deal with user churns. Experimental results from PeerSim and PlanetLab show the effectiveness and efficiency of CloudFog and our individual strategies in increasing user coverage, reducing response latency and bandwidth consumption.",
                "paper_link": "https://www.semanticscholar.org/paper/9b4bfff46d31f649238e2953ea14b5fe1954764d"
            },
            {
                "title": "Locality-aware and churn-resilient load-balancing algorithms in structured peer-to-peer networks",
                "abstract": "Structured peer-to-peer overlay networks, like distributed hash tables (DHTs), map data items to the network based on a consistent hashing function. Such mapping for data distribution has an inherent load balance problem. Data redistribution algorithms based on randomized matching of heavily loaded nodes with light ones can deal with the dynamics of DHTs. However, they are unable to consider the proximity of the nodes simultaneously. There are other methods that rely on auxiliary networks to facilitate locality-aware load redistribution. Due to the cost of network construction and maintenance, the locality-aware algorithms can hardly work for DHTs with churn. This paper presents a locality-aware randomized load-balancing algorithm to deal with both the proximity and network churn at the same time. We introduce a factor of randomness in the probing of lightly loaded nodes in a range of proximity. We further improve the efficiency by allowing the probing of multiple candidates (d-way) at a time. Simulation results show the superiority of the locality-aware two-way randomized algorithm in comparison with other random or locality-aware algorithms. In DHTs with churn, it performs no worse than the best chum-resilient algorithm. It takes advantage of node capacity heterogeneity and achieves good load balance effectively even in a skewed distribution of items",
                "paper_link": "https://www.semanticscholar.org/paper/078b80c69441a2706bb435abb3840eb4c253438e"
            },
            {
                "title": "Leveraging social networks for P2P content-based file sharing in disconnected MANETs",
                "abstract": "Current peer-to-peer (P2P) file sharing methods in mobile ad hoc networks (MANETs) can be classified into three groups: flooding-based, advertisement-based, and social contact-based. The first two groups of methods can easily have high overhead and low scalability. They are mainly developed for connected MANETs, in which end-to-end connectivity among nodes is ensured. The third group of methods adapts to the opportunistic nature of disconnected MANETs but fails to consider the social interests (i.e., contents) of mobile nodes, which can be exploited to improve the file searching efficiency. In this paper, we propose a P2P content-based file sharing system, namely SPOON, for disconnected MANETs. The system uses an interest extraction algorithm to derive a node's interests from its files for content-based file searching. For efficient file searching, SPOON groups common-interest nodes that frequently meet with each other as communities. It takes advantage of node mobility by designating stable nodes, which have the most frequent contact with community members, as community coordinators for intracommunity searching, and highly mobile nodes that visit other communities frequently as community ambassadors for intercommunity searching. An interest-oriented file searching scheme is proposed for high file searching efficiency. Additional strategies for file prefetching, querying-completion, and loop-prevention, and node churn consideration are discussed to further enhance the file searching efficiency. We first tested our system on the GENI Orbit testbed with a real trace and then conducted event-driven experiment with two real traces and NS2 simulation with simulated disconnected and connected MANET scenarios. The test results show that our system significantly lowers transmission cost and improves file searching success rate compared to current methods.",
                "paper_link": "https://www.semanticscholar.org/paper/46a23f095feed6564b5d79ddc222000319087f31"
            },
            {
                "title": "Consolidating complementary VMs with spatial/temporal-awareness in cloud datacenters",
                "abstract": "In cloud datacenters, effective resource provisioning is needed to maximize energy efficiency and utilization of cloud resources while guaranteeing the Service Level Agreement (SLA) for tenants. Previous resource provisioning strategies either allocate physical resources to virtual machines (VMs) based on static VM resource demands or dynamically handle the variations in VM resource requirements through live VM migrations. However, the former fail to maximize energy efficiency and resource utilization while the latter produce high migration overhead. To handle these problems, we propose an initial VM allocation mechanism that consolidates complementary VMs with spatial/temporal-awareness. Complementary VMs are the VMs whose total demand of each resource dimension (in the spatial space) nearly reaches their host's capacity during VM lifetime period (in the temporal space). Based on our observation of the existence of VM resource utilization patterns, the mechanism predicts the lifetime resource utilization patterns of short-term VMs or periodical resource utilization patterns of long-term VMs. Based on the predicted patterns, it coordinates the requirements of different resources and consolidates complementary VMs in the same physical machine (PM). This mechanism reduces the number of PMs needed to provide VM service hence increases energy efficiency and resource utilization and also reduces the number of VM migrations and SLA violations. Simulation based on two real traces and real-world testbed experiments show that our initial VM allocation mechanism significantly reduces the number of PMs used, SLA violations and VM migrations of the previous resource provisioning strategies.",
                "paper_link": "https://www.semanticscholar.org/paper/cf5ed93ea0611e7886f130d2f8715809969be482"
            },
            {
                "title": "An Energy-Efficient and Distributed Cooperation Mechanism for-Coverage Hole Detection and Healing in WSNs",
                "abstract": "Present approaches to achieve <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives> <inline-graphic xlink:href=\"qiu-ieq2-2767048.gif\"/></alternatives></inline-formula>-coverage for Wireless Sensor Networks still rely on centralized techniques. In this paper, we devise a distributed method for this problem, namely Distributed VOronoi based Cooperation scheme (DVOC), where nodes cooperate in hole detection and recovery. In previous Voronoi based schemes, each node only monitors its own critical points. Such methods are inefficient for <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives> <inline-graphic xlink:href=\"qiu-ieq3-2767048.gif\"/></alternatives></inline-formula>-coverage because the critical points are far away from their generating nodes in <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math> <alternatives><inline-graphic xlink:href=\"qiu-ieq4-2767048.gif\"/></alternatives></inline-formula>-order Voronoi diagram, causing high cost for transmission and computing. As a solution, DVOC enables nodes to monitor others\u2019 critical points around themselves by building local Voronoi diagrams (LVDs). Further, DVOC constrains the movement of every node to avoid generating new holes. If a node cannot reach its destination due to the constraint, its hole healing responsibility will fall to other cooperating nodes. The experimental results from the real world testbed demonstrate that DVOC outperforms the previous schemes.",
                "paper_link": "https://www.semanticscholar.org/paper/798d115bf8730561525f862857fae5ba763c2f49"
            },
            {
                "title": "Selective data replication for online social networks with distributed datacenters",
                "abstract": "Though the new OSN model, which deploys datacenters globally, helps reduce service latency, it causes higher inter-datacenter communication load. In Facebook, each datacenter has a full copy of all data, and the master datacenter updates all other datacenters, generating tremendous load in this new model. Distributed data storage, which only stores a user's data to his/her geographically closest datacenters mitigates the problem. However, frequent interactions between distant users lead to frequent inter-datacenter communication and hence long service latencies. In this paper, we aim to reduce inter-datacenter communications while still achieving low service latency. We first verify the benefits of the new model and present OSN typical properties that underlie the basis of our design. We then propose Selective Data replication mechanism in Distributed Datacenters (<inline-formula> <tex-math notation=\"LaTeX\">$SD^3$</tex-math><alternatives><inline-graphic xlink:type=\"simple\" xlink:href=\"liu-ieq1-2485266.gif\"/> </alternatives></inline-formula>). Since replicas need inter-datacenter data updates, datacenters in <inline-formula> <tex-math notation=\"LaTeX\">$SD^3$</tex-math><alternatives><inline-graphic xlink:type=\"simple\" xlink:href=\"liu-ieq2-2485266.gif\"/> </alternatives></inline-formula> jointly consider update rates and visit rates to select user data for replication; furthermore, <inline-formula><tex-math notation=\"LaTeX\">$SD^3$</tex-math><alternatives> <inline-graphic xlink:type=\"simple\" xlink:href=\"liu-ieq3-2485266.gif\"/></alternatives></inline-formula> atomizes users\u2019 different types of data (e.g., status update, friend post, music) for replication, ensuring that a replica always reduces inter-datacenter communication. <inline-formula><tex-math notation=\"LaTeX\">$SD^3$</tex-math><alternatives> <inline-graphic xlink:type=\"simple\" xlink:href=\"liu-ieq4-2485266.gif\"/></alternatives></inline-formula> also incorporates three strategies to further enhance its performance: locality-aware multicast update tree, replica deactivation, and datacenter congestion control. The results of trace-driven experiments on the real-world PlanetLab testbed demonstrate the higher efficiency and effectiveness of <inline-formula><tex-math notation=\"LaTeX\">$SD^3$</tex-math><alternatives> <inline-graphic xlink:type=\"simple\" xlink:href=\"liu-ieq5-2485266.gif\"/></alternatives></inline-formula> in comparison to other replication methods and the effectiveness of its three schemes.",
                "paper_link": "https://www.semanticscholar.org/paper/c292a42e252c41e254b75033bf8c065b1c350100"
            }
        ]
    },
    {
        "Professor": "Mark Sherriff",
        "Papers": [
            {
                "title": "Prioritization of regression tests using singular value decomposition with empirical change records",
                "abstract": "During development and testing, changes made to a system to repair a detected fault can often inject a new fault into the code base. These injected faults may not be in the same files that were just changed, since the effects of a change in the code base can have ramifications in other parts of the system. We propose a methodology for determining the effect of a change and then prioritizing regression test cases by gathering software change records and analyzing them through singular value decomposition. This methodology generates clusters of files that historically tend to change together. Combining these clusters with test case information yields a matrix that can be multiplied by a vector representing a new system modification to create a prioritized list of test cases. We performed a post hoc case study using this technique with three minor releases of a software product at IBM. We found that our methodology suggested additional regression tests in 50% of test runs and that the highest-priority suggested test found an additional fault 60% of the time.",
                "paper_link": "https://www.semanticscholar.org/paper/a5cff610373318b707833dd230cacfbfddcd87be"
            },
            {
                "title": "Empirical software change impact analysis using singular value decomposition",
                "abstract": "Verification and validation techniques often generate various forms of software development artifacts. Change records created from verification and validation efforts show how files in the system tend to change together in response to fixes for identified faults and failures. We propose a methodology for determining the impact of a new system modification by analyzing software change records through singular value decomposition. This methodology generates clusters of files that historically tend to change together to address faults and failures found in the code base. We performed a post hoc case study using this technique on five open source software systems. We determined that our technique was effective in identifying impacted files in a system from an introduced change when the developers tended to make small, targeted updates to the source system regularly. We further compared our technique against two other impact analysis techniques (Pathlmpact and Coveragelmpact) and found that our technique provided comparable results, while also identifying non-source files that could be impacted by the change.",
                "paper_link": "https://www.semanticscholar.org/paper/475add324ff6504679f93eab8f5850f8fcd747c0"
            },
            {
                "title": "Using LEGO MINDSTORMS NXT and LEJOS in an advanced software engineering course",
                "abstract": "This paper describes the benefits of using LeJOS and the Lego Mindstorms NXT set for teaching advanced software development. While Lego Mindstorms has been used in introduction to computer science courses, it is not reported to be widely used in a simulated production environment requiring such things as threading, network communications, and the implementation of command protocols. Additionally, because the Mindstorms NXT system supports Bluetooth communications with multiple devices, it is possible to use this system as the basis for a complex, communicating system requiring multiple software artifacts on different machines.",
                "paper_link": "https://www.semanticscholar.org/paper/b269756012f09938573caf76e8cfcf9f804afb16"
            },
            {
                "title": "A service learning practicum capstone",
                "abstract": "We present the design and execution of a Service Learning Practicum (SLP) course sequence intended to be year-long capstone for computer science seniors. Students are teamed into groups of six, and develop software for local nonprofit organizations. In addition to the structure of the course, we describe the challenges faced (legal, organizational, etc.), student perceptions via survey results, and provide a number of suggestions for other institutions who are looking to create a similar course sequence. At the end of the cap- stone experience, the customers are provided with working software that meet their current needs.",
                "paper_link": "https://www.semanticscholar.org/paper/82a30e6b8a19ae10f6f1b58b74215e0687903cc9"
            },
            {
                "title": "Designing a spatially aware and autonomous quadcopter",
                "abstract": "The United States creates or acquires increasingly more complex intelligence, surveillance, and reconnaissance (ISR) systems to maintain a strong, leading presence within the world. As a result, ISR systems have become more costly and difficult to manage. The research team focused on continuing previous year efforts of another team to utilize commercial off-the-shelf (COTS) technologies in the development of more flexible and cost-effective ISR systems. The primary goal was to design and implement an autonomous quadcopter that integrated an Android smartphone, an Arduino microcontroller, and several ultrasonic sensors to independently explore and map an unknown area. The project was broken down into three main tasks: construction of the quadcopter and integration of ultrasonic sensors, Android phone, and Arduino microcontroller, development of an Android application that generates navigation commands and avoids collisions, and development of an Android application that uses sensor data for Simultaneous Localization and Mapping (SLAM). This project, a proof-of-concept of a quadcopter system for autonomous navigation and mapping of an unknown environment, demonstrates the feasibility of developing inexpensive ISR systems with commercially available products. The team also found that the Arduino-Android interface was quite complex and caused issues with basic flight stability. The team also found that ultrasonic sensors were capable of partial SLAM by producing rudimentary maps under controlled conditions and simulated stable flight. However, the inexpensive sensors are unlikely to yield the detailed maps necessary for autonomous flight or actionable navigation information.",
                "paper_link": "https://www.semanticscholar.org/paper/e03f36f56ad82cd8c292daf15330ab59c540290b"
            },
            {
                "title": "A (updated) review of empiricism at the sigcse technical symposium",
                "abstract": "The computer science education (CSEd) research community consists of a large group of passionate CS educators who often contribute to other disciplines of CS research. There has been a trend in other disciplines toward more rigorous and empirical evaluation of various hypotheses. Prior investigations of the then-current state of CSEd research showed a distinct lack of rigor in the top research publication venues, with most papers falling in the general category of experience reports. In this paper, we present our examination of the two most recent proceedings of the SIGCSE Technical Symposium, providing a snapshot of the current state of empiricism at the largest CSEd venue. Our goal to categorize the current state of empiricism in the SIGCSE Technical Symposium and identify where the community might benefit from increased empiricism when conducting CSEd research. We found an increase in empirical validation of CSEd research to over 70%; however, our findings suggest that current CSEd research minimizes replication precluding meta-analysis and theory building.",
                "paper_link": "https://www.semanticscholar.org/paper/5becf98f00816af7cfc91187d4f594e0eca7497a"
            },
            {
                "title": "Automated fix generator for SQL injection attacks",
                "abstract": "A critical problem facing todaypsilas Internet community is the increasing number of attacks exploiting flaws found in Web applications. This paper specifically targets input validation vulnerabilities found in SQL queries that may lead to SQL Injection Attacks (SQLIAs). We introduce a tool that automatically detects and suggests fixes to SQL queries that are found to contain SQL Injection Vulnerabilities (SQLIVs). Testing was performed against phpBB v2.0, an open source forum package, to determine the accuracy and efficacy of our software.",
                "paper_link": "https://www.semanticscholar.org/paper/82008b973211cc38908650cc985a09fdebb27550"
            },
            {
                "title": "A systematic literature review of empiricism and norms of reporting in computing education research literature",
                "abstract": "\n Context.\n Computing Education Research (CER) is critical to help the computing education community and policy makers support the increasing population of students who need to learn computing skills for future careers. For a community to systematically advance knowledge about a topic, the members must be able to understand published work thoroughly enough to perform replications, conduct meta-analyses, and build theories. There is a need to understand whether published research allows the CER community to systematically advance knowledge and build theories.\n \n \n Objectives.\n The goal of this study is\n to characterize the reporting of empiricism in Computing Education Research literature by identifying whether publications include content necessary for researchers to perform replications, meta-analyses, and theory building.\n We answer three research questions related to this goal: (RQ1) What percentage of papers in CER venues have some form of empirical evaluation? (RQ2) Of the papers that have empirical evaluation, what are the characteristics of the empirical evaluation? (RQ3) Of the papers that have empirical evaluation, do they follow norms (both for inclusion and for labeling of information needed for replication, meta-analysis, and, eventually, theory-building) for reporting empirical work?\n \n \n Methods.\n We conducted a systematic literature review of the 2014 and 2015 proceedings or issues of five CER venues:\n Technical Symposium on Computer Science Education\n (SIGCSE TS),\n International Symposium on Computing Education Research\n (ICER),\n Conference on Innovation and Technology in Computer Science Education\n (ITiCSE),\n ACM Transactions on Computing Education\n (TOCE), and\n Computer Science Education\n (CSE). We developed and applied the\n CER Empiricism Assessment Rubric\n to the 427 papers accepted and published at these venues over 2014 and 2015. Two people evaluated each paper using the\n Base Rubric\n for characterizing the paper. An individual person applied the other rubrics to characterize the norms of reporting, as appropriate for the paper type. Any discrepancies or questions were discussed between multiple reviewers to resolve.\n \n \n Results.\n We found that over 80% of papers accepted across all five venues had some form of empirical evaluation. Quantitative evaluation methods were the most frequently reported. Papers most frequently reported results on interventions around pedagogical techniques, curriculum, community, or tools. There was a split in papers that had some type of comparison between an intervention and some other dataset or baseline. Most papers reported related work, following the expectations for doing so in the SIGCSE and CER community. However, many papers were lacking properly reported research objectives, goals, research questions, or hypotheses; description of participants; study design; data collection; and threats to validity. These results align with prior surveys of the CER literature.\n \n \n Conclusions.\n CER authors are contributing empirical results to the literature; however, not all norms for reporting are met. We encourage authors to provide clear, labeled details about their work so readers can use the study methodologies and results for replications and meta-analyses. As our community grows, our reporting of CER should mature to help establish computing education theory to support the next generation of computing learners.\n \n",
                "paper_link": "https://www.semanticscholar.org/paper/3895e7b4316660286b49540508718a1658f7b522"
            },
            {
                "title": "Early estimation of defect density using an in-process Haskell metrics model",
                "abstract": "Early estimation of defect density of a product is an important step towards the remediation of the problem associated with affordably guiding corrective actions in the software development process. This paper presents a suite of in-process metrics that leverages the software testing effort to create a defect density prediction model for use throughout the software development process. A case study conducted with Galois Connections, Inc. in a Haskell programming environment indicates that the resulting defect density prediction is indicative of the actual system defect density.",
                "paper_link": "https://www.semanticscholar.org/paper/f337235fa1d4c1d77da38775d6f3baebd463ba80"
            },
            {
                "title": "Compatibility of partnered students in computer science education",
                "abstract": "This paper details the results of an investigation into the compatibility of partnered computer science students. The study involved approximately 290 students at the University of Virginia (UVA). This study builds on the work of researchers at North Carolina State University (NCSU). NCSU researchers have conducted a number of studies on the compatibility of pair programmers. We examined many of the factors that the NCSU researchers explored in their studies (including personality type, learning style, skill level, programming self esteem, work ethic, and time management choices) in order to determine whether the conclusions of the research at NCSU also hold true at UVA. Consistent with the NCSU studies, we found that skill level continues to be the most important factor in student compatibility.",
                "paper_link": "https://www.semanticscholar.org/paper/270fe8a52bc6cc5f1f9602ee3c9b853eb09f756c"
            },
            {
                "title": "Designing a spatially aware, automated quadcopter using an Android control system",
                "abstract": "Intelligence gathering is a critical component of military operations. Unmanned aerial vehicles (UAVs) have become an increasingly useful tool due to their surveillance and reconnaissance capabilities. However, the use of many of these vehicles is limited to outdoor environments because of their size and reliance on Global Positioning Satellites (GPS). Knowledge of indoor environments is important so that the risk of entering an unsafe or unknown building can be minimized. This paper describes the development of a spatially aware, autonomous quadcopter that uses an Android control system and functions indoors. The system consists of a laser rangefinder for sensory input, a IOIO microcontroller for data communication across platforms, an autopilot system (APM) for flight control, and an Android phone for mission control. The Android Control and Sensor System (ACSS) is currently being developed by the Department of Defense (DOD), MITRE, and academic partners, and will be integrated into the solution. To be considered autonomous, the quadcopter must be able to make a map from the data provided by the laser rangefinder, determine its own location and position in that map, and then execute a set of navigational commands from the Android control system. The success of this project is measured by the system's ability to travel autonomously while simultaneously creating a map and being aware of its location.",
                "paper_link": "https://www.semanticscholar.org/paper/cae2311413c59d7e95f09f89650edb921c789461"
            },
            {
                "title": "Capstones and large projects in computing education",
                "abstract": "Capstone and large projects in computing education are used as a vehicle for giving students as close to a \u201creal-world\u201d experience in software development as possible within the constraints of a computing degree program. This special issue presents four articles that focus on empirical research on capstone or other large-scale projects. These articles discuss areas such as project selection, working with external stakeholders, choosing the appropriate development methodology, incorporating creative activities to support student engagement, and learning.",
                "paper_link": "https://www.semanticscholar.org/paper/a11a1c12d5cd9bfebc7edb40f81e41cd265f0502"
            },
            {
                "title": "Using groupings of static analysis alerts to identify files likely to contain field failures",
                "abstract": "In this paper, we propose a technique for leveraging historical field failure records in conjunction with automated static analysis alerts to determine which alerts or sets of alerts are predictive of a field failure. Our technique uses singular value decomposition to generate groupings of static analysis alert types, which we call alert signatures, that have been historically linked to field failure-prone files in previous releases of a software system. The signatures can be applied to sets of alerts from a current build of a software system. Files that have a matching alert signature are identified as having similar static analysis alert characteristics to files with known field failures in a previous release of the system. We performed a case study involving an industrial software system at IBM and found three distinct alert signatures that could be applied to the system. We found that 50% of the field failures reported since the last static analysis run could be discovered by examining the 10% of the files and static analysis alerts indicated by these three alert signatures. The remaining failures were either not detected by a signature which could be an indication of a new type of error in the field, or they were on areas of the code where no static analysis alerts were detected.",
                "paper_link": "https://www.semanticscholar.org/paper/153e7d5c1a3e7134aa39761aed2f80b3572d3758"
            },
            {
                "title": "\u201cInform, Experience, Implement\u201d\u2014Teaching an intensive high school summer course",
                "abstract": "During the summer of 2011, twenty-four high school students participated in an intense, three-week computer science course at the University of Virginia. The course met for twenty-one three-hour sessions, thus encompassing more contact time than a standard college-level course. The course was structured in an \u201cInform, Experience, Implement\u201d active-learning format: students were exposed to the history of a particular problem in context and participated in an active learning lesson regarding the topic before learning how to address the examined problem through programming. This structure helped integrate into the course best practices from experiential learning, kinesthetic outreach activities, and active learning pedagogy. Utilizing this three-part rotation curriculum achieved some important goals, including holding the interest of students during the summer for six hours a day and successfully motivating students who had no programming background.",
                "paper_link": "https://www.semanticscholar.org/paper/771793ad7541a3d05ae2ca386e23ef7747dc56c1"
            },
            {
                "title": "Teaching second-level Java and software engineering with Android",
                "abstract": "Over the past two years, second-year Java and software engineering courses have been taught at the University of Virginia and North Carolina State University utilizing the Android OS platform. Instructors taught a variety of traditional second-year topics, including abstraction, design, requirements, and testing, utilizing a variety of Android-based mobile devices. Anecdotal responses from student surveys and evaluations from five course sessions indicate that teaching lower-level courses with more advanced and current technology, even with a steeper learning curve, is beneficial. In this tutorial proposal, we outline our plan for presenting a session that would help educators incorporate the Android OS into their curriculum and how to use the system even if mobile devices are not available.",
                "paper_link": "https://www.semanticscholar.org/paper/277bfa561e609df00d2a298aa1b6dafab92728ae"
            },
            {
                "title": "Achievement unlocked: Investigating which gamification elements motivate students",
                "abstract": "Gamification has been used in many different ways to motivate individuals to wholly participate in some activity. One such venue has been in the gamification of learning to promote student interest. In this paper, we describe our efforts to investigate which aspects of gamification students find the most motivating. We present our gamification platform, GamerCard, which was used for four semesters in an upper-level game design course at our institution. We found that some gamification elements that are often thought to be motivating for participants had little to no effect on our course, while elements that specifically targeted making the student\u2019s standing in the course more transparent were the most effective.",
                "paper_link": "https://www.semanticscholar.org/paper/b41b462237556c7db26dd4c35dfecfbf0e31c41d"
            },
            {
                "title": "Defect density estimation through verification and validation",
                "abstract": "In industry, information on defect density of a product tends to become available too late in the software development process to affordably guide corrective actions. Our research objective is to build a parametric model which utilizes a persistent record of the validation and verification (V&V) practices used with a program to estimate the defect density of that program. The persistent record of the V&V practices are recorded as certificates which are automatically recorded and maintained with the code. To date, we have created a parametric modeling process with the help of the Center for Software Engineering at the University of Southern California and have created the second version of an Eclipse plug-in for recording V&V certificates.",
                "paper_link": "https://www.semanticscholar.org/paper/f91f353378a4fe987de687b785c075c765c3e533"
            },
            {
                "title": "A more cost-effective unattended ground sensor using commercial off-the-shelf products",
                "abstract": "The military is increasingly using sensors as part of its battlefield strategy. Sensors can be used as an alternative to placing soldiers in dangerous situations and the information that sensors collect helps leaders make better decisions. However, the cost of integrating these sensors into existing tactical networks has been a limiting factor in their adoption. Thus, the research team sought to use Android smartphones and Arduino microcontrollers, two commercial off-the-shelf (COTS) technologies, to create an inexpensive, \"plug-and-play\" interface to help relieve this issue. This interface was then demonstrated through the creation of a series of unattended ground vehicles (UGVs) that can be networked together in an ad-hoc wireless mesh network to collect sensor data from across an operational area. This project was broken down into two phases and the Agile development methodology was used throughout. During the first phase an interface between the Android phone and Arduino microcontroller was created and then integrated with sensors. During the second phase a robotics platform, path setting algorithm, and the ad-hoc wireless mesh network were developed. Collectively, this project demonstrated the feasibility of using open-source, commercially available parts to create unattended ground sensor (UGS) networks, thereby providing an alternative to current market offerings which are custom and proprietary in nature and therefore expensive and difficult to upgrade.",
                "paper_link": "https://www.semanticscholar.org/paper/2a4aec0d397a617d474c10b8b0d9e0b7b3c06a2d"
            },
            {
                "title": "Identifying fault-prone files using static analysis alerts through singular value decomposition",
                "abstract": "Static analysis tools tend to generate more alerts than a development team can reasonably examine without some form of guidance. In this paper, we propose a technique for leveraging field failures and historical change records to determine which sets of alerts are often associated with a field failure using singular value decomposition. We performed a case study on six major components of an industrial software system at IBM over six builds spanning eighteen months of development. Our technique identified fourteen alert types that comprised sets of alerts that could identify, on average, 45% of future fault-prone files and up to 65% in some instances.",
                "paper_link": "https://www.semanticscholar.org/paper/a2415e029a1aa3d1ca2f260cb5ce63e90e3b7a3e"
            },
            {
                "title": "Analyzing software artifacts through singular value decomposition to guide development decisions",
                "abstract": "SHERRIFF, MARK STEPHEN. Analyzing Software Artifacts through Singular Value Decomposition to Guide Development Decisions. (Under the direction of Laurie A. Williams.) During development, programming teams will produce numerous types of software development artifacts. A software development artifact is an intermediate or final product that is the result or by-product of software development. Hidden relationships and structures within a software system can be illuminated through singular value decomposition using software development artifacts, and these relationships can be leveraged to help guide software development questions regarding the interactions among software files. The goal of this research is to build and investigate a framework called Software Development Artifact Analysis (SDAA) that uses software development artifacts to illuminate underlying relationships within a system. SDAA provides guidelines for selecting and gathering software development artifacts, discovering relationships, and then leveraging the insights gained through the analysis of those relationships. We use singular value decomposition (SVD) to generate the relationships from a matrix of software development",
                "paper_link": "https://www.semanticscholar.org/paper/2b9a139d2ad1e79cb0c64d245fb5f7bc723fedac"
            }
        ]
    },
    {
        "Professor": "Kevin Skadron",
        "Papers": [
            {
                "title": "Rodinia: A benchmark suite for heterogeneous computing",
                "abstract": "This paper presents and characterizes Rodinia, a benchmark suite for heterogeneous computing. To help architects study emerging platforms such as GPUs (Graphics Processing Units), Rodinia includes applications and kernels which target multi-core CPU and GPU platforms. The choice of applications is inspired by Berkeley's dwarf taxonomy. Our characterization shows that the Rodinia benchmarks cover a wide range of parallel communication patterns, synchronization techniques and power consumption, and has led to some important architectural insight, such as the growing importance of memory-bandwidth limitations and the consequent importance of data layout.",
                "paper_link": "https://www.semanticscholar.org/paper/419f49c47f6fddae7235fb7c72335bccc1a3b743"
            },
            {
                "title": "Scalable parallel programming with cuda: Is cuda the parallel programming model that application developers have been waiting for?",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Temperature-aware microarchitecture",
                "abstract": "With power density and hence cooling costs rising exponentially, processor packaging can no longer be designed for the worst case, and there is an urgent need for runtime processor-level techniques that can regulate operating temperature when the package's capacity is exceeded. Evaluating such techniques, however, requires a thermal model that is practical for architectural studies. We describe HotSpot, an accurate yet fast model based on an equivalent circuit of thermal resistances and capacitances that correspond to microarchitecture blocks and essential aspects of the thermal package. Validation was performed using finite-element simulation. We also introduce several effective methods for dynamic thermal management (DTM): \"temperature-tracking\" frequency scaling, localized toggling, and migrating computation to spare hardware units. Modeling temperature at the microarchitecture level also shows that power metrics are poor predictors of temperature, and that sensor imprecision has a substantial impact on the performance of DTM.",
                "paper_link": "https://www.semanticscholar.org/paper/b5831b70e060f883e626dacf5a1b008ac7626265"
            },
            {
                "title": "HotSpot: A compact thermal modeling methodology for early-stage VLSI design",
                "abstract": "This paper presents HotSpot-a modeling methodology for developing compact thermal models based on the popular stacked-layer packaging scheme in modern very large-scale integration systems. In addition to modeling silicon and packaging layers, HotSpot includes a high-level on-chip interconnect self-heating power and thermal model such that the thermal impacts on interconnects can also be considered during early design stages. The HotSpot compact thermal modeling approach is especially well suited for preregister transfer level (RTL) and presynthesis thermal analysis and is able to provide detailed static and transient temperature information across the die and the package, as it is also computationally efficient.",
                "paper_link": "https://www.semanticscholar.org/paper/167f78125336294e184773d1469f816944af7e11"
            },
            {
                "title": "Temperature-aware microarchitecture: Modeling and implementation",
                "abstract": "With cooling costs rising exponentially, designing cooling solutions for worst-case power dissipation is prohibitively expensive. Chips that can autonomously modify their execution and power-dissipation characteristics permit the use of lower-cost cooling solutions while still guaranteeing safe temperature regulation. Evaluating techniques for this dynamic thermal management (DTM), however, requires a thermal model that is practical for architectural studies.This paper describes HotSpot, an accurate yet fast and practical model based on an equivalent circuit of thermal resistances and capacitances that correspond to microarchitecture blocks and essential aspects of the thermal package. Validation was performed using finite-element simulation. The paper also introduces several effective methods for DTM: \"temperature-tracking\" frequency scaling, \"migrating computation\" to spare hardware units, and a \"hybrid\" policy that combines fetch gating with dynamic voltage scaling. The latter two achieve their performance advantage by exploiting instruction-level parallelism, showing the importance of microarchitecture research in helping control the growth of cooling costs.Modeling temperature at the microarchitecture level also shows that power metrics are poor predictors of temperature, that sensor imprecision has a substantial impact on the performance of DTM, and that the inclusion of lateral resistances for thermal diffusion is important for accuracy.",
                "paper_link": "https://www.semanticscholar.org/paper/6681c5cecb6efb15f170786e04e05fc77820be50"
            },
            {
                "title": "A performance study of general-purpose applications on graphics processors using CUDA",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/81c2a5fcdb3b192790d484ea822cce888b77f66b"
            },
            {
                "title": "Bubble-up: Increasing utilization in modern warehouse scale computers via sensible co-locations",
                "abstract": "As much of the world's computing continues to move into the cloud, the overprovisioning of computing resources to ensure the performance isolation of latency-sensitive tasks, such as web search, in modern datacenters is a major contributor to low machine utilization. Being unable to accurately predict performance degradation due to contention for shared resources on multicore systems has led to the heavy handed approach of simply disallowing the co-location of high-priority, latency-sensitive tasks with other tasks. Performing this precise prediction has been a challenging and unsolved problem. In this paper, we present Bubble-Up, a characterization methodology that enables the accurate prediction of the performance degradation that results from contention for shared resources in the memory subsystem. By using a bubble to apply a tunable amount of \u201cpressure\u201d to the memory subsystem on processors in production datacenters, our methodology can predict the performance interference between co-locate applications with an accuracy within 1% to 2% of the actual performance degradation. Using this methodology to arrive at \u201csensible\u201d co-locations in Google's production datacenters with real-world large-scale applications, we can improve the utilization of a 500-machine cluster by 50% to 90% while guaranteeing a high quality of service of latency-sensitive applications.",
                "paper_link": "https://www.semanticscholar.org/paper/bd9d8d5fe2bdb6e17cf6bf23b5ca24838bbc64bf"
            },
            {
                "title": "Control-theoretic techniques and thermal-RC modeling for accurate and localized dynamic thermal management",
                "abstract": "This paper proposes the use of formal feedback control theory as a way to implement adaptive techniques in the processor architecture. Dynamic thermal management (DTM) is used as a test vehicle, and variations of a PID controller (Proportional-Integral-Differential) are developed and tested for adaptive control of fetch \"toggling.\" To accurately test the DTM mechanism being proposed, this paper also develops a thermal model based on lumped thermal resistances and thermal capacitances. This model is computationally efficient and tracks temperature at the granularity of individual functional blocks within the processor. Because localized heating occurs much faster than chip-wide heating, some parts of the processor are more likely, to be \"hot spots\" than others. Experiments using Wattch and the SPEC2000 benchmarks show that the thermal trigger threshold can be set within 0.2/spl deg/ of the maximum temperature and yet never enter thermal emergency. This cuts the performance loss of DTM by 65% compared to the previously described fetch toggling technique that uses a response of fixed magnitude.",
                "paper_link": "https://www.semanticscholar.org/paper/0ad82bc5cac34afcee9e7bbe43e9f3e2c786f98f"
            },
            {
                "title": "Accelerating compute-intensive applications with GPUs and FPGAs",
                "abstract": "Accelerators are special purpose processors designed to speed up compute-intensive sections of applications. Two extreme endpoints in the spectrum of possible accelerators are FPGAs and GPUs, which can often achieve better performance than CPUs on certain workloads. FPGAs are highly customizable, while GPUs provide massive parallel execution resources and high memory bandwidth. Applications typically exhibit vastly different performance characteristics depending on the accelerator. This is an inherent problem attributable to architectural design, middleware support and programming style of the target platform. For the best application-to-accelerator mapping, factors such as programmability, performance, programming cost and sources of overhead in the design flows must be all taken into consideration. In general, FPGAs provide the best expectation of performance, flexibility and low overhead, while GPUs tend to be easier to program and require less hardware resources. We present a performance study of three diverse applications - Gaussian elimination, data encryption standard (DES), and Needleman-Wunsch - on an FPGA, a GPU and a multicore CPU system. We perform a comparative study of application behavior on accelerators considering performance and code complexity. Based on our results, we present an application characteristic to accelerator platform mapping, which can aid developers in selecting an appropriate target architecture for their chosen application.",
                "paper_link": "https://www.semanticscholar.org/paper/63c5bade34dd225217a0a23604400c0bc32a4735"
            },
            {
                "title": "Compact thermal modeling for temperature-aware design",
                "abstract": "Thermal design in sub-100nm technologies is one of the major challenges to the CAD community. In this paper, we first introduce the idea of temperature-aware design. We then propose a compact thermal model which can be integrated with modern CAD tools to achieve a temperature-aware design methodology. Finally, we use the compact thermal model in a case study of microprocessor design to show the importance of using temperature as a guideline for the design. Results from our thermal model show that a temperature-aware design approach can provide more accurate estimations, and therefore better decisions and faster design convergence.",
                "paper_link": "https://www.semanticscholar.org/paper/d5140436720a1afa05c0c041434bfd523f697707"
            },
            {
                "title": "A characterization of the Rodinia benchmark suite with comparison to contemporary CMP workloads",
                "abstract": "The recently released Rodinia benchmark suite enables users to evaluate heterogeneous systems including both accelerators, such as GPUs, and multicore CPUs. As Rodinia sees higher levels of acceptance, it becomes important that researchers understand this new set of benchmarks, especially in how they differ from previous work. In this paper, we present recent extensions to Rodinia and conduct a detailed characterization of the Rodinia benchmarks (including performance results on an NVIDIA GeForce GTX480, the first product released based on the Fermi architecture). We also compare and contrast Rodinia with Parsec to gain insights into the similarities and differences of the two benchmark collections; we apply principal component analysis to analyze the application space coverage of the two suites. Our analysis shows that many of the workloads in Rodinia and Parsec are complementary, capturing different aspects of certain performance metrics.",
                "paper_link": "https://www.semanticscholar.org/paper/ba102fef512097da3bd8d6d5302859c5e193c8c7"
            },
            {
                "title": "Accelerating SQL database operations on a GPU with CUDA",
                "abstract": "Prior work has shown dramatic acceleration for various database operations on GPUs, but only using primitives that are not part of conventional database languages such as SQL. This paper implements a subset of the SQLite command processor directly on the GPU. This dramatically reduces the effort required to achieve GPU acceleration by avoiding the need for database programmers to use new programming languages such as CUDA or modify their programs to use non-SQL libraries.\n This paper focuses on accelerating SELECT queries and describes the considerations in an efficient GPU implementation of the SQLite command processor. Results on an NVIDIA Tesla C1060 achieve speedups of 20-70X depending on the size of the result set.",
                "paper_link": "https://www.semanticscholar.org/paper/dcd5b42153c09ea208d0a76a24d04847629b9194"
            },
            {
                "title": "Hotleakage: A temperature-aware model of subthreshold and gate leakage for architects",
                "abstract": "This report introduces HotLeakage, an architectural model for subthreshold and gate leakage that we have developed here at the University of Virginia. The most important features of HotLeakage are the explicit inclusion of temperature, voltage, gate leakage, and parameter variations, and the ability to recalculate leakage currents dynamically as temperature and voltage change due to operating conditions, DVS techniques, etc. HotLeakage provides default settings for 180nm through 70nm technologies for modeling cache and register files, and provides a simple interface for selecting alternate parameter values and for modeling alternative microarchitecture structures. It also provides models for several extant cache leakage control techniques, with an interface for adding further techniques. HotLeakage is currently a semi-independent module for use with SimpleScalar, but is sufficiently modular that it should be fairly easy to port to other simulators. Because sub-threshold leakage currents are exponentially dependent on temperature and voltage, because gate leakage is growing so rapidly, and because parameter variations can have a profound effect on simulation accuracy, we hope that HotLeakage will serve as a useful tool for microarchitects to more accurately evaluate issues related leakage power. HotLeakage is available for download athttp://lava.cs.virginia.edu/HotLeakage",
                "paper_link": "https://www.semanticscholar.org/paper/c2e222747c3ce97cc1e688741687b902994b71c8"
            },
            {
                "title": "Dynamic voltage scaling in multitier web servers with end-to-end delay control",
                "abstract": "The energy and cooling costs of Web server farms are among their main financial expenditures. This paper explores the benefits of dynamic voltage scaling (DVS) for power management in server farms. Unlike previous work, which addressed DVS on individual servers and on load-balanced server replicas, this paper addresses DVS in multistage service pipelines. Contemporary Web server installations typically adopt a three-tier architecture in which the first tier presents a Web interface, the second executes scripts that implement business logic, and the third serves database accesses. From a user's perspective, only the end-to-end response across the entire pipeline is relevant. This paper presents a rigorous optimization methodology and an algorithm for minimizing the total energy expenditure of the multistage pipeline subject to soft end-to-end response-time constraints. A distributed power management service is designed and evaluated on a real three-tier server prototype for coordinating DVS settings in a way that minimizes global energy consumption while meeting end-to-end delay constraints. The service is shown to consume as much as 30 percent less energy compared to the default (Linux) energy saving policy",
                "paper_link": "https://www.semanticscholar.org/paper/23265cad4d3f6dd2db1d0f5e58286f3ea98175af"
            },
            {
                "title": "Dynamic warp subdivision for integrated branch and memory divergence tolerance",
                "abstract": "SIMD organizations amortize the area and power of fetch, decode, and issue logic across multiple processing units in order to maximize throughput for a given area and power budget. However, throughput is reduced when a set of threads operating in lockstep (a warp) are stalled due to long latency memory accesses. The resulting idle cycles are extremely costly. Multi-threading can hide latencies by interleaving the execution of multiple warps, but deep multi-threading using many warps dramatically increases the cost of the register files (multi-threading depth x SIMD width), and cache contention can make performance worse. Instead, intra-warp latency hiding should first be exploited. This allows threads that are ready but stalled by SIMD restrictions to use these idle cycles and reduces the need for multi-threading among warps. This paper introduces dynamic warp subdivision (DWS), which allows a single warp to occupy more than one slot in the scheduler without requiring extra register file space. Independent scheduling entities allow divergent branch paths to interleave their execution, and allow threads that hit to run ahead. The result is improved latency hiding and memory level parallelism (MLP). We evaluate the technique on a coherent cache hierarchy with private L1 caches and a shared L2 cache. With an area overhead of less than 1%, experiments with eight data-parallel benchmarks show our technique improves performance on average by 1.7X.",
                "paper_link": "https://www.semanticscholar.org/paper/c0546dd48f078225cf4167c6587b6d5e32ab96c3"
            },
            {
                "title": "Energy-efficient mechanisms for managing thread context in throughput processors",
                "abstract": "Modern graphics processing units (GPUs) use a large number of hardware threads to hide both function unit and memory access latency. Extreme multithreading requires a complicated thread scheduler as well as a large register file, which is expensive to access both in terms of energy and latency. We present two complementary techniques for reducing energy on massively-threaded processors such as GPUs. First, we examine register file caching to replace accesses to the large main register file with accesses to a smaller structure containing the immediate register working set of active threads. Second, we investigate a two-level thread scheduler that maintains a small set of active threads to hide ALU and local memory access latency and a larger set of pending threads to hide main memory latency. Combined with register file caching, a two-level thread scheduler provides a further reduction in energy by limiting the allocation of temporary register cache resources to only the currently active subset of threads. We show that on average, across a variety of real world graphics and compute workloads, a 6-entry per-thread register file cache reduces the number of reads and writes to the main register file by 50% and 59% respectively. We further show that the active thread count can be reduced by a factor of 4 with minimal impact on performance, resulting in a 36% reduction of register file energy.",
                "paper_link": "https://www.semanticscholar.org/paper/e679f628891a2f055ce9bfae528d419b6d075426"
            },
            {
                "title": "Power-aware QoS management in web servers",
                "abstract": "Power management in data centers has become an increasingly important concern. Large server installations are designed to handle peak load, which may be significantly larger than in off-peak conditions. The increasing cost of energy consumption and cooling incurred in farms of high-performance Web servers make low-power operation during off-peak hours desirable. This paper investigates adaptive algorithms for dynamic voltage scaling in QoS-enabled Web servers to minimize energy consumption subject to service delay constraints. We implement these algorithms inside the Linux kernel. The instrumented kernel supports multiple client classes with per-class deadlines. Energy consumption is minimized by using a feedback loop that regulates frequency and voltage levels to keep the synthetic utilization around the aperiodic schedulability bound derived in an earlier publication. Enforcing the bound ensures that deadlines are met. Our evaluation of an Apache server running on the modifier Linux kernel shows that non-trivial off-peak energy savings are possible without sacrificing timeliness.",
                "paper_link": "https://www.semanticscholar.org/paper/e7316b8fea2da01dd489723831bf3513c3a6a554"
            },
            {
                "title": "Recent thermal management techniques for microprocessors",
                "abstract": "Microprocessor design has recently encountered many constraints such as power, energy, reliability, and temperature. Among these challenging issues, temperature-related issues have become especially important within the past several years. We summarize recent thermal management techniques for microprocessors, focusing on those that affect or rely on the microarchitecture. We categorize thermal management techniques into six main categories: temperature monitoring, microarchitectural techniques, floorplanning, OS/compiler techniques, liquid cooling techniques, and thermal reliability/security. Temperature monitoring, a requirement for Dynamic Thermal Management (DTM), includes temperature estimation and sensor placement techniques for accurate temperature measurement or estimation. Microarchitectural techniques include both static and dynamic thermal management techniques that control hardware structures. Floorplanning covers a range of thermal-aware floorplanning techniques for 2D and 3D microprocessors. OS/compiler techniques include thermal-aware task scheduling and instruction scheduling techniques. Liquid cooling techniques are higher-capacity alternatives to conventional air cooling techniques. Thermal reliability/security issues cover temperature-dependent reliability modeling, Dynamic Reliability Management (DRM), and malicious codes that specifically cause overheating. Temperature-related issues will only become more challenging as process technology continues to evolve and transistor densities scale up faster than power per transistor scales down. The overall objective of this survey is to give microprocessor designers a broad perspective on various aspects of designing thermal-aware microprocessors and to guide future thermal management studies.",
                "paper_link": "https://www.semanticscholar.org/paper/a034e2f3be1507c8171f2731c7da2fcd49d8c83f"
            },
            {
                "title": "A case for thermal-aware floorplanning at the microarchitectural level",
                "abstract": "In current day microprocessors, exponentially increasing power densities, leakage, cooling costs, and reliability concerns have resulted in temperature becoming a first class design constraint like performance and power. Hence, virtually every high performance microprocessor uses a combination of an elaborate thermal package and some form of Dynamic Thermal Management (DTM) scheme that adaptively controls its temperature. While DTM schemes exploit the important variable of power density to control temperature, this paper attempts to show that there is a significant peak temperature reduction potential in managing lateral heat spreading through floorplanning. It argues that this potential warrants consideration of the temperature-performance trade-off early in the design stage at the microarchitectural level using floorplanning. As a demonstration, it uses previously proposed wire delay model and floorplanning algorithm based on simulated annealing to present a profile-driven, thermal-aware floorplanning scheme that significantly reduces peak processor temperature with minimal performance impact that is quite competitive with DTM.",
                "paper_link": "https://www.semanticscholar.org/paper/1e3b45d1736a0d2733ac2db2d8a3025f3420d90e"
            },
            {
                "title": "Pannotia: Understanding irregular GPGPU graph applications",
                "abstract": "GPUs have become popular recently to accelerate general-purpose data-parallel applications. However, most existing work has focused on GPU-friendly applications with regular data structures and access patterns. While a few prior studies have shown that some irregular workloads can also achieve speedups on GPUs, this domain has not been investigated thoroughly. Graph applications are one such set of irregular workloads, used in many commercial and scientific domains. In particular, graph mining -as well as web and social network analysis- are promising applications that GPUs could accelerate. However, implementing and optimizing these graph algorithms on SIMD architectures is challenging because their data-dependent behavior results in significant branch and memory divergence. To address these concerns and facilitate research in this area, this paper presents and characterizes a suite of GPGPU graph applications, Pannotia, which is implemented in OpenCL and contains problems from diverse and important graph application domains. We perform a first-step characterization and analysis of these benchmarks and study their behavior on real hardware. We also use clustering analysis to illustrate the similarities and differences of the applications in the suite. Finally, we make architectural and scheduling suggestions that will improve their execution efficiency on GPUs.",
                "paper_link": "https://www.semanticscholar.org/paper/f715f8fb25f4f342ef99ef48304206bcc4ae5dcc"
            }
        ]
    },
    {
        "Professor": "Yixin Sun",
        "Papers": [
            {
                "title": "RAPTOR: Routing Attacks on Privacy in Tor",
                "abstract": "The Tor network is a widely used system for anonymous communication. However, Tor is known to be vulnerable to attackers who can observe traffic at both ends of the communication path. In this paper, we show that prior attacks are just the tip of the iceberg. We present a suite of new attacks, called Raptor, that can be launched by Autonomous Systems (ASes) to compromise user anonymity. First, AS-level adversaries can exploit the asymmetric nature of Internet routing to increase the chance of observing at least one direction of user traffic at both ends of the communication. Second, AS-level adversaries can exploit natural churn in Internet routing to lie on the BGP paths for more users over time. Third, strategic adversaries can manipulate Internet routing via BGP hijacks (to discover the users using specific Tor guard nodes) and interceptions (to perform traffic analysis). We demonstrate the feasibility of Raptor attacks by analyzing historical BGP data and Traceroute data as well as performing real-world attacks on the live Tor network, while ensuring that we do not harm real users. In addition, we outline the design of two monitoring frameworks to counter these attacks: BGP monitoring to detect control-plane attacks, and Traceroute monitoring to detect data-plane anomalies. Overall, our work motivates the design of anonymity systems that are aware of the dynamics of Internet routing.",
                "paper_link": "https://www.semanticscholar.org/paper/2c7de89466fc1cbaf51b4e4d64b3c95938fef7f0"
            },
            {
                "title": "Bamboozling certificate authorities with {BGP}",
                "abstract": "The Public Key Infrastructure (PKI) protects users from malicious man-in-the-middle attacks by having trusted Certificate Authorities (CAs) vouch for the domain names of servers on the Internet through digitally signed certificates. Ironically, the mechanism CAs use to issue certificates is itself vulnerable to man-in-the-middle attacks by network-level adversaries. Autonomous Systems (ASes) can exploit vulnerabilities in the Border Gateway Protocol (BGP) to hijack traffic destined to a victim\u2019s domain. In this paper, we rigorously analyze attacks that an adversary can use to obtain a bogus certificate. We perform the first real-world demonstration of BGP attacks to obtain bogus certificates from top CAs in an ethical manner. To assess the vulnerability of the PKI, we collect a dataset of 1.8 million certificates and find that an adversary would be capable of gaining a bogus certificate for the vast majority of domains. Finally, we propose and evaluate two countermeasures to secure the PKI: 1) CAs verifying domains from multiple vantage points to make it harder to launch a successful attack, and 2) a BGP monitoring system for CAs to detect suspicious BGP routes and delay certificate issuance to give network operators time to react to BGP attacks.",
                "paper_link": "https://www.semanticscholar.org/paper/520d5b47a4873172d23402476421ec24447c3f75"
            },
            {
                "title": "Counter-RAPTOR: Safeguarding Tor against active routing attacks",
                "abstract": "Tor is vulnerable to network-level adversaries who can observe both ends of the communication to deanonymize users. Recent work has shown that Tor is susceptible to the previously unknown active BGP routing attacks, called RAPTOR attacks, which expose Tor users to more network-level adversaries. In this paper, we aim to mitigate and detect such active routing attacks against Tor. First, we present a new measurement study on the resilience of the Tor network to active BGP prefix attacks. We show that ASes with high Tor bandwidth can be less resilient to attacks than other ASes. Second, we present a new Tor guard relay selection algorithm that incorporates resilience of relays into consideration to proactively mitigate such attacks. We show that the algorithm successfully improves the security for Tor clients by up to 36% on average (up to 166% for certain clients). Finally, we build a live BGP monitoring system that can detect routing anomalies on the Tor network in real time by performing an AS origin check and novel detection analytics. Our monitoring system successfully detects simulated attacks that are modeled after multiple known attack types as well as a real-world hijack attack (performed by us), while having low false positive rates.",
                "paper_link": "https://www.semanticscholar.org/paper/08d0d3e3ecb7914e8b29edbca4ab70f39c3822b7"
            },
            {
                "title": "Securing internet applications from routing attacks",
                "abstract": "Application-layer and network-layer defenses are critical for fortifying routing attacks.",
                "paper_link": "https://www.semanticscholar.org/paper/13574f155f520309128ad7710025efcbf24fbb60"
            },
            {
                "title": "Tempest: Temporal dynamics in anonymity systems",
                "abstract": "Abstract Many recent proposals for anonymous communication omit from their security analyses a consideration of the effects of time on important system components. In practice, many components of anonymity systems, such as the client location and network structure, exhibit changes and patterns over time. In this paper, we focus on the effect of such temporal dynamics on the security of anonymity networks. We present Tempest, a suite of novel attacks based on (1) client mobility, (2) usage patterns, and (3) changes in the underlying network routing. Using experimental analysis on real-world datasets, we demonstrate that these temporal attacks degrade user privacy across a wide range of anonymity networks, including deployed systems such as Tor; pathselection protocols for Tor such as DeNASA, TAPS, and Counter-RAPTOR; and network-layer anonymity protocols for Internet routing such as Dovetail and HORNET. The degradation is in some cases surprisingly severe. For example, a single host failure or network route change could quickly and with high certainty identify the client\u2019s ISP to a malicious host or ISP. The adversary behind each attack is relatively weak \u2013 generally passive and in control of one network location or a small number of hosts. Our findings suggest that designers of anonymity systems should rigorously consider the impact of temporal dynamics when analyzing anonymity.",
                "paper_link": "https://www.semanticscholar.org/paper/e0dbebd57f04c2422d57bdac5ef47246b2d4566d"
            },
            {
                "title": "DPSelect: a differential privacy based guard relay selection algorithm for Tor",
                "abstract": "Abstract Recent work has shown that Tor is vulnerable to attacks that manipulate inter-domain routing to compromise user privacy. Proposed solutions such as Counter-RAPTOR [29] attempt to ameliorate this issue by favoring Tor entry relays that have high resilience to these attacks. However, because these defenses bias Tor path selection on the identity of the client, they invariably leak probabilistic information about client identities. In this work, we make the following contributions. First, we identify a novel means to quantify privacy leakage in guard selection algorithms using the metric of Max-Divergence. Max-Divergence ensures that probabilistic privacy loss is within strict bounds while also providing composability over time. Second, we utilize Max-Divergence and multiple notions of entropy to understand privacy loss in the worst-case for Counter-RAPTOR. Our worst-case analysis provides a fresh perspective to the field, as prior work such as Counter-RAPTOR only analyzed average case-privacy loss. Third, we propose modifications to Counter-RAPTOR that incorporate worst-case Max-Divergence in its design. Specifically, we utilize the exponential mechanism (a mechanism for differential privacy) to guarantee a worst-case bound on Max-Divergence/privacy loss. For the quality function used in the exponential mechanism, we show that a Monte-Carlo sampling-based method for stochastic optimization can be used to improve multi-dimensional trade-offs between security, privacy, and performance. Finally, we demonstrate that compared to Counter-RAPTOR, our approach achieves an 83% decrease in Max-Divergence after one guard selection and a 245% increase in worst-case Shannon entropy after 5 guard selections. Notably, experimental evaluations using the Shadow emulator shows that our approach provides these privacy benefits with minimal impact on system performance.",
                "paper_link": "https://www.semanticscholar.org/paper/f912c614c0edda87d4eca53a61b1e12047c460d1"
            },
            {
                "title": "Using BGP to acquire bogus TLS certificates",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Countering Malicious Processes with Process-DNS Association.",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/30a3f6235011d12d9e11220ae279d003a741133e"
            },
            {
                "title": "Proof-of-Concept Study for a Roadway Reservation System",
                "abstract": "Learning from the success of reservation systems in industries, including the airline and hotel industries, transportation engineers have considered the use of a roadway reservation concept. Vehicles are traditionally allowed to use freeways on a first-come, first-served basis. In the proposed roadway reservation system, vehicles reserve a spot on the freeway in advance, and this reservation allows them to use some segments of the freeway within a certain period of time. Because the number of reservations issued by the roadway reservation system is controlled, this concept can be used to maintain a certain level of service on a freeway. Emerging vehicle-to-infrastructure and vehicle-to-vehicle communication technologies make the roadway reservation concept feasible. This study conducted a proof-of-concept test to investigate the potential benefits of a roadway reservation system. A VISSIM traffic network with a 20-mi-long two-lane freeway and some arterials served as the simulation test bed. The reservation algorithm was applied to a carefully designed baseline, and the results of the reservation scenario and the baseline scenario were compared. The reservation scenario outperformed the baseline scenario according to total delay time and emissions. When travel demand was 30% higher than capacity, the total delay time was 58.6% less and carbon dioxide emissions were 18.3% less in the reservation scenario than in the baseline scenario. Although this proof-of-concept study did not consider some practical issues, the proposed roadway reservation system outperformed the baseline scenario to such an extent that it provides strong evidence that the proposed reservation system has a promising future and deserves more attention, including feasibility testing for implementation.",
                "paper_link": "https://www.semanticscholar.org/paper/f2473502878d92340df93db50274bb581a7f76ee"
            },
            {
                "title": "Detecting Malware Injection with Program-DNS Behavior",
                "abstract": "Analyzing the DNS traffic of Internet hosts has been a successful technique to counter cyberattacks and identify connections to malicious domains. However, recent stealthy attacks hide malicious activities within seemingly legitimate connections to popular web services made by benign programs. Traditional DNS monitoring and signature-based detection techniques are ineffective against such attacks. To tackle this challenge, we present a new program-level approach that can effectively detect such stealthy attacks. Our method builds a fine-grained Program-DNS profile for each benign program that characterizes what should be the \u201cexpected\u201d DNS behavior. We find that malware-injected processes have DNS activities which significantly deviate from the Program-DNS profile of the benign program. We then develop six novel features based on the Program-DNS profile, and evaluate the features on a dataset of over 130 million DNS requests collected from a real-world enterprise and 8 million requests from malware-samples executed in a sandbox environment. We compare our detection results with that of previously-proposed features and demonstrate that our new features successfully detect 190 malware-injected processes which fail to be detected by previously-proposed features. Overall, our study demonstrates that fine-grained Program-DNS profiles can provide meaningful and effective features in building detectors for attack campaigns that bypass existing detection systems.",
                "paper_link": "https://www.semanticscholar.org/paper/d4abb321be8b427fd29f01504a6cde8fb9f496b2"
            },
            {
                "title": "RAPID: real-time alert investigation with context-aware prioritization for efficient threat discovery",
                "abstract": "Alerts reported by intrusion detection systems (IDSes) are often the starting points for attack campaign discovery and response procedures. However, the sheer number of alerts compared to the number of real attacks, along with the complexity of alert investigations, poses a challenge to achieving effective alert triage with limited computational resources. Automated procedures and human analysts could suffer from the burden of analyzing floods of alerts, and fail to respond to critical alerts promptly. To scale out the alert processing capability in enterprises, we present RAPID, a real-time alert investigation system to aid analysts perform provenance analysis tasks around alerts in an efficient and collaborative manner. RAPID is built based on two key insights: 1) space and time efficiency of alert investigations can be improved by avoiding the significant overlap between alert triage tasks; 2) prioritization of alert triage tasks should be dynamic to adapt to the newly discovered context. In doing so, RAPID maximizes the utilization of limited computation resources and time, and reacts to the most critical reasoning steps in a timely manner. More specifically, RAPID employs an interruptible tracking algorithm that efficiently uncovers the causal connections between alerts and propagates priorities based on the connections. Unlike prior work, RAPID does not rely on knowledge of existing threat ontologies and focuses on providing a general concurrent alert investigation platform with provenance analysis capabilities. We evaluate RAPID on a 1TB dataset from DARPA Transparent Computing (TC) program with 411 million events, including three attack campaigns. The results show that RAPID is able to improve space efficiency by up to three orders of magnitude and reduce the time of alert provenance analysis to discover all the major attack traces by up to 99%.",
                "paper_link": "https://www.semanticscholar.org/paper/c1441746a1865a827ecff8ef16d5c8033bea819f"
            },
            {
                "title": "Host level detect mechanism for malicious DNS activities",
                "abstract": "The server resources are abnormally consumed by the attackers using the denial-if service attacks. Denial-of- Service denies a victim from providing or receiving normal services. Distributed Denial of Service (DDoS) Attacks are generated in a \u201cmany to one\u201d dimension. In DDoS attack model large number of compromised host are gathered to send useless service requests, packets at the same time. Attackers select the hidden channel model for their communication. A C&C channel for a botnet needs to be reliable, redundant, noncentralized and easily disguised as legitimate traffic. Domain Name Service (DNS) provides a distributed infrastructure for storing, updating and disseminating data. DNS is targeted as a stealthy botnet command-and-control channel. Malicious DNS activities are hiding at the network level. Exponentially Distributed Query and Piggybacking Query attacks are detected using the markov chain analysis and statistical analysis mechanism. Probability distribution based analysis model is used to detect automatic domain flux attacks. DNS tunneling technique is used for transmitting arbitrary data via DNS protocol. The attack detection system is improved with security and privacy factors. Automated anomaly detection is adapted to the system. Navy bayesian classification technique is integrated to the system. Small query analysis mechanism is integrated with the system.",
                "paper_link": "https://www.semanticscholar.org/paper/fad77d03aa35f765e4c58f9f67b8fd414d22815d"
            },
            {
                "title": "Creating a Secure Underlay for the Internet",
                "abstract": "Adversaries can exploit inter-domain routing vulnerabilities to intercept communication and compromise the security of critical Internet applications. Meanwhile the deployment of secure routing solutions such as Border Gateway Protocol Security (BGPsec) and Scalability, Control and Isolation On Next-generation networks (SCION) are still limited. How can we leverage emerging secure routing backbones and extend their security properties to the broader Internet? We design and deploy an architecture to bootstrap secure routing. Our key insight is to abstract the secure routing backbone as a virtual Autonomous System (AS), called Secure Backbone AS (SBAS). While SBAS appears as one AS to the Internet, it is a federated network where routes are exchanged between participants using a secure backbone. SBAS makes BGP announcements for its customers' IP prefixes at multiple locations (referred to as Points of Presence or PoPs) allowing traffic from non-participating hosts to be routed to a nearby SBAS PoP (where it is then routed over the secure backbone to the true prefix owner). In this manner, we are the first to integrate a federated secure non-BGP routing backbone with the BGP-speaking Internet. We present a real-world deployment of our architecture that uses SCIONLab to emulate the secure backbone and the PEERING framework to make BGP announcements to the Internet. A combination of real-world attacks and Internet-scale simulations shows that SBAS substantially reduces the threat of routing attacks. Finally, we survey network operators to better understand optimal governance and incentive models.",
                "paper_link": "https://www.semanticscholar.org/paper/a78a9d6b91a815ca8b00ce72786bb2d0fa15e452"
            },
            {
                "title": "Exploring the Ecosystem of DNS HTTPS Resource Records: An End-to-End Perspective",
                "abstract": "The DNS HTTPS resource record is a new DNS record type designed for the delivery of configuration information and parameters required to initiate connections to HTTPS network services. In addition, it is a key enabler for TLS Encrypted ClientHello (ECH) by providing the cryptographic keying material needed to encrypt the initial exchange. To understand the adoption of this new DNS HTTPS record, we perform a longitudinal study on the server-side deployment of DNS HTTPS for Tranco top million domains, as well as an analysis of the client-side support for DNS HTTPS through snapshots from major browsers. To the best of our knowledge, our work is the first longitudinal study on DNS HTTPS server deployment, and the first known study on client-side support for DNS HTTPS. Despite the rapidly growing trend of DNS HTTPS adoption, our study highlights challenges and concerns in the deployment by both servers and clients, such as the complexity in properly maintaining HTTPS records and connection failure in browsers when the HTTPS record is not properly configured.",
                "paper_link": "https://www.semanticscholar.org/paper/54c1ef9c1b26ce4f96a23859e3417340af5b6a3f"
            },
            {
                "title": "Mutual TLS in Practice: A Deep Dive into Certificate Configurations and Privacy Issues",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/8017ef0ed724a7dcbb658ab4dcfe932f2201445d"
            },
            {
                "title": "Deciphering the Digital Veil: Exploring the Ecosystem of DNS HTTPS Resource Records",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Global Analysis with Aggregation-based Beaconing Detection across Large Campus Networks",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Behind the Scenes: Uncovering TLS and Server Certificate Practice of IoT Device Vendors in the Wild",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Enhancing Anonymity Systems under Network and User Dynamics",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/69c7fc8b19c7bada1901300eba675853aeb22ece"
            }
        ]
    },
    {
        "Professor": "Anil Vullikanti",
        "Papers": [
            {
                "title": "Modelling disease outbreaks in realistic urban social networks",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/1413dbfbbae1b59d52656db3dc48a4ee278e082f"
            },
            {
                "title": "Modeling targeted layered containment of an influenza pandemic in the United States",
                "abstract": "Planning a response to an outbreak of a pandemic strain of influenza is a high public health priority. Three research groups using different individual-based, stochastic simulation models have examined the consequences of intervention strategies chosen in consultation with U.S. public health workers. The first goal is to simulate the effectiveness of a set of potentially feasible intervention strategies. Combinations called targeted layered containment (TLC) of influenza antiviral treatment and prophylaxis and nonpharmaceutical interventions of quarantine, isolation, school closure, community social distancing, and workplace social distancing are considered. The second goal is to examine the robustness of the results to model assumptions. The comparisons focus on a pandemic outbreak in a population similar to that of Chicago, with \u22488.6 million people. The simulations suggest that at the expected transmissibility of a pandemic strain, timely implementation of a combination of targeted household antiviral prophylaxis, and social distancing measures could substantially lower the illness attack rate before a highly efficacious vaccine could become available. Timely initiation of measures and school closure play important roles. Because of the current lack of data on which to base such models, further field research is recommended to learn more about the sources of transmission and the effectiveness of social distancing measures in reducing influenza transmission.",
                "paper_link": "https://www.semanticscholar.org/paper/50c9404f4acb70b43d3ac3a22c2499dbba7379e9"
            },
            {
                "title": "Mobile Data Offloading through Opportunistic Communications and Social Participation",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Cellular traffic offloading through opportunistic communications: a case study",
                "abstract": "Due to the increasing popularity of various applications for smartphones, 3G networks are currently overloaded by mobile data traffic. Offloading cellular traffic through opportunistic communications is a promising solution to partially solve this problem, because there is no monetary cost for it. As a case study, we investigate the target-set selection problem for information delivery in the emerging Mobile Social Networks (MoSoNets). We propose to exploit opportunistic communications to facilitate the information dissemination and thus reduce the amount of cellular traffic. In particular, we study how to select the target set with only k users, such that we can minimize the cellular data traffic.\n In this scenario, initially the content service providers deliver information over cellular networks to only users in the target set. Then through opportunistic communications, target-users will further propagate the information among all the subscribed users. Finally, service providers will send the information to users who fail to receive it before the delivery deadline (i.e., delay-tolerance threshold). We propose three algorithms, called Greedy, Heuristic, and Random, for this problem and evaluate their performance through an extensive trace-driven simulation study. The simulation results verify the efficiency of these algorithms for both synthetic and real-world mobility traces. For example, the Heuristic algorithm can offload cellular traffic by up to 73.66% for a real-world mobility trace.",
                "paper_link": "https://www.semanticscholar.org/paper/af6d4fd231cd5b903ec02197e643272c854daa15"
            },
            {
                "title": "'Beating the news' with EMBERS: Forecasting Civil Unrest using Open Source Indicators",
                "abstract": "We describe the design, implementation, and evaluation of EMBERS, an automated, 24x7 continuous system for forecasting civil unrest across 10 countries of Latin America using open source indicators such as tweets, news sources, blogs, economic indicators, and other data sources. Unlike retrospective studies, EMBERS has been making forecasts into the future since Nov 2012 which have been (and continue to be) evaluated by an independent T&E team (MITRE). Of note, EMBERS has successfully forecast the June 2013 protests in Brazil and Feb 2014 violent protests in Venezuela. We outline the system architecture of EMBERS, individual models that leverage specific data sources, and a fusion and suppression engine that supports trading off specific evaluation criteria. EMBERS also provides an audit trail interface that enables the investigation of why specific predictions were made along with the data utilized for forecasting. Through numerous evaluations, we demonstrate the superiority of EMBERS over baserate methods and its capability to forecast significant societal happenings.",
                "paper_link": "https://www.semanticscholar.org/paper/6306b408f462e83e56159f785fb92e96f040d49b"
            },
            {
                "title": "Algorithmic aspects of capacity in wireless networks",
                "abstract": "This paper considers two inter-related questions: (i) Given a wireless ad-hoc network and a collection of source-destination pairs {(si,ti)}, what is the maximum throughput capacity of the network, i.e. the rate at which data from the sources to their corresponding destinations can be transferred in the network? (ii) Can network protocols be designed that jointly route the packets and schedule transmissions at rates close to the maximum throughput capacity? Much of the earlier work focused on random instances and proved analytical lower and upper bounds on the maximum throughput capacity. Here, in contrast, we consider arbitrary wireless networks. Further, we study the algorithmic aspects of the above questions: the goal is to design provably good algorithms for arbitrary instances. We develop analytical performance evaluation models and distributed algorithms for routing and scheduling which incorporate fairness, energy and dilation (path-length) requirements and provide a unified framework for utilizing the network close to its maximum throughput capacity.Motivated by certain popular wireless protocols used in practice, we also explore \"shortest-path like\" path selection strategies which maximize the network throughput. The theoretical results naturally suggest an interesting class of congestion aware link metrics which can be directly plugged into several existing routing protocols such as AODV, DSR, etc. We complement the theoretical analysis with extensive simulations. The results indicate that routes obtained using our congestion aware link metrics consistently yield higher throughput than hop-count based shortest path metrics.",
                "paper_link": "https://www.semanticscholar.org/paper/480c866452a0bcc113f6b921baa9bfbf6df6aaec"
            },
            {
                "title": "EpiFast: a fast algorithm for large scale realistic epidemic simulations on distributed memory systems",
                "abstract": "Large scale realistic epidemic simulations have recently become an increasingly important application of high-performance computing. We propose a parallel algorithm, EpiFast, based on a novel interpretation of the stochastic disease propagation in a contact network. We implement it using a master-slave computation model which allows scalability on distributed memory systems. EpiFast runs extremely fast for realistic simulations that involve: (i) large populations consisting of millions of individuals and their heterogeneous details, (ii) dynamic interactions between the disease propagation, the individual behaviors, and the exogenous interventions, as well as (iii) large number of replicated runs necessary for statistically sound estimates about the stochastic epidemic evolution. We find that EpiFast runs several magnitude faster than another comparable simulation tool while delivering similar results. EpiFast has been tested on commodity clusters as well as SGI shared memory machines. For a fixed experiment, if given more computing resources, it scales automatically and runs faster. Finally, EpiFast has been used as the major simulation engine in real studies with rather sophisticated settings to evaluate various dynamic interventions and to provide decision support for public health policy makers.",
                "paper_link": "https://www.semanticscholar.org/paper/30365a90b6925ea39356c0e8375be1447ced21e3"
            },
            {
                "title": "(2009). EpiFast: A fast algorithm for large scale realistic epidemic simulations on distributed memory systems",
                "abstract": "Large scale realistic epidemic simulations have recently become an increasingly important application of high-performance computing. We propose a parallel algorithm, EpiFast, based on a novel interpretation of the stochastic disease propagation in a contact network. We implement it using a master-slave computation model which allows scalability on distributed memory systems. EpiFast runs extremely fast for realistic simulations that involve: (i) large populations consisting of millions of individuals and their heterogeneous details, (ii) dynamic interactions between the disease propagation, the individual behaviors, and the exogenous interventions, as well as (iii) large number of replicated runs necessary for statistically sound estimates about the stochastic epidemic evolution. We find that EpiFast runs several magnitude faster than another comparable simulation tool while delivering similar results. EpiFast has been tested on commodity clusters as well as SGI shared memory machines. For a fixed experiment, if given more computing resources, it scales automatically and runs faster. Finally, EpiFast has been used as the major simulation engine in real studies with rather sophisticated settings to evaluate various dynamic interventions and to provide decision support for public health policy makers.",
                "paper_link": "https://www.semanticscholar.org/paper/30365a90b6925ea39356c0e8375be1447ced21e3"
            },
            {
                "title": "Generation and analysis of large synthetic social contact networks",
                "abstract": "We describe \u201cfirst principles\u201d based methods for developing synthetic urban and national scale social contact networks. Unlike simple random graph techniques, these methods use real world data sources and combine them with behavioral and social theories to synthesize networks. We develop a synthetic population for the United States modeling every individual in the population including household structure, demographics and a 24-hour activity sequence. The process involves collecting and manipulating public and proprietary data sets integrated into a common architecture for data exchange and then using these data sets to generate new relations. A social contact network is derived from the synthetic population based on physical co-location of interacting persons. We use graph measures to compare and contrast the structural characteristics of the social networks that span different urban regions. We then simulate diffusion processes on these networks and analyze similarities and differences in the structure of the networks.",
                "paper_link": "https://www.semanticscholar.org/paper/06ed109b9c92b9a87e81ec98fc75fbe0ef7ca86f"
            },
            {
                "title": "Mathematical models for covid-19 pandemic: a comparative analysis",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/dd74a3a343529174fe7c6485723cf2d5911c18ed"
            },
            {
                "title": "The distance-2 matching problem and its relationship to the MAC-layer capacity of ad hoc wireless networks",
                "abstract": "We consider the problem of determining the maximum capacity of the media access (MAC) layer in wireless ad hoc networks. Due to spatial contention for the shared wireless medium, not all nodes can concurrently transmit packets to each other in these networks. The maximum number of possible concurrent transmissions is, therefore, an estimate of the maximum network capacity, and depends on the MAC protocol being used. We show that for a large class of MAC protocols based on virtual carrier sensing using RTS/CTS messages, which includes the popular IEEE 802.11 standard, this problem may be modeled as a maximum Distance-2 matching ( D2EMIS) in the underlying wireless network: Given a graph G(V,E), find a set of edges E'/spl sube/E such that no two edges in E' are connected by another edge in E. D2EMIS is NP-complete. Our primary goal is to show that it can be approximated efficiently in networks that arise in practice. We do this by focusing on an admittedly simplistic, yet natural, graph-theoretic model for ad hoc wireless networks based on disk graphs, where a node can reach all other nodes within some distance (nodes may have unequal reach distances). We show that our approximation yields good capacity bounds. Our work is the first attempt at characterizing an important \"maximum\" measure of wireless network capacity, and can be used to shed light on previous topology formation protocols like Span and GAF that attempt to produce \"good\" or \"capacity-preserving\" topologies, while allowing nodes to alternate between sleep and awake states. Our work shows an efficient way to compute an upper bound on maximum wireless network capacity, thereby allowing topology formation algorithms to determine how close they are to optimal. We also outline a distributed algorithm for the problem for unit disk graphs, and briefly discuss extensions of our results to: 1) different node interference models; 2) directional antennas; and 3) other transceiver connectivity structures besides disk graphs.",
                "paper_link": "https://www.semanticscholar.org/paper/f392d0c33b12f655dd109774ea1e054fecea6235"
            },
            {
                "title": "Using data-driven agent-based models for forecasting emerging infectious diseases",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/43439ffdee4cdbe55085fc778a26d1badbd187cc"
            },
            {
                "title": "Structural and algorithmic aspects of massive social networks",
                "abstract": "We study the algorithmic and structural properties of very large, realistic social contact networks. We consider the social network for the city of Portland, Oregon, USA, developed as a part of the TRANSIMS/EpiSims project at the Los Alamos National Laboratory. The most expressive social contact network is a bipartite graph, with two types of nodes: people and locations; edges represent people visiting locations on a typical day. Three types of results are presented. (i) Our empirical results show that many basic characteristics of the dataset are well-modeled by a random graph approach suggested by Fan Chung Graham and Lincoln Lu (the CL-model), with a power-law degree distribution. (ii) We obtain fast approximation algorithms for computing basic structural properties such as clustering coefficients and shortest paths distribution. We also study the dominating set problem for such networks; this problem arose in connection with optimal sensor-placement for disease-detection. We present a fast approximation algorithm for computing near-optimal dominating sets. (iii) Given the close approximations provided by the CL-model to our original dataset and the large data-volume, we investigate fast methods for generating such random graphs. We present methods that can generate such a random network in near-linear time, and show that these variants asymptotically share many key features of the CL-model, and also match the Portland social network.The structural results have been used to study the impact of policy decisions for controlling large-scale epidemics in urban environments.",
                "paper_link": "https://www.semanticscholar.org/paper/0ab6bbe5734068febe4119246b7e007e9197dc3d"
            },
            {
                "title": "Computational epidemiology",
                "abstract": "The coronavirus disease (COVID-19) continued to strike as a highly infectious and fast-spreading disease in 2020 and 2021. As the research community actively responded to this pandemic, we saw the release of many COVID-19-related datasets and visualization dashboards. However, existing resources are insufficient to support multiscale and multifaceted modeling or simulation, which is suggested to be important by the computational epidemiology literature. This work presents a curated multiscale geospatial dataset with an interactive visualization dashboard under the context of COVID-19. This open dataset will allow researchers to conduct numerous projects or analyses relating to COVID-19 or simply geospatial-related scientific studies. The interactive visualization platform enables users to visualize the spread of the disease at different scales (e.g., country level to individual neighborhoods), and allows users to interact with the policies enforced at these scales (e.g., the closure of borders and lockdowns) to observe their impacts on the epidemiology.",
                "paper_link": "https://www.semanticscholar.org/paper/5e705ce89d6c8e21adaa56857297bcb3bbc1d326"
            },
            {
                "title": "Equilibria in topology control games for ad hoc networks",
                "abstract": "We study topology control problems in ad hoc networks where network nodes get to choose their power levels in order to ensure desired connectivity properties. Unlike most other work on this topic, we assume that the network nodes are owned by different entities, whose only goal is to maximize their own utility that they get out of the network without considering the overall performance of the network. Game theory is the appropriate tool to study such selfish nodes: we define several topology control games in which the nodes need to choose power levels in order to connect to other nodes in the network to reach their communication partners while at the same time minimizing their costs. We study Nash equilibria and show that\u2014among the games we define\u2014these can only be guaranteed to exist if each network node is required to be connected to all other nodes (we call this the Strong Connectivity Game). For a variation called Connectivity Game, where each node is only required to be connected (possibly via intermediate nodes) to a given set of nodes, we show that Nash equilibria do not necessarily exist. We further study how to find Nash equilibria with incentive-compatible algorithms and compare the cost of Nash equilibria to the cost of a social optimum, which is a radius assignment that minimizes the total cost in a network where nodes cooperate. We also study variations of the games; one where nodes not only have to be connected, but k-connected, and one that we call the Reachability Game, where nodes have to reach as many other nodes as possible, while keeping costs low. We extend our study of the Strong Connectivity Game and the Connectivity Game to wireless networks with directional antennas and wireline networks, where nodes need to choose neighbors to which they will pay a link. Our work is a first step towards game-theoretic analyses of topology control in wireless and wireline networks.",
                "paper_link": "https://www.semanticscholar.org/paper/9587b1daa75546699fc8118f7a83c742e45a82a4"
            },
            {
                "title": "Distributed algorithms for constructing approximate minimum spanning trees in wireless sensor networks",
                "abstract": "While there are distributed algorithms for the MST problem, these algorithms require relatively large number of messages and time; this makes these algorithms impractical for resource-constrained networks such as ad hoc wireless sensor networks. In such networks, a sensor has very limited power, and any algorithm needs to be simple, local, and energy efficient for being practical. Motivated by these considerations, we design and analyze a class of simple and local distributed algorithms called nearest neighbor tree (NNT) algorithms for energy-efficient construction of MSTs in a wireless ad hoc setting. We assume that the nodes are uniformly distributed in a unit square and show provable bounds on the performance with respect to both the quality of the spanning tree produced and the energy needed to construct them. In particular, we show that NNT produces a close approximation to the MST, and they can be maintained dynamically with polylogarithmic number of rearrangements under node insertions/deletions. We also perform extensive simulations of our algorithms. We tested our algorithms on both uniformly random distributions of nodes, and on a realistic distributions of nodes in an urban setting. Simulations validate the theoretical results and show that the bounds are much better in practice.",
                "paper_link": "https://www.semanticscholar.org/paper/55c72f4ce8f0ba8beeb8c7be5c5ae1b67de1815f"
            },
            {
                "title": "Distributed Algorithms for Constructing Approximate Minimum Spanning Trees in Wireless Networks",
                "abstract": "While there are distributed algorithms for the Minimum Spanning Tree (MST) problem, these algorithms require relatively large number of messages and time, and are fairly involved, making them impractical for resource-constrained networks such as wireless sensor networks. In such networks, a sensor has very limited power, and any algorithm needs to be simple, local, and energy efficient. Motivated by these considerations, we design and analyze a class of simple and local distributed algorithms called Nearest Neighbor Tree(NNT) algorithms for energy-efficient construction of an approximate MST in wireless networks. Assuming that the nodes are unifor mly distributed, we show provable bounds on both thequality of the spanning tree produced and theenergy needed to construct them. We show that while NNT produces a close approximation to the MST, it consumes asymptotically less energy than the classi cal message-optimal distributed MST algorithm due to Gallager , Humblet, and Spira. Further, the NNTs can be maintained dynamically with polylogarithmic rearrangements under node insertions/deletions. We also perform extensive simulati ons, which show that the bounds are much better in practice. Our results, to the best of our knowledge, demonstrate the first tradeoff between the quality of approximation and the energy required for building spanning trees on wireless networks, and motiv ate similar considerations for other important problems.",
                "paper_link": "https://www.semanticscholar.org/paper/a0bc8da461c6587b9eb8239be46229ca0cb24682"
            },
            {
                "title": "Optimal constrained graph exploration",
                "abstract": "We address the problem of exploring an unknown graph <i>G</i> = (<i>V</i>, <i>E</i>) from a given start node <i>s</i> with either a tethered robot or a robot with a fuel tank of limited capacity, the former being a tighter constraint. In both variations of the problem, the robot can only move along the edges of the graph, i.e, it cannot jump between non-adjacent vertices. In the tethered robot case, if the tether (rope) has length <i>l</i>, then the robot must remain within distance <i>l</i> from the start node <i>s</i>. In the second variation, a fuel tank of limited capacity forces the robot to return to <i>s</i> after traversing <i>C</i> edges. The efficiency of algorithms for both variations of the problem is measured by the number of edges traversed during the exploration. We present an algorithm for a tethered robot which explores the graph in <i>&Ogr;</i>(\u00a6<i>E</i>\u00a6) edge traversals. The problem of exploration using a robot with a limited fuel tank capacity can be solved with a simple reduction from the tethered robot case and also yields a <i>&Ogr;</i>(\u00a6<i>E</i>\u00a6) algorithm. This improves on the previous best known bound of <i>&Ogr;</i>(\u00a6<i>E</i>\u00a6 + \u00a6<i>V</i>\u00a6log <sup>2</sup>\u00a6<i>V</i>\u00a6) in [4]. Since the lower bound for the graph exploration problems is \u00a6<i>E</i>\u00a6, our algorithm is optimal, thus answering the open problem of Awerbuch, Betke, Rivest, and Singh [3].",
                "paper_link": "https://www.semanticscholar.org/paper/ededb526b0f3f723cd520782dec122e5cbdd6a21"
            },
            {
                "title": "Optimal constrained graph exploration",
                "abstract": "We address the problem of exploring an unknown graph <i>G</i> = (<i>V</i>, <i>E</i>) from a given start node <i>s</i> with either a tethered robot or a robot with a fuel tank of limited capacity, the former being a tighter constraint. In both variations of the problem, the robot can only move along the edges of the graph, i.e, it cannot jump between non-adjacent vertices. In the tethered robot case, if the tether (rope) has length <i>l</i>, then the robot must remain within distance <i>l</i> from the start node <i>s</i>. In the second variation, a fuel tank of limited capacity forces the robot to return to <i>s</i> after traversing <i>C</i> edges. The efficiency of algorithms for both variations of the problem is measured by the number of edges traversed during the exploration. We present an algorithm for a tethered robot which explores the graph in <i>&Ogr;</i>(\u00a6<i>E</i>\u00a6) edge traversals. The problem of exploration using a robot with a limited fuel tank capacity can be solved with a simple reduction from the tethered robot case and also yields a <i>&Ogr;</i>(\u00a6<i>E</i>\u00a6) algorithm. This improves on the previous best known bound of <i>&Ogr;</i>(\u00a6<i>E</i>\u00a6 + \u00a6<i>V</i>\u00a6log <sup>2</sup>\u00a6<i>V</i>\u00a6) in [4]. Since the lower bound for the graph exploration problems is \u00a6<i>E</i>\u00a6, our algorithm is optimal, thus answering the open problem of Awerbuch, Betke, Rivest, and Singh [3].",
                "paper_link": "https://www.semanticscholar.org/paper/ededb526b0f3f723cd520782dec122e5cbdd6a21"
            },
            {
                "title": "Modeling of future COVID-19 cases, hospitalizations, and deaths, by vaccination rates and nonpharmaceutical intervention scenarios\u2014United States, April\u2013September 2021",
                "abstract": "After a period of rapidly declining U.S. COVID-19 incidence during January-March 2021, increases occurred in several jurisdictions (1,2) despite the rapid rollout of a large-scale vaccination program. This increase coincided with the spread of more transmissible variants of SARS-CoV-2, the virus that causes COVID-19, including B.1.1.7 (1,3) and relaxation of COVID-19 prevention strategies such as those for businesses, large-scale gatherings, and educational activities. To provide long-term projections of potential trends in COVID-19 cases, hospitalizations, and deaths, COVID-19 Scenario Modeling Hub teams used a multiple-model approach comprising six models to assess the potential course of COVID-19 in the United States across four scenarios with different vaccination coverage rates and effectiveness estimates and strength and implementation of nonpharmaceutical interventions (NPIs) (public health policies, such as physical distancing and masking) over a 6-month period (April-September 2021) using data available through March 27, 2021 (4). Among the four scenarios, an accelerated decline in NPI adherence (which encapsulates NPI mandates and population behavior) was shown to undermine vaccination-related gains over the subsequent 2-3 months and, in combination with increased transmissibility of new variants, could lead to surges in cases, hospitalizations, and deaths. A sharp decline in cases was projected by July 2021, with a faster decline in the high-vaccination scenarios. High vaccination rates and compliance with public health prevention measures are essential to control the COVID-19 pandemic and to prevent surges in hospitalizations and deaths in the coming months.",
                "paper_link": "https://www.semanticscholar.org/paper/7fe0b8461a767f95e5c67c1cf6069441fec76cd4"
            }
        ]
    },
    {
        "Professor": "Tianhao Wang",
        "Papers": [
            {
                "title": "Locally differentially private protocols for frequency estimation",
                "abstract": "Protocols satisfying Local Differential Privacy (LDP) enable parties to collect aggregate information about a population while protecting each user\u2019s privacy, without relying on a trusted third party. LDP protocols (such as Google\u2019s RAPPOR) have been deployed in real-world scenarios. In these protocols, a user encodes his private information and perturbs the encoded value locally before sending it to an aggregator, who combines values that users contribute to infer statistics about the population. In this paper, we introduce a framework that generalizes several LDP protocols proposed in the literature. Our framework yields a simple and fast aggregation algorithm, whose accuracy can be precisely analyzed. Our in-depth analysis enables us to choose optimal parameters, resulting in two new protocols (i.e., Optimized Unary Encoding and Optimized Local Hashing) that provide better utility than protocols previously proposed. We present precise conditions for when each proposed protocol should be used, and perform experiments that demonstrate the advantage of our proposed protocols.",
                "paper_link": "https://www.semanticscholar.org/paper/659996fa1d5c8eb8d5d92e94318156248cf33176"
            },
            {
                "title": "Privacy at scale: Local differential privacy in practice",
                "abstract": "Local differential privacy (LDP), where users randomly perturb their inputs to provide plausible deniability of their data without the need for a trusted party, has been adopted recently by several major technology organizations, including Google, Apple and Microsoft. This tutorial aims to introduce the key technical underpinnings of these deployed systems, to survey current research that addresses related problems within the LDP model, and to identify relevant open problems and research directions for the community.",
                "paper_link": "https://www.semanticscholar.org/paper/4545f59bf5914695d2742562838ab4b063dc279d"
            },
            {
                "title": "When machine unlearning jeopardizes privacy",
                "abstract": "The right to be forgotten states that a data owner has the right to erase their data from an entity storing it. In the context of machine learning (ML), the right to be forgotten requires an ML model owner to remove the data owner's data from the training set used to build the ML model, a process known asmachine unlearning. While originally designed to protect the privacy of the data owner, we argue that machine unlearning may leave some imprint of the data in the ML model and thus create unintended privacy risks. In this paper, we perform the first study on investigating the unintended information leakage caused by machine unlearning. We propose a novel membership inference attack that leverages the different outputs of an ML model's two versions to infer whether a target sample is part of the training set of the original model but out of the training set of the corresponding unlearned model. Our experiments demonstrate that the proposed membership inference attack achieves strong performance. More importantly, we show that our attack in multiple cases outperforms the classical membership inference attack on the original ML model, which indicates that machine unlearning can have counterproductive effects on privacy. We notice that the privacy degradation is especially significant for well-generalized ML models where classical membership inference does not perform well. We further investigate four mechanisms to mitigate the newly discovered privacy risks and show that releasing the predicted label only, temperature scaling, and differential privacy are effective. We believe that our results can help improve privacy protection in practical implementations of machine unlearning. \\footnoteOur code is available at \\urlhttps://github.com/MinChen00/UnlearningLeaks.",
                "paper_link": "https://www.semanticscholar.org/paper/4ebdc89a4dac0d05d8a2567e3b7849be198cb0ba"
            },
            {
                "title": "Locally differentially private frequent itemset mining",
                "abstract": "The notion of Local Differential Privacy (LDP) enables users to respond to sensitive questions while preserving their privacy. The basic LDP frequent oracle (FO) protocol enables an aggregator to estimate the frequency of any value. But when each user has a set of values, one needs an additional padding and sampling step to find the frequent values and estimate their frequencies. In this paper, we formally define such padding and sample based frequency oracles (PSFO). We further identify the privacy amplification property in PSFO. As a result, we propose SVIM, a protocol for finding frequent items in the set-valued LDP setting. Experiments show that under the same privacy guarantee and computational cost, SVIM significantly improves over existing methods. With SVIM to find frequent items, we propose SVSM to effectively find frequent itemsets, which to our knowledge has not been done before in the LDP setting.",
                "paper_link": "https://www.semanticscholar.org/paper/d1288816b2677080bf7ac31d121d9d013359c66a"
            },
            {
                "title": "Calm: Consistent adaptive local marginal for marginal release under local differential privacy",
                "abstract": "Marginal tables are the workhorse of capturing the correlations among a set of attributes. We consider the problem of constructing marginal tables given a set of user's multi-dimensional data while satisfying Local Differential Privacy (LDP), a privacy notion that protects individual user's privacy without relying on a trusted third party. Existing works on this problem perform poorly in the high-dimensional setting; even worse, some incur very expensive computational overhead. In this paper, we propose CALM, Consistent Adaptive Local Marginal, that takes advantage of the careful challenge analysis and performs consistently better than existing methods. More importantly, CALM can scale well with large data dimensions and marginal sizes. We conduct extensive experiments on several real world datasets. Experimental results demonstrate the effectiveness and efficiency of CALM over existing methods.",
                "paper_link": "https://www.semanticscholar.org/paper/5bbf83fb7d65dae82250f4ac51f5388832c301ba"
            },
            {
                "title": "Graph unlearning",
                "abstract": "With the greater emphasis on privacy and security in our society, the problem of graph unlearning \u2014 revoking the influence of specific data on the trained GNN model, is drawing increasing attention. However, ranging from machine unlearning to recently emerged graph unlearning methods, existing efforts either resort to retraining paradigm, or perform approximate erasure that fails to consider the inter-dependency between connected neighbors or imposes constraints on GNN structure, therefore hard to achieve satisfying performance-complexity trade-offs. In this work, we explore the influence function tailored for graph unlearning, so as to improve the unlearning efficacy and efficiency for graph unlearning. We first present a unified problem formulation of diverse graph unlearning tasks w.r.t. node, edge, and feature. Then, we recognize the crux to the inability of traditional influence function for graph unlearning, and devise Graph Influence Function (GIF), a model-agnostic unlearning method that can efficiently and accurately estimate parameter changes in response to a \u03f5 -mass perturbation in deleted data. The idea is to supplement the objective of the traditional influence function with an additional loss term of the influenced neighbors due to the structural dependency. Further deductions on the closed-form solution of parameter changes provide a better understanding of the unlearning mechanism. We conduct extensive experiments on four representative GNN models and three benchmark datasets to justify the superiority of GIF for diverse graph unlearning tasks in terms of unlearning efficacy, model utility, and unlearning efficiency. Our implementations are available at https://github.com/wujcan/GIF-torch/.",
                "paper_link": "https://www.semanticscholar.org/paper/3c40e8354176746226cfaed3603c8bc9db06c854"
            },
            {
                "title": "Locally differentially private heavy hitter identification",
                "abstract": "The notion of Local Differential Privacy (LDP) enables users to answer sensitive questions while preserving their privacy. The basic LDP frequency oracle protocol enables the aggregator to estimate the frequency of any value. But when the domain of input values is large, finding the most frequent values, also known as the heavy hitters, by estimating the frequencies of all possible values, is computationally infeasible. In this paper, we propose an LDP protocol for identifying heavy hitters. In our proposed protocol, which we call Prefix Extending Method (PEM), users are divided into groups, with each group reporting a prefix of her value. We analyze how to choose optimal parameters for the protocol and identify two design principles for designing LDP protocols with high utility. Experiments show that under the same privacy guarantee and computational cost, PEM has better utility on both synthetic and real-world datasets than existing solutions.",
                "paper_link": "https://www.semanticscholar.org/paper/23feaf87ea8e84e51de8a4824c636a15580826fc"
            },
            {
                "title": "PrivSyn: Differentially Private Data Synthesis",
                "abstract": "The use of differentially private synthetic data has been adopted as a common security measure for the public release of sensitive data. However, the existing solutions either suffer from serious privacy budget splitting or fail to fully automate the generation procedures. In this study, we propose an automated system for synthesizing differentially private synthetic tabular data, called DPView. Our key insight is that high-dimensional data synthesis can be accomplished by utilizing the domain sizes of attributes, which are public information, whereas identifying the correlation among attributes is necessary but leads to severe privacy budget splitting. In addition, we analytically optimize both the privacy budget allocation and consistency procedures of the proposed method through mathematical programming. We further propose two novel methods, including iterative non-negativity and consistency-aware normalization, to postprocess the synthetic data. An extensive set of experimental results demonstrates the superior utility of DPView.",
                "paper_link": "https://www.semanticscholar.org/paper/3595270b1d142996e6e0662f011422702bb12eea"
            },
            {
                "title": "Estimating numerical distributions under local differential privacy",
                "abstract": "When collecting information, local differential privacy (LDP) relieves the concern of privacy leakage from users' perspective, as user's private information is randomized before sent to the aggregator. We study the problem of recovering the distribution over a numerical domain while satisfying LDP. While one can discretize a numerical domain and then apply the protocols developed for categorical domains, we show that taking advantage of the numerical nature of the domain results in better trade-off of privacy and utility. We introduce a new reporting mechanism, called the square wave (SW) mechanism, which exploits the numerical nature in reporting. We also develop an Expectation Maximization with Smoothing (EMS) algorithm, which is applied to aggregated histograms from the SW mechanism to estimate the original distributions. Extensive experiments demonstrate that our proposed approach, SW with EMS, consistently outperforms other methods in a variety of utility metrics.",
                "paper_link": "https://www.semanticscholar.org/paper/9ca6b3b602acffa94baf8c77fbb06ed0af513c4c"
            },
            {
                "title": "An empirical analysis of memorization in fine-tuned autoregressive language models",
                "abstract": "Large language models are shown to present privacy risks through memorization of training data, andseveral recent works have studied such risks for the pre-training phase. Little attention, however, has been given to the fine-tuning phase and it is not well understood how different fine-tuning methods (such as fine-tuning the full model, the model head, and adapter) compare in terms of memorization risk. This presents increasing concern as the \u201cpre-train and fine-tune\u201d paradigm proliferates. In this paper, we empirically study memorization of fine-tuning methods using membership inference and extraction attacks, and show that their susceptibility to attacks is very different. We observe that fine-tuning the head of the model has the highest susceptibility to attacks, whereas fine-tuning smaller adapters appears to be less vulnerable to known extraction attacks.",
                "paper_link": "https://www.semanticscholar.org/paper/51256ee5425d5c425b84e7fac011775d8eff0d1c"
            },
            {
                "title": "Answering multi-dimensional analytical queries under local differential privacy",
                "abstract": "Multi-dimensional analytical (MDA) queries are often issued against a fact table with predicates on (categorical or ordinal) dimensions and aggregations on one or more measures. In this paper, we study the problem of answering MDA queries under local differential privacy (LDP). In the absence of a trusted agent, sensitive dimensions are encoded in a privacy-preserving (LDP) way locally before being sent to the data collector. The data collector estimates the answers to MDA queries, based on the encoded dimensions. We propose several LDP encoders and estimation algorithms, to handle a large class of MDA queries with different types of predicates and aggregation functions. Our techniques are able to answer these queries with tight error bounds and scale well in high-dimensional settings (i.e., error is polylogarithmic in dimension sizes). We conduct experiments on real and synthetic data to verify our theoretical results, and compare our solution with marginal-estimation based solutions.",
                "paper_link": "https://www.semanticscholar.org/paper/000bf47c744f7c180992222a5f5523073dc2f6bb"
            },
            {
                "title": "Locally Differentially Private Frequency Estimation with Consistency",
                "abstract": "Local Differential Privacy (LDP) protects user privacy from the data collector. LDP protocols have been increasingly deployed in the industry. A basic building block is frequency oracle (FO) protocols, which estimate frequencies of values. While several FO protocols have been proposed, the design goal does not lead to optimal results for answering many queries. In this paper, we show that adding post-processing steps to FO protocols by exploiting the knowledge that all individual frequencies should be non-negative and they sum up to one can lead to significantly better accuracy for a wide range of tasks, including frequencies of individual values, frequencies of the most frequent values, and frequencies of subsets of values. We consider 10 different methods that exploit this knowledge differently. We establish theoretical relationships between some of them and conducted extensive experimental evaluations to understand which methods should be used for different query tasks.",
                "paper_link": "https://www.semanticscholar.org/paper/b27acac335a0e38c975a455c501342b7349b7f55"
            },
            {
                "title": "Differential privacy for text analytics via natural text sanitization",
                "abstract": "Texts convey sophisticated knowledge. However, texts also convey sensitive information. Despite the success of general-purpose language models and domain-specific mechanisms with differential privacy (DP), existing text sanitization mechanisms still provide low utility, as cursed by the high-dimensional text representation. The companion issue of utilizing sanitized texts for downstream analytics is also under-explored. This paper takes a direct approach to text sanitization. Our insight is to consider both sensitivity and similarity via our new local DP notion. The sanitized texts also contribute to our sanitization-aware pretraining and fine-tuning, enabling privacy-preserving natural language processing over the BERT language model with promising utility. Surprisingly, the high utility does not boost up the success rate of inference attacks.",
                "paper_link": "https://www.semanticscholar.org/paper/ff9d04fc15a2c52d982b5b7daa787a373ed7f899"
            },
            {
                "title": "Towards Effective Differential Privacy Communication for Users\u2019 Data Sharing Decision and Comprehension",
                "abstract": "Differential privacy protects an individual\u2019s privacy by perturbing data on an aggregated level (DP) or individual level (LDP). We report four online human-subject experiments investigating the effects of using different approaches to communicate differential privacy techniques to laypersons in a health app data collection setting. Experiments 1 and 2 investigated participants\u2019 data disclosure decisions for low-sensitive and high-sensitive personal information when given different DP or LDP descriptions. Experiments 3 and 4 uncovered reasons behind participants\u2019 data sharing decisions, and examined participants\u2019 subjective and objective comprehensions of these DP or LDP descriptions. When shown descriptions that explain the implications instead of the definition/processes of DP or LDP technique, participants demonstrated better comprehension and showed more willingness to share information with LDP than with DP, indicating their understanding of LDP\u2019s stronger privacy guarantee compared with DP.",
                "paper_link": "https://www.semanticscholar.org/paper/11bce08d28aa42debf95b1f2be82ef0ae96e1989"
            },
            {
                "title": "Continuous release of data streams under both centralized and local differential privacy",
                "abstract": "We study the problem of publishing a stream of real-valued data satisfying differential privacy (DP). One major challenge is that the maximal possible value in the stream can be quite large, leading to enormous DP noise and bad utility. To reduce the maximal value and noise, one way is to estimate a threshold so that values above it can be truncated. The intuition is that, in many scenarios, only a few values are large; thus truncation does not change the original data much. We develop such a method that finds a suitable threshold with DP. Given the threshold, we then propose an online hierarchical method and several post-processing techniques. Building on these ideas, we formalize the steps in a framework for the private publishing of streaming data. Our framework consists of three components: a threshold optimizer that privately estimates the threshold, a perturber that adds calibrated noise to the stream, and a smoother that improves the result using post-processing. Within our framework, we also design an algorithm satisfying the more stringent DP setting called local DP. Using four real-world datasets, we demonstrate that our mechanism outperforms the state-of-the-art by a factor of $6-10$ orders of magnitude in terms of utility (measured by the mean squared error of the typical scenario of answering a random range query).",
                "paper_link": "https://www.semanticscholar.org/paper/1192d66961645de1ea85e9431c75354b8fa88fdc"
            },
            {
                "title": "Improving utility and security of the shuffler-based differential privacy",
                "abstract": "When collecting information, local differential privacy (LDP) alleviates privacy concerns of users because their private information is randomized before being sent it to the central aggregator. LDP imposes large amount of noise as each user executes the randomization independently. To address this issue, recent work introduced an intermediate server with the assumption that this intermediate server does not collude with the aggregator. Under this assumption, less noise can be added to achieve the same privacy guarantee as LDP, thus improving utility for the data collection task. This paper investigates this multiple-party setting of LDP. We analyze the system model and identify potential adversaries. We then make two improvements: a new algorithm that achieves a better privacy-utility tradeoff; and a novel protocol that provides better protection against various attacks. Finally, we perform experiments to compare different methods and demonstrate the benefits of using our proposed method.",
                "paper_link": "https://www.semanticscholar.org/paper/b82f52d208bc236409cfb9eeb44b07a0217b9df3"
            },
            {
                "title": "Answering multi-dimensional range queries under local differential privacy",
                "abstract": "In this paper, we tackle the problem of answering multi-dimensional range queries under local differential privacy. There are three key technical challenges: capturing the correlations among attributes, avoiding the curse of dimensionality, and dealing with the large domains of attributes. None of the existing approaches satisfactorily deals with all three challenges. Overcoming these three challenges, we first propose an approach called Two-Dimensional Grids (TDG). Its main idea is to carefully use binning to partition the two-dimensional (2-D) domains of all attribute pairs into 2-D grids that can answer all 2-D range queries and then estimate the answer of a higher dimensional range query from the answers of the associated 2-D range queries. However, in order to reduce errors due to noises, coarse granularities are needed for each attribute in 2-D grids, losing fine-grained distribution information for individual attributes. To correct this deficiency, we further propose Hybrid-Dimensional Grids (HDG), which also introduces 1-D grids to capture finer-grained information on distribution of each individual attribute and combines information from 1-D and 2-D grids to answer range queries. To make HDG consistently effective, we provide a guideline for properly choosing granularities of grids based on an analysis of how different sources of errors are impacted by these choices. Extensive experiments conducted on real and synthetic datasets show that HDG can give a significant improvement over the existing approaches.",
                "paper_link": "https://www.semanticscholar.org/paper/816df66820820903e44565043486e5c222db4f5a"
            },
            {
                "title": "DP-Forward: Fine-tuning and inference on language models with differential privacy in forward pass",
                "abstract": "Differentially private stochastic gradient descent (DP-SGD) adds noise to gradients in back-propagation, safeguarding training data from privacy leakage, particularly membership inference. It fails to cover (inference-time) threats like embedding inversion and sensitive attribute inference. It is also costly in storage and computation when used to fine-tune large pre-trained language models (LMs). We propose DP-Forward, which directly perturbs embedding matrices in the forward pass of LMs. It satisfies stringent local DP requirements for training and inference data. To instantiate it using the smallest matrix-valued noise, we devise an analytic matrix Gaussian mechanism (aMGM) by drawing possibly non-i.i.d. noise from a matrix Gaussian distribution. We then investigate perturbing outputs from different hidden (sub-)layers of LMs with aMGM noises. Its utility on three typical tasks almost hits the non-private baseline and outperforms DP-SGD by up to 7.7pp at a moderate privacy level. It saves 3x time and memory costs compared to DP-SGD with the latest high-speed library. It also reduces the average success rates of embedding inversion and sensitive attribute inference by up to 88pp and 41pp, respectively, whereas DP-SGD fails.",
                "paper_link": "https://www.semanticscholar.org/paper/385c2ee0bf829676d1a5aacfc697fc6a9d245ed5"
            },
            {
                "title": "PrivTrace: Differentially Private Trajectory Synthesis by Adaptive Markov Models",
                "abstract": "Publishing trajectory data (individual's movement information) is very useful, but it also raises privacy concerns. To handle the privacy concern, in this paper, we apply differential privacy, the standard technique for data privacy, together with Markov chain model, to generate synthetic trajectories. We notice that existing studies all use Markov chain model and thus propose a framework to analyze the usage of the Markov chain model in this problem. Based on the analysis, we come up with an effective algorithm PrivTrace that uses the first-order and second-order Markov model adaptively. We evaluate PrivTrace and existing methods on synthetic and real-world datasets to demonstrate the superiority of our method.",
                "paper_link": "https://www.semanticscholar.org/paper/2266b8c00783e9bd06873b0e108b87f83c13835d"
            },
            {
                "title": "Locally differentially private sparse vector aggregation",
                "abstract": "Vector mean estimation is a central primitive in federated analytics. In vector mean estimation, each user $i \\in[n]$ holds a real-valued vector $v_{i} \\in[-1,1]^{d}$, and a server wants to estimate the mean of all n vectors; we would additionally like to protect each user\u2019s privacy. In this paper, we consider the k-sparse version of the vector mean estimation problem. That is, suppose each user\u2019s vector has at most k non-zero coordinates in its d-dimensional vector, and moreover, $k \\ll d$. In practice, since the universe size d can be very large (e.g., the space of all possible URLs), we would like the per-user communication to be succinct, i.e., independent of or (poly-)logarithmic in the universe size.In this paper, we show matching upper- and lower-bounds for the k-sparse vector mean estimation problem under local differential privacy (LDP). Specifically, we construct new mechanisms that achieve asymptotically optimal error as well as succinct communication, either under user-level-LDP or event-level-LDP. We implement our algorithms and evaluate them on synthetic and real-world datasets. Our experiments show that we can often achieve one or two orders of magnitude reduction in error compared with prior work under typical choices of parameters, while incurring insignificant communication cost.",
                "paper_link": "https://www.semanticscholar.org/paper/22498b52c758f7e478110c105c8ade930da010db"
            }
        ]
    },
    {
        "Professor": "Hongning Wang",
        "Papers": [
            {
                "title": "Latent aspect rating analysis on review text data: a rating regression approach",
                "abstract": "In this paper, we define and study a new opinionated text data analysis problem called Latent Aspect Rating Analysis (LARA), which aims at analyzing opinions expressed about an entity in an online review at the level of topical aspects to discover each individual reviewer's latent opinion on each aspect as well as the relative emphasis on different aspects when forming the overall judgment of the entity. We propose a novel probabilistic rating regression model to solve this new text mining problem in a general way. Empirical experiments on a hotel review data set show that the proposed latent rating regression model can effectively solve the problem of LARA, and that the detailed analysis of opinions at the level of topical aspects enabled by the proposed model can support a wide range of application tasks, such as aspect opinion summarization, entity ranking based on aspect ratings, and analysis of reviewers rating behavior.",
                "paper_link": "https://www.semanticscholar.org/paper/6ff505e63ffebf419736d6c65741ee63b3ea720e"
            },
            {
                "title": "Latent aspect rating analysis without aspect keyword supervision",
                "abstract": "Mining detailed opinions buried in the vast amount of review text data is an important, yet quite challenging task with widespread applications in multiple domains. Latent Aspect Rating Analysis (LARA) refers to the task of inferring both opinion ratings on topical aspects (e.g., location, service of a hotel) and the relative weights reviewers have placed on each aspect based on review content and the associated overall ratings. A major limitation of previous work on LARA is the assumption of pre-specified aspects by keywords. However, the aspect information is not always available, and it may be difficult to pre-define appropriate aspects without a good knowledge about what aspects are actually commented on in the reviews.\n In this paper, we propose a unified generative model for LARA, which does not need pre-specified aspect keywords and simultaneously mines 1) latent topical aspects, 2) ratings on each identified aspect, and 3) weights placed on different aspects by a reviewer. Experiment results on two different review data sets demonstrate that the proposed model can effectively perform the Latent Aspect Rating Analysis task without the supervision of aspect keywords. Because of its generality, the proposed model can be applied to explore all kinds of opinionated text data containing overall sentiment judgments and support a wide range of interesting application tasks, such as aspect-based opinion summarization, personalized entity ranking and recommendation, and reviewer behavior analysis.",
                "paper_link": "https://www.semanticscholar.org/paper/d4338f021d53fa01fe8d72e4715617f9ce158924"
            },
            {
                "title": "Explainable recommendation via multi-task learning in opinionated text data",
                "abstract": "Explaining automatically generated recommendations allows users to make more informed and accurate decisions about which results to utilize, and therefore improves their satisfaction. In this work, we develop a multi-task learning solution for explainable recommendation. Two companion learning tasks of user preference modeling for recommendation and opinionated content modeling for explanation are integrated via a joint tensor factorization. As a result, the algorithm predicts not only a user's preference over a list of items, i.e., recommendation, but also how the user would appreciate a particular item at the feature level, i.e., opinionated textual explanation. Extensive experiments on two large collections of Amazon and Yelp reviews confirmed the effectiveness of our solution in both recommendation and explanation tasks, compared with several existing recommendation algorithms. And our extensive user study clearly demonstrates the practical value of the explainable recommendations generated by our algorithm.",
                "paper_link": "https://www.semanticscholar.org/paper/8b0791892bd7ae6004fc7afa0a78289c5f1dc995"
            },
            {
                "title": "Enhancing personalized search by mining and modeling task behavior",
                "abstract": "Personalized search systems tailor search results to the current user intent using historic search interactions. This relies on being able to find pertinent information in that user's search history, which can be challenging for unseen queries and for new search scenarios. Building richer models of users' current and historic search tasks can help improve the likelihood of finding relevant content and enhance the relevance and coverage of personalization methods. The task-based approach can be applied to the current user's search history, or as we focus on here, all users' search histories as so-called \"groupization\" (a variant of personalization whereby other users' profiles can be used to personalize the search experience). We describe a method whereby we mine historic search-engine logs to find other users performing similar tasks to the current user and leverage their on-task behavior to identify Web pages to promote in the current ranking. We investigate the effectiveness of this approach versus query-based matching and finding related historic activity from the current user (i.e., group versus individual). As part of our studies we also explore the use of the on-task behavior of particular user cohorts, such as people who are expert in the topic currently being searched, rather than all other users. Our approach yields promising gains in retrieval performance, and has direct implications for improving personalization in search systems.",
                "paper_link": "https://www.semanticscholar.org/paper/afc8cf3418cf96f1153dbe29613604eaa6fb484c"
            },
            {
                "title": "DeepMeSH: deep semantic representation for improving large-scale MeSH indexing",
                "abstract": "Paper not found or no abstract available",
                "paper_link": "Link not available"
            },
            {
                "title": "Exploring and exploiting user search behavior on mobile and tablet devices to improve search relevance",
                "abstract": "In this paper, we present a log-based study on user search behavior comparisons on three different platforms: desktop, mobile and tablet. We use three-month search logs in 2012 from a commercial search engine for our study. Our objective is to better understand how and to what extent mobile and tablet searchers behave differently than desktop users. Our study spans a variety of aspects including query categorization, query length, search time distribution, search location distribution, user click patterns and so on. From our data set, we reveal that there are significant differences between user search patterns in these three platforms, and therefore use the same ranking system is not an optimal solution for all of them. Consequently, we propose a framework that leverages a set of domain-specific features, along with the training data from desktop search, to further improve the search relevance for mobile and tablet platforms. Experimental results demonstrate that by transferring knowledge from desktop search, search relevance on mobile and tablet can be greatly improved.",
                "paper_link": "https://www.semanticscholar.org/paper/98b19917d3e662364550f9c42dbf6e0a84d79280"
            },
            {
                "title": "Factorization bandits for interactive recommendation",
                "abstract": "\n \n We perform online interactive recommendation via a factorization-based bandit algorithm. Low-rank matrix completion is performed over an incrementally constructed user-item preference matrix, where an upper confidence bound based item selection strategy is developed to balance the exploit/explore trade-off during online learning. Observable contextual features and dependency among users (e.g., social influence) are leveraged to improve the algorithm's convergence rate and help conquer cold-start in recommendation. A high probability sublinear upper regret bound is proved for the developed algorithm, where considerable regret reduction is achieved on both user and item sides. Extensive experimentations on both simulations and large-scale real-world datasets confirmed the advantages of the proposed algorithm compared with several state-of-the-art factorization-based and bandit-based collaborative filtering methods.\n \n",
                "paper_link": "https://www.semanticscholar.org/paper/361f5c406f71d56e18450ffe0c095e9d49badfe4"
            },
            {
                "title": "Contextual Bandits in a Collaborative Environment",
                "abstract": "Contextual bandit algorithms provide principled online learning solutions to find optimal trade-offs between exploration and exploitation with companion side-information. They have been extensively used in many important practical scenarios, such as display advertising and content recommendation. A common practice estimates the unknown bandit parameters pertaining to each user independently. This unfortunately ignores dependency among users and thus leads to suboptimal solutions, especially for the applications that have strong social components. In this paper, we develop a collaborative contextual bandit algorithm, in which the adjacency graph among users is leveraged to share context and payoffs among neighboring users while online updating. We rigorously prove an improved upper regret bound of the proposed collaborative bandit algorithm comparing to conventional independent bandit algorithms. Extensive experiments on both synthetic and three large-scale real-world datasets verified the improvement of our proposed algorithm against several state-of-the-art contextual bandit algorithms.",
                "paper_link": "https://www.semanticscholar.org/paper/59e6926b6b6399b856c6aa453df21a0ee5634572"
            },
            {
                "title": "Learning to extract cross-session search tasks",
                "abstract": "Search tasks, comprising a series of search queries serving the same information need, have recently been recognized as an accurate atomic unit for modeling user search intent. Most prior research in this area has focused on short-term search tasks within a single search session, and heavily depend on human annotations for supervised classification model learning. In this work, we target the identification of long-term, or cross-session, search tasks (transcending session boundaries) by investigating inter-query dependencies learned from users' searching behaviors. A semi-supervised clustering model is proposed based on the latent structural SVM framework, and a set of effective automatic annotation rules are proposed as weak supervision to release the burden of manual annotation. Experimental results based on a large-scale search log collected from Bing.com confirms the effectiveness of the proposed model in identifying cross-session search tasks and the utility of the introduced weak supervision signals. Our learned model enables a more comprehensive understanding of users' search behaviors via search logs and facilitates the development of dedicated search-engine support for long-term tasks.",
                "paper_link": "https://www.semanticscholar.org/paper/5e53614696435379ee93830ff9228f1a599f813c"
            },
            {
                "title": "A model-based reinforcement learning with adversarial training for online recommendation",
                "abstract": "Reinforcement learning is effective in optimizing policies for recommender systems. Current solutions mostly focus on model-free approaches, which require frequent interactions with a real environment, and thus are expensive in model learning. Offline evaluation methods, such as importance sampling, can alleviate such limitations, but usually request a large amount of logged data and do not work well when the action space is large. In this work, we propose a model-based reinforcement learning solution which models the user-agent interaction for offline policy learning via a generative adversarial network. To reduce bias in the learnt policy, we use the discriminator to evaluate the quality of generated sequences and rescale the generated rewards. Our theoretical analysis and empirical evaluations demonstrate the effectiveness of our solution in identifying patterns from given offline data and learning policies based on the offline and generated data.",
                "paper_link": "https://www.semanticscholar.org/paper/7cfe40baca6b698c5b3a6620072308a4ab0ab740"
            },
            {
                "title": "Context Attentive Document Ranking and Query Suggestion",
                "abstract": "We present a context-aware neural ranking model to exploit users' on-task search activities and enhance retrieval performance. In particular, a two-level hierarchical recurrent neural network is introduced to learn search context representation of individual queries, search tasks, and corresponding dependency structure by jointly optimizing two companion retrieval tasks: document ranking and query suggestion. To identify variable dependency structure between search context and users' ongoing search activities, attention at both levels of recurrent states are introduced. Extensive experiment comparisons against a rich set of baseline methods and an in-depth ablation analysis confirm the value of our proposed approach for modeling search context buried in search tasks.",
                "paper_link": "https://www.semanticscholar.org/paper/448b64df68335d3695a37c54770e7d5cd5f6fe68"
            },
            {
                "title": "Learning Contextual Bandits in a Non-stationary Environment",
                "abstract": "Multi-armed bandit algorithms have become a reference solution for handling the explore/exploit dilemma in recommender systems, and many other important real-world problems, such as display advertisement. However, such algorithms usually assume a stationary reward distribution, which hardly holds in practice as users' preferences are dynamic. This inevitably costs a recommender system consistent suboptimal performance. In this paper, we consider the situation where the underlying distribution of reward remains unchanged over (possibly short) epochs and shifts at unknown time instants. In accordance, we propose a contextual bandit algorithm that detects possible changes of environment based on its reward estimation confidence and updates its arm selection strategy respectively. Rigorous upper regret bound analysis of the proposed algorithm demonstrates its learning effectiveness in such a non-trivial environment. Extensive empirical evaluations on both synthetic and real-world datasets for recommendation confirm its practical utility in a changing environment.",
                "paper_link": "https://www.semanticscholar.org/paper/2ac1373744bec77a2e2a6f98753ff94be1cf9dd6"
            },
            {
                "title": "Personalized ranking model adaptation for web search",
                "abstract": "Search engines train and apply a single ranking model across all users, but searchers' information needs are diverse and cover a broad range of topics. Hence, a single user-independent ranking model is insufficient to satisfy different users' result preferences. Conventional personalization methods learn separate models of user interests and use those to re-rank the results from the generic model. Those methods require significant user history information to learn user preferences, have low coverage in the case of memory-based methods that learn direct associations between query-URL pairs, and have limited opportunity to markedly affect the ranking given that they only re-order top-ranked items. In this paper, we propose a general ranking model adaptation framework for personalized search. Using a given user-independent ranking model trained offline and limited number of adaptation queries from individual users, the framework quickly learns to apply a series of linear transformations, e.g., scaling and shifting, over the parameters of the given global ranking model such that the adapted model can better fit each individual user's search preferences. Extensive experimentation based on a large set of search logs from a major commercial Web search engine confirms the effectiveness of the proposed method compared to several state-of-the-art ranking model adaptation methods.",
                "paper_link": "https://www.semanticscholar.org/paper/281fb2243cf22eacca2b995d6b276c954b803043"
            },
            {
                "title": "Learning online discussion structures by conditional random fields",
                "abstract": "Online forum discussions are emerging as valuable information repository, where knowledge is accumulated by the interaction among users, leading to multiple threads with structures. Such replying structure in each thread conveys important information about the discussion content. Unfortunately, not all the online forum sites would explicitly record such replying relationship, making it hard to for both users and computers to digest the information buried in a thread discussion. In this paper, we propose a probabilistic model in the Conditional Random Fields framework to predict the replying structure for a threaded online discussion. Different from previous thread reconstruction methods, most of which fail to consider dependency between the posts, we cast the problem as a supervised structure learning problem to incorporate the features describing the structural dependency among the discussion content and learn their relationship. Experiment results on three different online forums show that the proposed method can well capture the replying structures in online discussion threads, and multiple tasks such as forum search and question answering can benefit from the reconstructed replying structures.",
                "paper_link": "https://www.semanticscholar.org/paper/06ee1d838f553173c04aabe90b8aabc04ebe4b1e"
            },
            {
                "title": "Learning Hidden Features for Contextual Bandits",
                "abstract": "Contextual bandit algorithms provide principled online learning solutions to find optimal trade-offs between exploration and exploitation with companion side-information. Most contextual bandit algorithms simply assume the learner would have access to the entire set of features, which govern the generation of payoffs from a user to an item. However, in practice it is challenging to exhaust all relevant features ahead of time, and oftentimes due to privacy or sampling constraints many factors are unobservable to the algorithm. Failing to model such hidden factors leads a system to make constantly suboptimal predictions. In this paper, we propose to learn the hidden features for contextual bandit algorithms. Hidden features are explicitly introduced in our reward generation assumption, in addition to the observable contextual features. A scalable bandit algorithm is achieved via coordinate descent, in which closed form solutions exist at each iteration for both hidden features and bandit parameters. Most importantly, we rigorously prove that the developed contextual bandit algorithm achieves a sublinear upper regret bound with high probability, and a linear regret is inevitable if one fails to model such hidden features. Extensive experimentation on both simulations and large-scale real-world datasets verified the advantages of the proposed algorithm compared with several state-of-the-art contextual bandit algorithms and existing ad-hoc combinations between bandit algorithms and matrix factorization methods.",
                "paper_link": "https://www.semanticscholar.org/paper/89c03d116f10d9efe61a21557f2ef87810417e04"
            },
            {
                "title": "Returning is Believing: Optimizing Long-term User Engagement in Recommender Systems",
                "abstract": "In this work, we propose to improve long-term user engagement in a recommender system from the perspective of sequential decision optimization, where users' click and return behaviors are directly modeled for online optimization. A bandit-based solution is formulated to balance three competing factors during online learning, including exploitation for immediate click, exploitation for expected future clicks, and exploration of unknowns for model estimation. We rigorously prove that with a high probability our proposed solution achieves a sublinear upper regret bound in maximizing cumulative clicks from a population of users in a given period of time, while a linear regret is inevitable if a user's temporal return behavior is not considered when making the recommendations. Extensive experimentation on both simulations and a large-scale real-world dataset collected from Yahoo frontpage news recommendation log verified the effectiveness and significant improvement of our proposed algorithm compared with several state-of-the-art online learning baselines for recommendation.",
                "paper_link": "https://www.semanticscholar.org/paper/fba0a105b03a3acaab8d0b08d873584401c0d074"
            },
            {
                "title": "Multi-task learning for document ranking and query suggestion",
                "abstract": null,
                "paper_link": "https://www.semanticscholar.org/paper/15e12b3b6719e8e4516b7516fae212ea61a94fce"
            },
            {
                "title": "Adapting deep RankNet for personalized search",
                "abstract": "RankNet is one of the widely adopted ranking models for web search tasks. However, adapting a generic RankNet for personalized search is little studied. In this paper, we first continue-trained a variety of RankNets with different number of hidden layers and network structures over a previously trained global RankNet model, and observed that a deep neural network with five hidden layers gives the best performance. To further improve the performance of adaptation, we propose a set of novel methods categorized into two groups. In the first group, three methods are proposed to properly assess the usefulness of each adaptation instance and only leverage the most informative instances to adapt a user-specific RankNet model. These assessments are based on KL-divergence, click entropy or a heuristic to ignore top clicks in adaptation queries. In the second group, two methods are proposed to regularize the training of the neural network in RankNet: one of these methods regularize the error back-propagation via a truncated gradient approach, while the other method limits the depth of the back propagation when adapting the neural network. We empirically evaluate our approaches using a large-scale real-world data set. Experimental results exhibit that our methods all give significant improvements over a strong baseline ranking system, and the truncated gradient approach gives the best performance, significantly better than all others.",
                "paper_link": "https://www.semanticscholar.org/paper/9d4f933f80500a8a5e57cb3a7ceac766cf6f8317"
            },
            {
                "title": "D\u00e9j\u00e0 vu: A Contextualized Temporal Attention Mechanism for Sequential Recommendation",
                "abstract": "Predicting users\u2019 preferences based on their sequential behaviors in history is challenging and crucial for modern recommender systems. Most existing sequential recommendation algorithms focus on transitional structure among the sequential actions, but largely ignore the temporal and context information, when modeling the influence of a historical event to current prediction. In this paper, we argue that the influence from the past events on a user\u2019s current action should vary over the course of time and under different context. Thus, we propose a Contextualized Temporal Attention Mechanism that learns to weigh historical actions\u2019 influence on not only what action it is, but also when and how the action took place. More specifically, to dynamically calibrate the relative input dependence from the self-attention mechanism, we deploy multiple parameterized kernel functions to learn various temporal dynamics, and then use the context information to determine which of these reweighing kernels to follow for each input. In empirical evaluations on two large public recommendation datasets, our model consistently outperformed an extensive set of state-of-the-art sequential recommendation methods.",
                "paper_link": "https://www.semanticscholar.org/paper/41e2874e8ee0abe309724494ad635d9f9f9a74fa"
            },
            {
                "title": "Adversarial domain adaptation for machine reading comprehension",
                "abstract": "In this paper, we focus on unsupervised domain adaptation for Machine Reading Comprehension (MRC), where the source domain has a large amount of labeled data, while only unlabeled passages are available in the target domain. To this end, we propose an Adversarial Domain Adaptation framework (AdaMRC), where (i) pseudo questions are first generated for unlabeled passages in the target domain, and then (ii) a domain classifier is incorporated into an MRC model to predict which domain a given passage-question pair comes from. The classifier and the passage-question encoder are jointly trained using adversarial learning to enforce domain-invariant representation learning. Comprehensive evaluations demonstrate that our approach (i) is generalizable to different MRC models and datasets, (ii) can be combined with pre-trained large-scale language models (such as ELMo and BERT), and (iii) can be extended to semi-supervised learning.",
                "paper_link": "https://www.semanticscholar.org/paper/5e73ab0179d78adecf44d4fbc34c3671032533a7"
            }
        ]
    }
]